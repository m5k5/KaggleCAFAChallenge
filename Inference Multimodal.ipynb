{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1d77e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 17:15:31.209773: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-09 17:15:31.662776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/ML/cafa-5-protein-function-prediction\n",
      "/mnt/e/ML/output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "DATA_PATH_INTERPRO = os.getenv('DATA_PATH_INTERPRO')\n",
    "print(DATA_PATH)\n",
    "print(DATA_PATH_INTERPRO)\n",
    "\n",
    "# subontology (CCO, BPO or BPO)\n",
    "SOs = ['CCO', 'MFO', 'BPO']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3544f8a",
   "metadata": {},
   "source": [
    "## Reading fasta, obo and tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b515f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "sequences = [rec.seq for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Test (Targets)/testsuperset.fasta\"),\"fasta\")]\n",
    "ids = [rec.id for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Test (Targets)/testsuperset.fasta\"),\"fasta\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e492f673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 141865 sequences in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} sequences in the dataset.\".format(len(sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2898414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import obonet\n",
    "\n",
    "# Read the taxrank ontology\n",
    "url = os.path.join(DATA_PATH, \"Train/go-basic.obo\")\n",
    "graph = obonet.read_obo(url)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c099fc2",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99f6a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2055\n",
      "4282\n",
      "11807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pickle \n",
    "\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'MLB_CCO.pkl'), 'rb') as f:\n",
    "    mlbCco = pickle.load(f)\n",
    "with open(os.path.join(DATA_PATH,'MLB_MFO.pkl'), 'rb') as f:\n",
    "    mlbMfo = pickle.load(f)\n",
    "with open(os.path.join(DATA_PATH,'MLB_BPO.pkl'), 'rb') as f:\n",
    "    mlbBpo = pickle.load(f)\n",
    "\n",
    "print(len(mlbCco.classes_))\n",
    "print(len(mlbMfo.classes_))\n",
    "print(len(mlbBpo.classes_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92019f8c",
   "metadata": {},
   "source": [
    "## Amino acids encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b747477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aminos_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e42462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_dict = {'A': 1, 'B':24, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'O': 21, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'U': 22, 'V': 18, 'W': 19, 'Y': 20, 'X':30, 'Z':23}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70bc50",
   "metadata": {},
   "source": [
    "## T5 Embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23ae9aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141865,)\n",
      "(141865, 1025)\n",
      "(141864, 1025)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column_1</th>\n",
       "      <th>Column_2</th>\n",
       "      <th>Column_3</th>\n",
       "      <th>Column_4</th>\n",
       "      <th>Column_5</th>\n",
       "      <th>Column_6</th>\n",
       "      <th>Column_7</th>\n",
       "      <th>Column_8</th>\n",
       "      <th>Column_9</th>\n",
       "      <th>Column_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Column_1015</th>\n",
       "      <th>Column_1016</th>\n",
       "      <th>Column_1017</th>\n",
       "      <th>Column_1018</th>\n",
       "      <th>Column_1019</th>\n",
       "      <th>Column_1020</th>\n",
       "      <th>Column_1021</th>\n",
       "      <th>Column_1022</th>\n",
       "      <th>Column_1023</th>\n",
       "      <th>Column_1024</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q9CQV8</th>\n",
       "      <td>0.054705</td>\n",
       "      <td>0.063420</td>\n",
       "      <td>-0.015320</td>\n",
       "      <td>-0.016506</td>\n",
       "      <td>0.042195</td>\n",
       "      <td>0.021592</td>\n",
       "      <td>-0.118535</td>\n",
       "      <td>-0.063298</td>\n",
       "      <td>-0.046146</td>\n",
       "      <td>-0.102311</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019581</td>\n",
       "      <td>-0.043712</td>\n",
       "      <td>-0.072322</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.018459</td>\n",
       "      <td>-0.047278</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>-0.043319</td>\n",
       "      <td>0.036009</td>\n",
       "      <td>0.063093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P62259</th>\n",
       "      <td>0.090373</td>\n",
       "      <td>0.089842</td>\n",
       "      <td>-0.023887</td>\n",
       "      <td>-0.011446</td>\n",
       "      <td>0.051465</td>\n",
       "      <td>0.020982</td>\n",
       "      <td>-0.110989</td>\n",
       "      <td>-0.066646</td>\n",
       "      <td>-0.041259</td>\n",
       "      <td>-0.087551</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024399</td>\n",
       "      <td>-0.041957</td>\n",
       "      <td>-0.066329</td>\n",
       "      <td>0.006856</td>\n",
       "      <td>0.028449</td>\n",
       "      <td>-0.053758</td>\n",
       "      <td>0.009699</td>\n",
       "      <td>-0.053350</td>\n",
       "      <td>0.019644</td>\n",
       "      <td>0.079630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P68510</th>\n",
       "      <td>0.043588</td>\n",
       "      <td>0.039572</td>\n",
       "      <td>-0.014332</td>\n",
       "      <td>-0.011769</td>\n",
       "      <td>0.045109</td>\n",
       "      <td>0.015847</td>\n",
       "      <td>-0.103339</td>\n",
       "      <td>-0.047735</td>\n",
       "      <td>-0.022730</td>\n",
       "      <td>-0.091452</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029648</td>\n",
       "      <td>-0.037944</td>\n",
       "      <td>-0.046043</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.018028</td>\n",
       "      <td>-0.030746</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>-0.044464</td>\n",
       "      <td>0.030974</td>\n",
       "      <td>0.040322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P61982</th>\n",
       "      <td>0.055668</td>\n",
       "      <td>0.049560</td>\n",
       "      <td>-0.019646</td>\n",
       "      <td>-0.006977</td>\n",
       "      <td>0.039897</td>\n",
       "      <td>0.021177</td>\n",
       "      <td>-0.108079</td>\n",
       "      <td>-0.047191</td>\n",
       "      <td>-0.031517</td>\n",
       "      <td>-0.100057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023210</td>\n",
       "      <td>-0.041704</td>\n",
       "      <td>-0.048440</td>\n",
       "      <td>0.006088</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>-0.046751</td>\n",
       "      <td>-0.006635</td>\n",
       "      <td>-0.041455</td>\n",
       "      <td>0.016683</td>\n",
       "      <td>0.057030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O70456</th>\n",
       "      <td>0.022637</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>-0.002696</td>\n",
       "      <td>-0.034456</td>\n",
       "      <td>0.034854</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>-0.114046</td>\n",
       "      <td>-0.050019</td>\n",
       "      <td>-0.026491</td>\n",
       "      <td>-0.097928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019185</td>\n",
       "      <td>-0.032108</td>\n",
       "      <td>-0.051394</td>\n",
       "      <td>0.008448</td>\n",
       "      <td>0.015208</td>\n",
       "      <td>-0.037987</td>\n",
       "      <td>0.030977</td>\n",
       "      <td>-0.042407</td>\n",
       "      <td>0.041232</td>\n",
       "      <td>0.047161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Column_1  Column_2  Column_3  Column_4  Column_5  Column_6  Column_7  \\\n",
       "ids                                                                            \n",
       "Q9CQV8  0.054705  0.063420 -0.015320 -0.016506  0.042195  0.021592 -0.118535   \n",
       "P62259  0.090373  0.089842 -0.023887 -0.011446  0.051465  0.020982 -0.110989   \n",
       "P68510  0.043588  0.039572 -0.014332 -0.011769  0.045109  0.015847 -0.103339   \n",
       "P61982  0.055668  0.049560 -0.019646 -0.006977  0.039897  0.021177 -0.108079   \n",
       "O70456  0.022637  0.014306 -0.002696 -0.034456  0.034854  0.020822 -0.114046   \n",
       "\n",
       "        Column_8  Column_9  Column_10  ...  Column_1015  Column_1016  \\\n",
       "ids                                    ...                             \n",
       "Q9CQV8 -0.063298 -0.046146  -0.102311  ...    -0.019581    -0.043712   \n",
       "P62259 -0.066646 -0.041259  -0.087551  ...    -0.024399    -0.041957   \n",
       "P68510 -0.047735 -0.022730  -0.091452  ...    -0.029648    -0.037944   \n",
       "P61982 -0.047191 -0.031517  -0.100057  ...    -0.023210    -0.041704   \n",
       "O70456 -0.050019 -0.026491  -0.097928  ...    -0.019185    -0.032108   \n",
       "\n",
       "        Column_1017  Column_1018  Column_1019  Column_1020  Column_1021  \\\n",
       "ids                                                                       \n",
       "Q9CQV8    -0.072322     0.002404     0.018459    -0.047278     0.012195   \n",
       "P62259    -0.066329     0.006856     0.028449    -0.053758     0.009699   \n",
       "P68510    -0.046043     0.003603     0.018028    -0.030746     0.003671   \n",
       "P61982    -0.048440     0.006088     0.020110    -0.046751    -0.006635   \n",
       "O70456    -0.051394     0.008448     0.015208    -0.037987     0.030977   \n",
       "\n",
       "        Column_1022  Column_1023  Column_1024  \n",
       "ids                                            \n",
       "Q9CQV8    -0.043319     0.036009     0.063093  \n",
       "P62259    -0.053350     0.019644     0.079630  \n",
       "P68510    -0.044464     0.030974     0.040322  \n",
       "P61982    -0.041455     0.016683     0.057030  \n",
       "O70456    -0.042407     0.041232     0.047161  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings = np.load(os.path.join(DATA_PATH, \"t5/test_embeds.npy\"))\n",
    "\n",
    "column_num = train_embeddings.shape[1]\n",
    "t5df = pd.DataFrame(\n",
    "    train_embeddings, columns=[\"Column_\" + str(i) for i in range(1, column_num + 1)]\n",
    ")\n",
    "t5Dimension = t5df.shape[1]\n",
    "\n",
    "train_protein_ids = np.load(os.path.join(DATA_PATH, \"t5/test_ids.npy\"))\n",
    "t5df[\"ids\"] = train_protein_ids\n",
    "print(train_protein_ids.shape)\n",
    "print(t5df.shape)\n",
    "t5df = t5df.drop_duplicates(subset=[\"ids\"])\n",
    "print(t5df.shape)\n",
    "t5df.set_index(\"ids\", inplace=True)\n",
    "t5df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79ce144",
   "metadata": {},
   "source": [
    "## Interpro Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf029114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'MLB_InterPro_'+\"BPO\"+'.pkl'), 'rb') as f:\n",
    "    mlbInterpro= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e46da43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  testsuperset1.fasta.json\n",
      "Processing  testsuperset2.fasta.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "allInterproData =[]\n",
    "\n",
    "for root,dirs,files in os.walk(os.path.join(DATA_PATH_INTERPRO, \"test\")):\n",
    "    for f in files:\n",
    "        if f.endswith(\".json\"):\n",
    "            print(\"Processing \", f)\n",
    "            with open(os.path.join(root, f)) as inputFile:\n",
    "                iprData = json.load(inputFile)\n",
    "            allInterproData=[*allInterproData, *iprData[\"results\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0626f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139946"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allInterproData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33e93405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb1c8f124b840a587a99ea6acbf8765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139946 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "iprIds = {}\n",
    "\n",
    "\n",
    "for entry in tqdm(allInterproData):\n",
    "    entryId = entry[\"xref\"][0][\"id\"]\n",
    "    matches=[]\n",
    "    for match in entry[\"matches\"]:\n",
    "        sigEntry = match[\"signature\"][\"entry\"]\n",
    "        if(sigEntry):\n",
    "            type = sigEntry[\"type\"]\n",
    "            if type==\"DOMAIN\" or type==\"REPEAT\" or type==\"FAMILY\" or type==\"HOMOLOGOUS_SUPERFAMILY\":\n",
    "                iprId = match[\"signature\"][\"entry\"][\"accession\"]\n",
    "                matches.append(iprId)\n",
    "    iprIds[entryId] = matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89212dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testInput = mlbInterpro.transform([iprIds[\"Q55G04\"]])\n",
    "np.count_nonzero(testInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa146de0",
   "metadata": {},
   "source": [
    "## Physiochemical Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b84bfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading presaved data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from Bio.Seq import MutableSeq, Seq\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "if os.path.exists(os.path.join(DATA_PATH, \"PCDictTest\"+\".pkl\")):\n",
    "    print(\"Loading presaved data\")\n",
    "    with open(os.path.join(DATA_PATH, \"PCDictTest\"+\".pkl\"), 'rb') as f:\n",
    "        PCDict = pickle.load(f)\n",
    "else:\n",
    "    PCDict = {}\n",
    "\n",
    "    for i,seq in enumerate(tqdm(sequences)):\n",
    "\n",
    "        index = ids[i]\n",
    "        \n",
    "        X =ProteinAnalysis(seq)\n",
    "\n",
    "        if \"X\" in seq or \"U\" in seq or \"O\" in seq or \"B\" in seq or \"Z\" in seq:\n",
    "            cleanedSeq = seq.replace(\"X\", \"A\")\n",
    "            cleanedSeq = cleanedSeq.replace(\"U\", \"A\")\n",
    "            cleanedSeq = cleanedSeq.replace(\"O\", \"A\")\n",
    "            cleanedSeq = cleanedSeq.replace(\"B\", \"A\")\n",
    "            cleanedSeq = cleanedSeq.replace(\"Z\", \"A\")\n",
    "            XClean =ProteinAnalysis(cleanedSeq)\n",
    "            flex = XClean.flexibility()\n",
    "            molW = XClean.molecular_weight()\n",
    "            instabIdx = XClean.instability_index()\n",
    "            gravy = XClean.gravy()\n",
    "        else:\n",
    "            flex= X.flexibility()\n",
    "            molW = X.molecular_weight()\n",
    "            instabIdx = X.instability_index()\n",
    "            gravy = X.gravy()\n",
    "\n",
    "        if len(flex)>10:\n",
    "            idx = np.round(np.linspace(0, len(flex) - 1, 10)).astype(int)\n",
    "            flex = np.array(flex)[idx]\n",
    "        elif len(flex)<10:\n",
    "            flex = np.pad(flex, (0,10-len(flex)))\n",
    "\n",
    "        protS= X.protein_scale(aa_dict,100)\n",
    "        if len(protS)>10:\n",
    "            idx = np.round(np.linspace(0, len(protS) - 1, 10)).astype(int)\n",
    "            protS = np.array(protS)[idx]\n",
    "        elif len(protS)<10:\n",
    "            protS = np.pad(protS, (0,10-len(protS)))\n",
    "\n",
    "        #Adding all the physiochemical properties (N = 53)\n",
    "        PCDict[index] = [ molW, X.aromaticity(), instabIdx, *list(X.get_amino_acids_percent().values()),\n",
    "                *flex, gravy, *protS, X.isoelectric_point(), X.charge_at_pH(7), X.charge_at_pH(3), X.charge_at_pH(10), *X.molar_extinction_coefficient(),\n",
    "                *X.secondary_structure_fraction()]\n",
    "        \n",
    "    with open(os.path.join(DATA_PATH, \"PCDictTest\"+\".pkl\"), 'wb') as f:\n",
    "        pickle.dump(PCDict, f)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfd823ba",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f722e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "TRAIN_VAL_SPLIT = 0.7\n",
    "k = 3\n",
    "PCLength = len(PCDict[ids[0]])\n",
    "\n",
    "allAA = list(aa_dict.keys())\n",
    "allAA.sort()\n",
    "allCombinations= list(product(*(allAA for i in range(k))))\n",
    "allCombinations=np.array([''.join(el) for el in allCombinations])\n",
    "\n",
    "positionDict = dict(zip(allCombinations, np.arange(0,allCombinations.size).T))\n",
    "\n",
    "#Use numpy vectorize to speed up the mapping (hopefully)\n",
    "mapping = lambda x: aa_dict[x]\n",
    "vectMapping = np.vectorize(mapping)\n",
    "\n",
    "\n",
    "\n",
    "def generator():\n",
    "  for idxTrain,seqTrain in enumerate(sequences):\n",
    "      entryIdTrain = ids[idxTrain]\n",
    "\n",
    "      kmersTrain = [seqTrain[j:j+k] if j < len(seqTrain)-(k-1) else 0 for j,el in enumerate(seqTrain)]\n",
    "      kmersTrain = kmersTrain[0:-(k-1)]\n",
    "      kmersTrain = [str(el) for el in kmersTrain]\n",
    "      valuesTrain, countsTrain = np.unique(kmersTrain, return_counts=True)\n",
    "      freqVectorTrain=np.zeros(allCombinations.shape)\n",
    "      for lTrain,vTrain in enumerate(valuesTrain):\n",
    "          freqVectorTrain[positionDict[vTrain]] = countsTrain[lTrain]\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "        #supress the warnings for unknown classes\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if entryIdTrain in iprIds:\n",
    "          xTrain  = mlbInterpro.transform([iprIds[entryIdTrain]])\n",
    "        else:\n",
    "          xTrain  = mlbInterpro.transform([[]])\n",
    "      \n",
    "\n",
    "      #Adding all the physiochemical properties (N = 53)\n",
    "      pcPropsTrain = PCDict[entryIdTrain]\n",
    "\n",
    "      t5data = t5df.loc[entryIdTrain].to_numpy()\n",
    "     \n",
    "      yield (entryIdTrain, np.array(pcPropsTrain),xTrain[0],freqVectorTrain, t5data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd15e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq ID: \n",
      "Q9CQV8\n",
      "\n",
      "PC Input: \n",
      "(53,)\n",
      "[2.80861080e+04 7.72357724e-02 4.63687805e+01 8.94308943e-02\n",
      " 8.13008130e-03 5.28455285e-02 1.26016260e-01 2.43902439e-02\n",
      " 4.06504065e-02 8.13008130e-03]\n",
      "\n",
      "Interpro Input: \n",
      "(38293,)\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "kMer Input: \n",
      "(15625,)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "t5 Input: \n",
      "(1024,)\n",
      "[ 0.05470492  0.06342026 -0.01531996 -0.01650625  0.04219466  0.02159161\n",
      " -0.11853468 -0.06329785 -0.04614646 -0.10231141 -0.05789618 -0.00766989\n",
      " -0.04101066  0.04766249  0.04528048 -0.01139968  0.02377699 -0.08064673\n",
      " -0.06053895 -0.04856941]\n",
      "\n",
      "The first sample has 53 Interpro input classes\n",
      "The first sample has 2 kMer input classes\n"
     ]
    }
   ],
   "source": [
    "g = generator()\n",
    "test = next(g)\n",
    "print(\"Seq ID: \\n{}\\n\".format(test[0]))\n",
    "print(\"PC Input: \\n{}\\n{}\\n\".format(test[1].shape, test[1][0:10]))\n",
    "print(\"Interpro Input: \\n{}\\n{}\\n\".format(test[2].shape, test[2][0:10]))\n",
    "print(\"kMer Input: \\n{}\\n{}\\n\".format(test[3].shape, test[3][0:20]))\n",
    "print(\"t5 Input: \\n{}\\n{}\\n\".format(test[4].shape, test[4][0:20]))\n",
    "print(\"The first sample has {} Interpro input classes\".format(np.count_nonzero(test[1])))\n",
    "print(\"The first sample has {} kMer input classes\".format(np.count_nonzero(test[2])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10c4a51a",
   "metadata": {},
   "source": [
    "## Tensorflow Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63f0d2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 17:16:03.565450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-09 17:16:03.596570: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-09 17:16:03.596914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-09 17:16:03.598644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-09 17:16:03.598963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-09 17:16:03.599277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-09 17:16:04.061427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-09 17:16:04.061800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-09 17:16:04.061812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-09 17:16:04.062125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-09 17:16:04.062177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6595 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:2b:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<tf.Tensor: shape=(), dtype=string, numpy=b'Q9CQV8'>, <tf.Tensor: shape=(53,), dtype=float32, numpy=\n",
      "array([ 2.80861074e+04,  7.72357732e-02,  4.63687820e+01,  8.94308910e-02,\n",
      "        8.13008100e-03,  5.28455302e-02,  1.26016259e-01,  2.43902430e-02,\n",
      "        4.06504050e-02,  8.13008100e-03,  4.06504050e-02,  8.13008100e-02,\n",
      "        1.01626016e-01,  3.25203240e-02,  5.69105707e-02,  1.21951215e-02,\n",
      "        6.09756112e-02,  4.06504050e-02,  7.72357732e-02,  4.87804860e-02,\n",
      "        4.47154455e-02,  8.13008100e-03,  4.47154455e-02,  1.02064288e+00,\n",
      "        1.01961899e+00,  9.99678552e-01,  1.04596424e+00,  9.68892872e-01,\n",
      "        1.03008330e+00,  1.05335712e+00,  1.01052380e+00,  1.02227378e+00,\n",
      "        1.05423808e+00, -6.93495929e-01,  9.85148525e+00,  9.86138630e+00,\n",
      "        1.03762379e+01,  1.08811884e+01,  1.02277231e+01,  1.03267326e+01,\n",
      "        1.03663368e+01,  1.01089106e+01,  1.01881189e+01,  9.28712845e+00,\n",
      "        4.77250862e+00, -1.42744741e+01,  3.06518860e+01, -3.24159012e+01,\n",
      "        2.73900000e+04,  2.75150000e+04,  2.64227629e-01,  1.86991870e-01,\n",
      "        3.49593490e-01], dtype=float32)>, <tf.Tensor: shape=(38293,), dtype=int32, numpy=array([0, 0, 0, ..., 0, 0, 0], dtype=int32)>, <tf.Tensor: shape=(15625,), dtype=int32, numpy=array([1, 0, 0, ..., 0, 0, 0], dtype=int32)>, <tf.Tensor: shape=(1024,), dtype=float32, numpy=\n",
      "array([ 0.05470492,  0.06342026, -0.01531996, ..., -0.04331931,\n",
      "        0.03600927,  0.06309301], dtype=float32)>)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(generator, output_signature=(\n",
    "    tf.TensorSpec(shape=(),dtype=tf.dtypes.string),\n",
    "    tf.TensorSpec(shape=(PCLength,), dtype=tf.float32),                 #Physiochemical properties\n",
    "    tf.TensorSpec(shape=(len(mlbInterpro.classes_),), dtype=tf.int32),  #Interpro Classes\n",
    "    tf.TensorSpec(shape=(allCombinations.shape[0],), dtype=tf.int32),   #kMers\n",
    "    tf.TensorSpec(shape=(t5Dimension,), dtype=tf.float32)))              #t5\n",
    "\n",
    "print(list(dataset.take(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9113c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "CCOmodel = tf.keras.saving.load_model(os.path.join(DATA_PATH, \"model_CCO_epoch_13_valF1Score0.5785\"))\n",
    "MFOmodel = tf.keras.saving.load_model(os.path.join(DATA_PATH, \"model_MFO_epoch_18_valF1Score0.5756\"))\n",
    "BPOmodel = tf.keras.saving.load_model(os.path.join(DATA_PATH, \"model_BPO_epoch_18_valF1Score0.3447\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cefc1f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 278/278.0 [05:43<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# probs= CCOmodel.predict(tf.expand_dims(list(dataset.take(64))[0][1], 0))\n",
    "# prediction= [1 if p > 0.5 else 0 for p in probs[0]]\n",
    "# probabilities= probs[probs>0.5]\n",
    "# # classes = np.argwhere(prediction)\n",
    "# print(mlb.inverse_transform(np.array([prediction])))\n",
    "# print(probabilities)\n",
    "\n",
    "BATCHSIZE=512\n",
    "\n",
    "batchedDataset = dataset.batch(BATCHSIZE)\n",
    "tableData=[]\n",
    "\n",
    "# for entries, data in tqdm(batchedDataset):\n",
    "for entries, x_batch_trainPC, x_batch_trainIP, x_batch_trainKmer, x_batch_trainT5 in tqdm(batchedDataset, total=int(np.ceil(len(sequences)/BATCHSIZE))):\n",
    "\n",
    "    probsCCO = CCOmodel.predict_on_batch((x_batch_trainPC, x_batch_trainIP, x_batch_trainKmer, x_batch_trainT5 ))\n",
    "    probsMFO = MFOmodel.predict_on_batch((x_batch_trainPC, x_batch_trainIP, x_batch_trainKmer, x_batch_trainT5 ))\n",
    "    probsBPO = BPOmodel.predict_on_batch((x_batch_trainPC, x_batch_trainIP, x_batch_trainKmer, x_batch_trainT5 ))\n",
    "\n",
    "    for i,prob in enumerate(probsCCO):\n",
    "        prediction = np.where(probsCCO[i] > 0.5, 1, 0)\n",
    "        # prediction= [1 if p > 0.5 else 0 for p in prob]\n",
    "        probabilities= prob[prob>0.5]\n",
    "        entry = entries[i]\n",
    "        GOs = mlbCco.inverse_transform(np.array([prediction]))\n",
    "        for j,g in enumerate(GOs[0]):\n",
    "            tableData.append([entry.numpy().decode(\"utf-8\") , g, probabilities[j]])\n",
    "\n",
    "    for i,prob in enumerate(probsMFO):\n",
    "        prediction = np.where(probsMFO[i] > 0.5, 1, 0)\n",
    "        # prediction= [1 if p > 0.5 else 0 for p in prob]\n",
    "        probabilities= prob[prob>0.5]\n",
    "        entry = entries[i]\n",
    "        GOs = mlbMfo.inverse_transform(np.array([prediction]))\n",
    "        for j,g in enumerate(GOs[0]):\n",
    "            tableData.append([entry.numpy().decode(\"utf-8\") , g, probabilities[j]])\n",
    "\n",
    "    for i,prob in enumerate(probsBPO):\n",
    "        prediction = np.where(probsBPO[i] > 0.5, 1, 0)\n",
    "        # prediction= [1 if p > 0.5 else 0 for p in prob]\n",
    "        probabilities= prob[prob>0.5]\n",
    "        entry = entries[i]\n",
    "        GOs = mlbBpo.inverse_transform(np.array([prediction]))\n",
    "        for j,g in enumerate(GOs[0]):\n",
    "            tableData.append([entry.numpy().decode(\"utf-8\") , g, probabilities[j]])\n",
    "\n",
    "        \n",
    "results = pd.DataFrame(tableData, columns=['Entry ID', 'GO', 'Probability'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79bc939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(os.path.join(DATA_PATH, \"submissionMultiModal.tsv\"), sep=\"\\t\", header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
