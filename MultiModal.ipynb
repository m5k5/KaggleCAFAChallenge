{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "# try:\n",
    "#   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# except:\n",
    "#   # Invalid device or cannot modify virtual devices once initialized.\n",
    "#   pass\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "DATA_PATH_INTERPRO = os.getenv('DATA_PATH_INTERPRO')\n",
    "print(DATA_PATH)\n",
    "print(DATA_PATH_INTERPRO)\n",
    "\n",
    "# Choose subontology (CCO, MFO or BPO)\n",
    "SO = 'BPO'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3544f8a",
   "metadata": {},
   "source": [
    "## Reading fasta, obo and tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b515f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "sequences = [rec.seq for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Train/train_sequences.fasta\"),\"fasta\")]\n",
    "ids = [rec.id for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Train/train_sequences.fasta\"),\"fasta\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2898414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import obonet\n",
    "\n",
    "# Read the taxrank ontology\n",
    "url = os.path.join(DATA_PATH, \"Train/go-basic.obo\")\n",
    "graph = obonet.read_obo(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4bf949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "dfSO = df.loc[df[\"aspect\"]==SO]\n",
    "uniqueTerms = dfSO[\"term\"].unique()\n",
    "termsArr = list(dfSO[\"term\"].to_numpy())\n",
    "\n",
    "uniqueTermsDict={}\n",
    "for i,el in enumerate(uniqueTerms):\n",
    "    uniqueTermsDict[el] = i\n",
    "    \n",
    "print(dfSO.shape)\n",
    "df=dfSO\n",
    "\n",
    "df.set_index(\"EntryID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dec705",
   "metadata": {},
   "outputs": [],
   "source": [
    "testID = df.index.to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGo = pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "dfGo = dfGo.loc[dfGo[\"aspect\"]==SO]\n",
    "\n",
    "dfGo.set_index(\"term\", inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5338b7f8",
   "metadata": {},
   "source": [
    "## GO analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1058ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = df[\"term\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb35584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name = {id_: data.get('name') for id_, data in graph.nodes(data=True)}\n",
    "name_to_id = {data['name']: id_ for id_, data in graph.nodes(data=True) if 'name' in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68179caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SO==\"CCO\":\n",
    "    GOTarget=name_to_id[\"cellular_component\"]\n",
    "elif SO==\"MFO\":\n",
    "    GOTarget=name_to_id[\"molecular_function\"]\n",
    "elif SO==\"BPO\":\n",
    "    GOTarget = name_to_id[\"biological_process\"]\n",
    "\n",
    "def getAllAncestors(go):\n",
    "    try:\n",
    "        paths = networkx.all_simple_paths(\n",
    "            graph,\n",
    "            source=go,\n",
    "            target=GOTarget\n",
    "        )\n",
    "    except:\n",
    "        return []\n",
    "    gos = []\n",
    "    for path in paths:\n",
    "        for node in path:\n",
    "            gos.append(node)\n",
    "    return list(set(gos))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c099fc2",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cae9a3b0",
   "metadata": {},
   "source": [
    "The task is a multilabel classification: The output has several possible targets (Gene Ontologies) but each can only be 1 (existing) or 0 (non existing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8e63ed9",
   "metadata": {},
   "source": [
    "Extract label weights from IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIa = pd.read_csv(os.path.join(DATA_PATH, \"IA.txt\"), sep='\\t', header=None)\n",
    "\n",
    "dfIa.set_index(0, inplace=True)\n",
    "\n",
    "labelWeights=[]\n",
    "allIndices = dfIa.index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "notFound=0\n",
    "for go in item_counts.index.to_list():\n",
    "    if go in allIndices:\n",
    "        labelWeights.append(dfIa.loc[go].to_numpy()[0])\n",
    "    else:\n",
    "        notFound += 1\n",
    "        labelWeights.append(0)\n",
    "\n",
    "print(\"Not found GOs: {} (set to 0)\".format(notFound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeee590",
   "metadata": {},
   "outputs": [],
   "source": [
    "topGOs=item_counts.index.to_list()\n",
    "\n",
    "threshold=0\n",
    "labelWeights=np.array(labelWeights)\n",
    "selection = labelWeights>threshold\n",
    "topGOs=np.array(topGOs)[selection]\n",
    "\n",
    "if os.path.exists(os.path.join(DATA_PATH, \"GODataSizes_\"+SO+\".npy\")):\n",
    "    print(\"Loading presaved data\")\n",
    "    GODataSizes = np.load(os.path.join(DATA_PATH, \"GODataSizes_\"+SO+\".npy\"))\n",
    "else:\n",
    "    GODataSizes= [dfGo.loc[g].size for g in topGOs]\n",
    "    np.save(os.path.join(DATA_PATH, \"GODataSizes_\"+SO), GODataSizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#At least 10 samples\n",
    "print(np.count_nonzero(np.array(GODataSizes)>5))\n",
    "GODataSizes= np.array(GODataSizes)\n",
    "GOsWithSufficientData = topGOs[GODataSizes>5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pickle\n",
    "\n",
    "print(len(topGOs))\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([GOsWithSufficientData])\n",
    "\n",
    "dftest=df.loc[testID]\n",
    "indices = dftest[\"term\"].to_numpy()\n",
    "print(indices)\n",
    "print(mlb.transform([indices]))\n",
    "print(len(mlb.classes_))\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'MLB_'+SO+'.pkl'), 'wb') as f:\n",
    "    pickle.dump(mlb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb189ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelWeightsCorr=[]\n",
    "occurenceScores=[]\n",
    "occurenceDict={}\n",
    "termHist= df[\"term\"].value_counts()\n",
    "maxGoCount = termHist.max()\n",
    "\n",
    "notFound=0\n",
    "for go in mlb.classes_:\n",
    "    if go in allIndices:\n",
    "        occurenceScore = (maxGoCount-termHist[go])/maxGoCount\n",
    "        occurenceScores.append(occurenceScore)\n",
    "        occurenceDict[go] = termHist[go]\n",
    "        labelWeightsCorr.append(dfIa.loc[go].to_numpy()[0])\n",
    "    else:\n",
    "        notFound += 1\n",
    "        labelWeightsCorr.append(0)\n",
    "\n",
    "print(\"Not found GOs: {} (set to 0)\".format(notFound))\n",
    "labelWeightsCorr=np.array(labelWeightsCorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19458edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestorDict = {}\n",
    "goIdxTensors=[]\n",
    "\n",
    "for goIdx, go in tqdm(enumerate(mlb.classes_)):\n",
    "    ancestors = getAllAncestors(go)\n",
    "    ancestors.append(go)\n",
    "    with warnings.catch_warnings():\n",
    "        #supress the warnings for unknown classes\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        fullPath = mlb.transform([ancestors])[0]\n",
    "    fullPathTensor = tf.constant(fullPath)\n",
    "    goIdxTensor = tf.constant(goIdx)\n",
    "    goIdxTensors.append(goIdxTensor)\n",
    "    ancestorDict[goIdxTensor.ref()] = fullPathTensor\n",
    "    \n",
    "# AncestorDictTensor = tf.lookup.KeyValueTensorInitializer(tf.constant(list(ancestorDict.keys())),tf.constant(np.array(list(ancestorDict.values()))))\n",
    "# AncestorDictLookup = tf.lookup.StaticHashTable(AncestorDictTensor,default_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd187102",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestorDict[goIdxTensors[12].ref()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16b39a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.inverse_transform(np.array([ancestorDict[goIdxTensors[12].ref()]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c81df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(ancestorDict.values())).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92019f8c",
   "metadata": {},
   "source": [
    "## Amino acids encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_dict = {'A': 1, 'B':24, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'O': 21, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'U': 22, 'V': 18, 'W': 19, 'Y': 20, 'X':30, 'Z':23}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c2a48",
   "metadata": {},
   "source": [
    "## T5 Embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = np.load(os.path.join(DATA_PATH, \"t5/train_embeds.npy\"))\n",
    "\n",
    "column_num = train_embeddings.shape[1]\n",
    "t5df = pd.DataFrame(\n",
    "    train_embeddings, columns=[\"Column_\" + str(i) for i in range(1, column_num + 1)]\n",
    ")\n",
    "t5Dimension = t5df.shape[1]\n",
    "\n",
    "train_protein_ids = np.load(os.path.join(DATA_PATH, \"t5/train_ids.npy\"))\n",
    "t5df[\"ids\"] = train_protein_ids\n",
    "print(train_protein_ids.shape)\n",
    "print(t5df.shape)\n",
    "t5df.set_index(\"ids\", inplace=True)\n",
    "t5df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d206203f",
   "metadata": {},
   "source": [
    "## Interpro Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43765061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "root = ET.parse(os.path.join(DATA_PATH, \"interpro.xml\")).getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e280b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "possibleDomains=[]\n",
    "allIPTypes=[]\n",
    "for child in root:\n",
    "    if \"type\" in child.attrib:\n",
    "        allIPTypes.append(child.attrib[\"type\"])\n",
    "        if(child.attrib[\"type\"]==\"Domain\" or child.attrib[\"type\"]==\"Repeat\" or child.attrib[\"type\"]==\"Family\" or child.attrib[\"type\"]==\"Homologous_superfamily\"):\n",
    "            # print(child.tag, child.attrib)\n",
    "            possibleDomains.append(child.attrib[\"id\"])\n",
    "\n",
    "set(allIPTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b46e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(possibleDomains)\n",
    "\n",
    "mlbInterPro = MultiLabelBinarizer()\n",
    "mlbInterPro.fit([possibleDomains])\n",
    "\n",
    "\n",
    "print(mlbInterPro.transform([[\"IPR000001\"]]))\n",
    "print(len(mlbInterPro.classes_))\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'MLB_InterPro_'+SO+'.pkl'), 'wb') as f:\n",
    "    pickle.dump(mlbInterPro, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7cd04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "allInterproData =[]\n",
    "\n",
    "for root,dirs,files in os.walk(os.path.join(DATA_PATH_INTERPRO, \"train\")):\n",
    "    for f in files:\n",
    "        if f.endswith(\".json\"):\n",
    "            print(\"Processing \", f)\n",
    "            with open(os.path.join(root, f)) as inputFile:\n",
    "                iprData = json.load(inputFile)\n",
    "            allInterproData=[*allInterproData, *iprData[\"results\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd8d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allInterproData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iprIds = {}\n",
    "\n",
    "\n",
    "for entry in tqdm(allInterproData):\n",
    "    entryId = entry[\"xref\"][0][\"id\"]\n",
    "    matches=[]\n",
    "    for match in entry[\"matches\"]:\n",
    "        sigEntry = match[\"signature\"][\"entry\"]\n",
    "        if(sigEntry):\n",
    "            type = sigEntry[\"type\"]\n",
    "            if type==\"DOMAIN\" or type==\"REPEAT\" or type==\"FAMILY\" or type==\"HOMOLOGOUS_SUPERFAMILY\":\n",
    "                iprId = match[\"signature\"][\"entry\"][\"accession\"]\n",
    "                matches.append(iprId)\n",
    "    iprIds[entryId] = matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2cd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testInput = mlbInterPro.transform([iprIds[\"Q55G04\"]])\n",
    "np.count_nonzero(testInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e073e2",
   "metadata": {},
   "source": [
    "## Physiochemical Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from Bio.Seq import MutableSeq, Seq\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "if os.path.exists(os.path.join(DATA_PATH, \"PCDict\"+\".pkl\")):\n",
    "    print(\"Loading presaved data\")\n",
    "    with open(os.path.join(DATA_PATH, \"PCDict\"+\".pkl\"), 'rb') as f:\n",
    "        PCDict = pickle.load(f)\n",
    "else:\n",
    "    PCDict = {}\n",
    "\n",
    "    for i,seq in enumerate(tqdm(sequences)):\n",
    "\n",
    "        index = ids[i]\n",
    "        \n",
    "        X =ProteinAnalysis(seq)\n",
    "\n",
    "        if \"X\" in seq or \"U\" in seq or \"O\" in seq or \"B\" in seq or \"Z\" in seq:\n",
    "            cleanedSeq = seq.replace(\"X\", \"A\")\n",
    "            cleanedSeq = cleanedSeq.replace(\"U\", \"A\")\n",
    "            cleanedSeq = cleanedSeq.replace(\"O\", \"A\")\n",
    "            cleanedSeq = cleanedSeq.replace(\"B\", \"A\")\n",
    "            cleanedSeq = cleanedSeq.replace(\"Z\", \"A\")\n",
    "            XClean =ProteinAnalysis(cleanedSeq)\n",
    "            flex = XClean.flexibility()\n",
    "            molW = XClean.molecular_weight()\n",
    "            instabIdx = XClean.instability_index()\n",
    "            gravy = XClean.gravy()\n",
    "        else:\n",
    "            flex= X.flexibility()\n",
    "            molW = X.molecular_weight()\n",
    "            instabIdx = X.instability_index()\n",
    "            gravy = X.gravy()\n",
    "\n",
    "        if len(flex)>10:\n",
    "            idx = np.round(np.linspace(0, len(flex) - 1, 10)).astype(int)\n",
    "            flex = np.array(flex)[idx]\n",
    "        elif len(flex)<10:\n",
    "            flex = np.pad(flex, (0,10-len(flex)))\n",
    "\n",
    "        protS= X.protein_scale(aa_dict,100)\n",
    "        if len(protS)>10:\n",
    "            idx = np.round(np.linspace(0, len(protS) - 1, 10)).astype(int)\n",
    "            protS = np.array(protS)[idx]\n",
    "        elif len(protS)<10:\n",
    "            protS = np.pad(protS, (0,10-len(protS)))\n",
    "\n",
    "        #Adding all the physiochemical properties (N = 53)\n",
    "        PCDict[index] = [ molW, X.aromaticity(), instabIdx, *list(X.get_amino_acids_percent().values()),\n",
    "                *flex, gravy, *protS, X.isoelectric_point(), X.charge_at_pH(7), X.charge_at_pH(3), X.charge_at_pH(10), *X.molar_extinction_coefficient(),\n",
    "                *X.secondary_structure_fraction()]\n",
    "        \n",
    "    with open(os.path.join(DATA_PATH, \"PCDict\"+\".pkl\"), 'wb') as f:\n",
    "        pickle.dump(PCDict, f)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfd823ba",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll=pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "soEntries = dfAll.loc[dfAll[\"aspect\"]==SO]\n",
    "soEntryIds = soEntries[\"EntryID\"].unique()\n",
    "\n",
    "print(soEntryIds)\n",
    "\n",
    "dfAll.set_index(\"EntryID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f722e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "TRAIN_VAL_SPLIT = 0.7\n",
    "k = 3\n",
    "PCLength = len(PCDict[ids[0]])\n",
    "\n",
    "allAA = list(aa_dict.keys())\n",
    "allAA.sort()\n",
    "allCombinations= list(product(*(allAA for i in range(k))))\n",
    "allCombinations=np.array([''.join(el) for el in allCombinations])\n",
    "\n",
    "positionDict = dict(zip(allCombinations, np.arange(0,allCombinations.size).T))\n",
    "\n",
    "#Use numpy vectorize to speed up the mapping (hopefully)\n",
    "mapping = lambda x: aa_dict[x]\n",
    "vectMapping = np.vectorize(mapping)\n",
    "\n",
    "# Shuffle the data\n",
    "import random\n",
    "random.seed(516213)\n",
    "c = list(zip(sequences, ids))\n",
    "random.shuffle(c)\n",
    "sequencesShuffle, idsShuffle = zip(*c)\n",
    "\n",
    "\n",
    "#Train Validation Split\n",
    "split = int(np.floor(len(sequencesShuffle)*TRAIN_VAL_SPLIT))\n",
    "print(split)\n",
    "trainSeq = sequencesShuffle[0:split]\n",
    "valSeq = sequencesShuffle[split+1:]\n",
    "trainIds = idsShuffle[0:split]\n",
    "valIds = idsShuffle[split+1:]\n",
    "\n",
    "\n",
    "def generator():\n",
    "  for idxTrain,seqTrain in enumerate(trainSeq):\n",
    "      entryIdTrain = trainIds[idxTrain]\n",
    "      if entryIdTrain in soEntryIds:\n",
    "        labelDataTrain = df.loc[entryIdTrain]\n",
    "        # indices = labelData[\"termToken\"].to_numpy()\n",
    "        indicesTrain = labelDataTrain[\"term\"].to_numpy()\n",
    "      else: \n",
    "        indicesTrain=[]\n",
    "        continue\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "        #supress the warnings for unknown classes\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        yTrain = mlb.transform([indicesTrain])\n",
    "\n",
    "      kmersTrain = [seqTrain[j:j+k] if j < len(seqTrain)-(k-1) else 0 for j,el in enumerate(seqTrain)]\n",
    "      kmersTrain = kmersTrain[0:-(k-1)]\n",
    "      kmersTrain = [str(el) for el in kmersTrain]\n",
    "      valuesTrain, countsTrain = np.unique(kmersTrain, return_counts=True)\n",
    "      freqVectorTrain=np.zeros(allCombinations.shape)\n",
    "      for lTrain,vTrain in enumerate(valuesTrain):\n",
    "          freqVectorTrain[positionDict[vTrain]] = countsTrain[lTrain]\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "        #supress the warnings for unknown classes\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if entryIdTrain in iprIds:\n",
    "          xTrain  = mlbInterPro.transform([iprIds[entryIdTrain]])\n",
    "        else:\n",
    "          xTrain  = mlbInterPro.transform([[]])\n",
    "      \n",
    "\n",
    "      #Adding all the physiochemical properties (N = 53)\n",
    "      pcPropsTrain = PCDict[entryIdTrain]\n",
    "\n",
    "      t5data = t5df.loc[entryIdTrain].to_numpy()\n",
    "     \n",
    "      yield (np.array(pcPropsTrain),xTrain[0],freqVectorTrain, t5data, yTrain[0])\n",
    "\n",
    "\n",
    "def generatorVal():\n",
    "  for idxVal,seqVal in enumerate(valSeq):\n",
    "      entryIdVal = valIds[idxVal]\n",
    "      if entryIdVal in soEntryIds:\n",
    "        labelDataVal = df.loc[entryIdVal]\n",
    "        # indices = labelData[\"termToken\"].to_numpy()\n",
    "        indicesVal = labelDataVal[\"term\"].to_numpy()\n",
    "      else: \n",
    "        indicesVal=[]\n",
    "        continue\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "        #supress the warnings for unknown classes\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        yVal = mlb.transform([indicesVal])\n",
    "\n",
    "      kmersVal = [seqVal[jVal:jVal+k] if jVal < len(seqVal)-(k-1) else 0 for jVal,elVal in enumerate(seqVal)]\n",
    "      kmersVal = kmersVal[0:-(k-1)]\n",
    "      kmersVal = [str(el) for el in kmersVal]\n",
    "      valuesVal, countsVal = np.unique(kmersVal, return_counts=True)\n",
    "      freqVectorVal=np.zeros(allCombinations.shape)\n",
    "      for lVal,vVal in enumerate(valuesVal):\n",
    "          freqVectorVal[positionDict[vVal]] = countsVal[lVal]\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "        #supress the warnings for unknown classes\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if entryIdVal in iprIds:\n",
    "          xVal  = mlbInterPro.transform([iprIds[entryIdVal]])\n",
    "        else:\n",
    "          xVal  = mlbInterPro.transform([[]])\n",
    "\n",
    "      #Adding all the physiochemical properties (N = 53)\n",
    "      pcPropsVal = PCDict[entryIdVal]\n",
    "\n",
    "      t5dataVal = t5df.loc[entryIdVal].to_numpy()\n",
    "      \n",
    "      yield (np.array(pcPropsVal),xVal[0],freqVectorVal, t5dataVal, yVal[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = generatorVal()\n",
    "test = next(g)\n",
    "print(\"PC Input: \\n{}\\n{}\\n\".format(test[0].shape, test[0][0:10]))\n",
    "print(\"Interpro Input: \\n{}\\n{}\\n\".format(test[1].shape, test[1][0:10]))\n",
    "print(\"kMer Input: \\n{}\\n{}\\n\".format(test[2].shape, test[2][0:20]))\n",
    "print(\"t5 Input: \\n{}\\n{}\\n\".format(test[3].shape, test[3][0:20]))\n",
    "print(\"Targets: \\n{}\\n{}\\n\".format(test[4].shape, test[4][0:20]))\n",
    "print(\"The first sample has {} Interpro input classes\".format(np.count_nonzero(test[1])))\n",
    "print(\"The first sample has {} kMer input classes\".format(np.count_nonzero(test[2])))\n",
    "print(\"The first sample has {} output classes\".format(np.count_nonzero(test[4])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10c4a51a",
   "metadata": {},
   "source": [
    "## Tensorflow Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input =  53 physiochemical properties, Interpro Domains, kMers\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(generator, output_signature=(\n",
    "    tf.TensorSpec(shape=(PCLength,), dtype=tf.float32),                 #Physiochemical properties\n",
    "    tf.TensorSpec(shape=(len(mlbInterPro.classes_),), dtype=tf.int32),  #Interpro Classes\n",
    "    tf.TensorSpec(shape=(allCombinations.shape[0],), dtype=tf.int32),   #kMers\n",
    "    tf.TensorSpec(shape=(t5Dimension,), dtype=tf.float32),              #t5\n",
    "    tf.TensorSpec(shape=(len(mlb.classes_),), dtype=tf.int32)))         #GO Classes (Output)\n",
    "\n",
    "datasetVal = tf.data.Dataset.from_generator(generatorVal, output_signature=(\n",
    "    tf.TensorSpec(shape=(PCLength,), dtype=tf.float32),                 #Physiochemical properties\n",
    "    tf.TensorSpec(shape=(len(mlbInterPro.classes_),), dtype=tf.int32),  #Interpro Classes\n",
    "    tf.TensorSpec(shape=(allCombinations.shape[0],), dtype=tf.int32),   #kMers\n",
    "    tf.TensorSpec(shape=(t5Dimension,), dtype=tf.float32),              #t5\n",
    "    tf.TensorSpec(shape=(len(mlb.classes_),), dtype=tf.int32)))         #GO Classes (Output)\n",
    "print(list(datasetVal.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a554f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98752d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "VOCAB_SIZE=len(aa_dict)\n",
    "EMBED_DIM=10\n",
    "\n",
    "def createModel():\n",
    "    inputsPC = tf.keras.Input(shape=(PCLength,))\n",
    "    inputsIP = tf.keras.Input(shape=(len(mlbInterPro.classes_),))\n",
    "    inputsKmer = tf.keras.Input(shape=(allCombinations.shape[0],))\n",
    "    inputsT5 = tf.keras.Input(shape=(t5Dimension,))\n",
    "    \n",
    "    # xT5 = layers.Attention()([inputsT5,inputsT5])\n",
    "    xT5 = layers.Dense(128)(inputsT5)\n",
    "    xT5 = layers.LeakyReLU()(xT5)\n",
    "    xT5 = layers.BatchNormalization()(xT5)\n",
    "    xT5Res = layers.Dropout(0.1)(xT5)\n",
    "    xT5 = layers.Dense(128)(xT5Res)\n",
    "    xT5 = layers.LeakyReLU()(xT5)\n",
    "    xT5 = layers.BatchNormalization()(xT5)\n",
    "    xT5 = layers.Dropout(0.1)(xT5)\n",
    "    xT5 = tf.concat([xT5,xT5Res],1)\n",
    "\n",
    "    # xPC = layers.Attention()([inputsPC,inputsPC])\n",
    "    xPC = layers.Dense(128)(inputsPC)\n",
    "    xPC = layers.LeakyReLU()(xPC)\n",
    "    xPC = layers.BatchNormalization()(xPC)\n",
    "    xPCRes = layers.Dropout(0.1)(xPC)\n",
    "    xPC = layers.Dense(128)(xPCRes)\n",
    "    xPC = layers.LeakyReLU()(xPC)\n",
    "    xPC = layers.BatchNormalization()(xPC)\n",
    "    xPC = layers.Dropout(0.1)(xPC)\n",
    "    xPC = tf.concat([xPC,xPCRes],1)\n",
    "\n",
    "    # xIP = layers.Attention()([inputsIP,inputsIP])\n",
    "    xIP = layers.Dense(128)(inputsIP)\n",
    "    xIP = layers.LeakyReLU()(xIP)\n",
    "    xIP = layers.BatchNormalization()(xIP)\n",
    "    xIPRes = layers.Dropout(0.1)(xIP)\n",
    "    xIP = layers.Dense(128)(xIPRes)\n",
    "    xIP = layers.LeakyReLU()(xIP)\n",
    "    xIP = layers.BatchNormalization()(xIP)\n",
    "    xIP = layers.Dropout(0.1)(xIP)\n",
    "    xIP = tf.concat([xIP,xIPRes],1)\n",
    "\n",
    "    # xKmer = layers.Attention()([inputsKmer,inputsKmer])\n",
    "    xKmer = layers.Dense(128)(inputsKmer)\n",
    "    xKmer = layers.LeakyReLU()(xKmer)\n",
    "    xKmer = layers.BatchNormalization()(xKmer)\n",
    "    xKmerRes = layers.Dropout(0.1)(xKmer)\n",
    "    xKmer = layers.Dense(128)(xKmerRes)\n",
    "    xKmer = layers.LeakyReLU()(xKmer)\n",
    "    xKmer = layers.BatchNormalization()(xKmer)\n",
    "    xKmer = layers.Dropout(0.1)(xKmer)\n",
    "    xKmer = tf.concat([xKmer,xKmerRes],1)\n",
    "\n",
    "    concat = tf.concat([xPC,xIP,xKmer,xT5],1)\n",
    "\n",
    "    x = layers.Dense(256)(concat)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    outputs=layers.Dense(len(mlb.classes_), activation=tf.keras.activations.sigmoid)(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputsPC, inputsIP, inputsKmer, inputsT5], outputs=outputs, name=\"DenseMultiModal4\")\n",
    "\n",
    "model = createModel()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "dot_img_file = './model_1.png'\n",
    "tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ddc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "decaySteps=5000\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate, first_decay_steps=decaySteps,\n",
    "                                                                t_mul=2.0, m_mul=0.7)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "#     initial_learning_rate, decay_steps=decaySteps, alpha=0.01)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate,decay_steps=decaySteps,decay_rate=0.9,staircase=False)\n",
    "step = np.linspace(0,decaySteps*3)\n",
    "lr = lr_schedule(step)\n",
    "plt.figure(figsize = (8,6))\n",
    "# plt.yscale(\"log\")\n",
    "plt.plot(step, lr)\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.xlabel('step')\n",
    "_ = plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f43e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE=64\n",
    "LOG_INTERVAL=20\n",
    "epochs = 20\n",
    "saveModel=True\n",
    "\n",
    "\n",
    "log_dir = \"./logs/\"+model.name+\"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_\"+SO\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,\n",
    "                                                      write_graph=True, update_freq=5)\n",
    "\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# Instantiate an optimizer .\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule)\n",
    "\n",
    "# Instantiate a loss function.\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "# loss_fn = WeightedBinaryCE(np.ones(len(mlb.classes_)))\n",
    "# loss_fn = WeightedBinaryCE(labelWeightsCorr)\n",
    "# loss_fn = WeightedComboLoss(labelWeightsCorr, alpha=0.5, beta=0.5, labelSmoothing=0.05)\n",
    "loss_fn = WeightedComboLoss(labelWeightsCorr/np.max(labelWeightsCorr)+occurenceScores, alpha=0.5, beta=0.5)\n",
    "# loss_fn = tf.keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True, gamma=2)\n",
    "\n",
    "train_acc_metric = WeightedAccuracy(classWeights=labelWeightsCorr)\n",
    "train_f1_metric = WeightedF1(classWeights=labelWeightsCorr, threshold=0.5)\n",
    "train_prec = WeightedPrecision(classWeights=labelWeightsCorr)\n",
    "train_rec = WeightedRecall(classWeights=labelWeightsCorr)\n",
    "\n",
    "val_acc_metric = WeightedAccuracy(classWeights=labelWeightsCorr)\n",
    "val_f1_metric = WeightedF1(classWeights=labelWeightsCorr, threshold=0.5)\n",
    "val_prec = WeightedPrecision(classWeights=labelWeightsCorr)\n",
    "val_rec = WeightedRecall(classWeights=labelWeightsCorr)\n",
    "\n",
    "batchedDataset = dataset.batch(BATCH_SIZE, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "batchedDatasetVal = datasetVal.batch(BATCH_SIZE, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# batchedDataset = batchedDataset.cache(os.path.join(DATA_PATH, \"datasetCache\"+SO))\n",
    "# batchedDatasetVal = batchedDatasetVal.cache(os.path.join(DATA_PATH, \"datasetCacheVal\"+SO))\n",
    "\n",
    "@tf.function()\n",
    "def trainStep(x_batch_train, y_batch_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = model(x_batch_train, training=True) \n",
    "\n",
    "        #Add all ancestor GOs based on the predictions\n",
    "        # probsNew = tf.TensorArray(tf.float32, size=probs.shape[0])\n",
    "        # for probsIdx in range(probs.shape[0]):\n",
    "        #     predictedIdx = tf.where(tf.math.greater(probs[probsIdx], 0.5))\n",
    "        #     predictedProbs = tf.gather_nd(probs[probsIdx], predictedIdx)\n",
    "        #     # print(predictedIdx.numpy(), predictedProbs.numpy())\n",
    "        #     vectToAdd = tf.TensorArray(tf.float32, size=predictedIdx.shape[0])\n",
    "        #     for addingIdx, predIdx in enumerate(predictedIdx):\n",
    "        #         ancestorsArr = ancestorDict[goIdxTensors[predIdx.numpy()[0]].ref()]\n",
    "        #         vectToAdd.write(addingIdx, tf.math.multiply(tf.cast(ancestorsArr, tf.float32), predictedProbs[addingIdx])).mark_used()\n",
    "        #     vectToAddTensors = vectToAdd.stack()\n",
    "            # print(vectToAddTensors.numpy())\n",
    "            # probsNew.write(probsIdx, tf.math.reduce_max(vectToAddTensors,axis=0)).mark_used()\n",
    "        # probsNewTensor = probsNew.stack()\n",
    "        \n",
    "        loss_value = loss_fn(y_batch_train, probs)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "    #Gradient clipping\n",
    "    # grads = [tf.clip_by_norm(g, 2.0) for g in grads]\n",
    "\n",
    "    train_acc_metric.update_state(y_batch_train, probs)\n",
    "    train_f1_metric.update_state(y_batch_train, probs)\n",
    "    train_prec.update_state(y_batch_train, probs)\n",
    "    train_rec.update_state(y_batch_train, probs)\n",
    "\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "    return loss_value\n",
    "\n",
    "@tf.function()\n",
    "def valStep(x_batch_val, y_batch_val):\n",
    "    valProbs = model(x_batch_val, training=False)\n",
    "    # Update val metrics\n",
    "    val_acc_metric.update_state(y_batch_val, valProbs)\n",
    "    val_f1_metric.update_state(y_batch_val, valProbs)\n",
    "    val_prec.update_state(y_batch_val, valProbs)\n",
    "    val_rec.update_state(y_batch_val, valProbs)\n",
    "\n",
    "maxStep=0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch+1,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_trainPC, x_batch_trainIP, x_batch_trainKmer, x_batch_trainT5, y_batch_train) in enumerate(batchedDataset):\n",
    "        loss_value =trainStep((x_batch_trainPC, x_batch_trainIP, x_batch_trainKmer, x_batch_trainT5),y_batch_train)\n",
    "\n",
    "        # Log \n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            template = 'Epoch {}/Step {}, Loss: {:.5f}, Accuracy: {:.5f}, F1: {:.4f}, Prec: {:.4f}, Rec: {:.4f}, lr: {:.5f}'\n",
    "            print(template.format(epoch+1, step,loss_value.numpy(), \n",
    "                                    train_acc_metric.result(),train_f1_metric.result(),\n",
    "                                    train_prec.result(), train_rec.result(), optimizer.learning_rate.numpy()))\n",
    "            \n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', loss_value, step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('accuracy', train_acc_metric.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('f1', train_f1_metric.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('prec', train_prec.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('rec', train_rec.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('learning rate', optimizer.learning_rate.numpy(), step=maxStep*epoch+step)\n",
    "                summary_writer.flush()\n",
    "\n",
    "    \n",
    "    train_acc_metric.reset_states()\n",
    "    train_f1_metric.reset_states()\n",
    "    train_prec.reset_states()\n",
    "    train_rec.reset_states()\n",
    "\n",
    "    maxStep=step\n",
    "\n",
    "    print(\"Epoch finished. Start validation\")\n",
    "    for x_batch_valPC, x_batch_valIP, x_batch_valKmer, x_batchVal_trainT5, y_batch_val in batchedDatasetVal:\n",
    "        valStep((x_batch_valPC, x_batch_valIP, x_batch_valKmer, x_batchVal_trainT5), y_batch_val)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    val_f1 = val_f1_metric.result()\n",
    "    val_f1_metric.reset_states()\n",
    "    val_precision = val_prec.result()\n",
    "    val_prec.reset_states()\n",
    "    val_recall = val_rec.result()\n",
    "    val_rec.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Validation f1: %.4f\" % (float(val_f1),))\n",
    "    print(\"Validation precision: %.4f\" % (float(val_precision),))\n",
    "    print(\"Validation recall: %.4f\" % (float(val_recall),))\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('valAcc', float(val_acc), step=epoch)\n",
    "        tf.summary.scalar('valF1', float(val_f1), step=epoch)\n",
    "        tf.summary.scalar('valPrecision', float(val_precision), step=epoch)\n",
    "        tf.summary.scalar('valRecall', float(val_recall), step=epoch)\n",
    "        summary_writer.flush()\n",
    "    if saveModel:\n",
    "      model.save(os.path.join(DATA_PATH, \"model_\"+SO+\"_epoch_{}_valF1Score{:.4f}\".format(epoch, float(val_f1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.save(os.path.join(DATA_PATH, \"model_\"+SO+\"_epoch_{}_valf1Score{:.3f}\".format(epoch, float(val_f1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e389b420",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64376b1",
   "metadata": {},
   "source": [
    "Which GO classes are most misclassified? Is it correlated with the class imbalance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "layerGOs={}\n",
    "for layer, nodes in enumerate(networkx.topological_generations(graph)):\n",
    "    # layerGOs[layer] = nodes\n",
    "    for n in nodes:\n",
    "        layerGOs[n] = layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# probs= model.predict(tf.expand_dims(list(datasetVal.take(32))[10][0], 0))\n",
    "# prediction= [1 if p > 0.5 else 0 for p in probs[0]]\n",
    "# probabilities= probs[probs>0.5]\n",
    "# # classes = np.argwhere(prediction)\n",
    "# print(mlb.inverse_transform(np.array([prediction])))\n",
    "# print(probabilities)\n",
    "\n",
    "\n",
    "tp=[]\n",
    "fp=[]\n",
    "fn=[]\n",
    "\n",
    "for i, (xPC, xIP, xKmer,xt5, y) in enumerate(tqdm(batchedDatasetVal)):\n",
    "    probsArr = model((xPC, xIP, xKmer, xt5), training=False)\n",
    "    for idxProbs, probs in enumerate(probsArr):\n",
    "        probs = probs.numpy()\n",
    "        prediction= [1 if p > 0.5 else 0 for p in probs]\n",
    "        predClasses = mlb.inverse_transform(np.array([prediction]))[0]\n",
    "        trueClasses = mlb.inverse_transform(np.array([y[idxProbs,:]]))[0]\n",
    "        for pred in predClasses:\n",
    "            if pred in trueClasses:\n",
    "                tp.append(pred)\n",
    "            else:\n",
    "                fp.append(pred)\n",
    "        for trueClass in trueClasses:\n",
    "            if not trueClass in predClasses:\n",
    "                fn.append(trueClass)\n",
    "    \n",
    "    if i>100:\n",
    "        break\n",
    "\n",
    "tpValues, tpCounts = np.unique(tp, return_counts=True)\n",
    "fpValues, fpCounts = np.unique(fp, return_counts=True)\n",
    "fnValues, fnCounts = np.unique(fn, return_counts=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13486ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in tpValues[tpCounts>600]:\n",
    "    try:\n",
    "        print(n, \": \", layerGOs[n])\n",
    "    except:\n",
    "        print(\"failed for \", n)\n",
    "    \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(tpValues[tpCounts>600], tpCounts[tpCounts>600])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"True Positives\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist([layerGOs[tpVal] for tpVal in tpValues])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Histogram of layer depth of True Positives\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist([occurenceDict[tpVal] for tpVal in tpValues], bins=50, range=(0,10000))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Histogram of occurence count of True Positives\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in fpValues[fpCounts > 200]:\n",
    "    print(n, \": \", layerGOs[n])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(fpValues[fpCounts > 200], fpCounts[fpCounts > 200])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"False Positives\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist([layerGOs[fpVal] for fpVal in fpValues[fpCounts > 10]])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Histogram of layer depth of False Positives with more than 10 occurences\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist([occurenceDict[fpVal] for fpVal in fpValues], bins=50, range=(0,10000))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\n",
    "    \"Histogram of occurence counts of False Positives\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in fnValues[fnCounts>200]:\n",
    "    print(n, \": \", layerGOs[n])\n",
    "    \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(fnValues[fnCounts>200], fnCounts[fnCounts>200])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"False Negatives\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist([layerGOs[fnVal] for fnVal in fnValues[fnCounts>10] ])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Histogram of layer depth of False Negatives with more than 10 occurences\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist([occurenceDict[fnVal] for fnVal in fnValues], bins=50, range=(0,10000))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Histogram of occurence count of False Negatives\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2168ef4",
   "metadata": {},
   "source": [
    "- The GO terms that are true positives most of the time are from different layers in the GO graph. There is no tendency that the model is better at classifying GOs at a higher layer, i.e. are more general.\n",
    "- The GO terms that are true positive have different frequencies in the dataset. But there is a tendency that true positives are the less common GOs, which is not an expected behavior.\n",
    "- The GO terms that are false positives or false negatives show a clear tendency to occur in a lower layer of the GO graph (which is more specific). But there are also some errors in the higher layers.\n",
    "- The GO terms that are false positives or false negatives show a clear tendency to occur in the classes that are less common in the dataset. So this is an indicator that class imbalance is a problem here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
