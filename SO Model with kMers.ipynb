{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "# try:\n",
    "#   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# except:\n",
    "#   # Invalid device or cannot modify virtual devices once initialized.\n",
    "#   pass\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "print(DATA_PATH)\n",
    "\n",
    "# Choose subontology (CCO, MFO or BPO)\n",
    "SO = 'BPO'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3544f8a",
   "metadata": {},
   "source": [
    "## Reading fasta, obo and tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b515f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "sequences = [rec.seq for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Train/train_sequences.fasta\"),\"fasta\")]\n",
    "ids = [rec.id for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Train/train_sequences.fasta\"),\"fasta\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2898414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import obonet\n",
    "\n",
    "# Read the taxrank ontology\n",
    "url = os.path.join(DATA_PATH, \"Train/go-basic.obo\")\n",
    "graph = obonet.read_obo(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4bf949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "dfSO = df.loc[df[\"aspect\"]==SO]\n",
    "uniqueTerms = dfSO[\"term\"].unique()\n",
    "termsArr = list(dfSO[\"term\"].to_numpy())\n",
    "\n",
    "uniqueTermsDict={}\n",
    "for i,el in enumerate(uniqueTerms):\n",
    "    uniqueTermsDict[el] = i\n",
    "    \n",
    "print(dfSO.shape)\n",
    "df=dfSO\n",
    "\n",
    "df.set_index(\"EntryID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dec705",
   "metadata": {},
   "outputs": [],
   "source": [
    "testID = df.index.to_list()[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5338b7f8",
   "metadata": {},
   "source": [
    "## GO analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1058ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = df[\"term\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb35584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name = {id_: data.get('name') for id_, data in graph.nodes(data=True)}\n",
    "name_to_id = {data['name']: id_ for id_, data in graph.nodes(data=True) if 'name' in data}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c099fc2",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cae9a3b0",
   "metadata": {},
   "source": [
    "The task is a multilabel classification: The output has several possible targets (Gene Ontologies) but each can only be 1 (existing) or 0 (non existing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8e63ed9",
   "metadata": {},
   "source": [
    "Extract label weights from IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIa = pd.read_csv(os.path.join(DATA_PATH, \"IA.txt\"), sep='\\t', header=None)\n",
    "\n",
    "dfIa.set_index(0, inplace=True)\n",
    "\n",
    "labelWeights=[]\n",
    "allIndices = dfIa.index.tolist()\n",
    "\n",
    "# if SO == \"BPO\":\n",
    "#     item_counts=item_counts[0:10]\n",
    "#     # item_counts=item_counts[0:len(item_counts)//2]\n",
    "\n",
    "notFound=0\n",
    "for go in item_counts.index.to_list():\n",
    "    if go in allIndices:\n",
    "        labelWeights.append(dfIa.loc[go].to_numpy()[0])\n",
    "    else:\n",
    "        notFound += 1\n",
    "        labelWeights.append(0)\n",
    "\n",
    "print(\"Not found GOs: {} (set to 0)\".format(notFound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pickle\n",
    "\n",
    "topGOs= item_counts\n",
    "topGOs=topGOs.index.to_list()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([topGOs])\n",
    "\n",
    "dftest=df.loc[testID]\n",
    "indices = dftest[\"term\"].to_numpy()\n",
    "print(indices)\n",
    "print(mlb.transform([indices]))\n",
    "print(len(mlb.classes_))\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'MLB_'+SO+'.pkl'), 'wb') as f:\n",
    "    pickle.dump(mlb, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92019f8c",
   "metadata": {},
   "source": [
    "## Amino acids encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aminos_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_dict = {'A': 1, 'B':24, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'O': 21, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'U': 22, 'V': 18, 'W': 19, 'Y': 20, 'X':30, 'Z':23}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfd823ba",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqLengths = [len(seq) for seq in sequences]\n",
    "maxLen = max(seqLengths)\n",
    "print(\"The max. length of the sequences is {}\".format(maxLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e581e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfAll=pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "soEntries = dfAll.loc[dfAll[\"aspect\"]==SO]\n",
    "soEntryIds = soEntries[\"EntryID\"].unique()\n",
    "\n",
    "# print(len(seqEntries))\n",
    "print(soEntryIds)\n",
    "\n",
    "# SoSequences = []\n",
    "# for entry in soEntryIds:\n",
    "#     SoSequences.append(sequences[ids.index(entry)])\n",
    "\n",
    "# print(len(SoSequences))\n",
    "dfAll.set_index(\"EntryID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f722e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "TRAIN_VAL_SPLIT = 0.7\n",
    "k = 3\n",
    "\n",
    "allAA = list(aa_dict.keys())\n",
    "allAA.sort()\n",
    "allCombinations= list(product(*(allAA for i in range(k))))\n",
    "allCombinations=np.array([''.join(el) for el in allCombinations])\n",
    "\n",
    "positionDict = dict(zip(allCombinations, np.arange(0,allCombinations.size).T))\n",
    "\n",
    "#Use numpy vectorize to speed up the mapping (hopefully)\n",
    "mapping = lambda x: aa_dict[x]\n",
    "vectMapping = np.vectorize(mapping)\n",
    "\n",
    "# Shuffle the data\n",
    "import random\n",
    "random.seed(516213)\n",
    "c = list(zip(sequences, ids))\n",
    "random.shuffle(c)\n",
    "sequencesShuffle, idsShuffle = zip(*c)\n",
    "\n",
    "\n",
    "#Train Validation Split\n",
    "split = int(np.floor(len(sequencesShuffle)*TRAIN_VAL_SPLIT))\n",
    "print(split)\n",
    "trainSeq = sequencesShuffle[0:split]\n",
    "valSeq = sequencesShuffle[split+1:]\n",
    "trainIds = idsShuffle[0:split]\n",
    "valIds = idsShuffle[split+1:]\n",
    "\n",
    "\n",
    "def generator():\n",
    "  for i,seq in enumerate(trainSeq):\n",
    "      entryId = trainIds[i]\n",
    "      \n",
    "      if entryId in soEntryIds:\n",
    "        labelData = df.loc[entryId]\n",
    "        # indices = labelData[\"termToken\"].to_numpy()\n",
    "        indices = labelData[\"term\"].to_numpy()\n",
    "      else:\n",
    "        indices=[]\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "          #supress the warnings for unknown classes\n",
    "          warnings.simplefilter(\"ignore\")\n",
    "          y = mlb.transform([indices])\n",
    "      \n",
    "      \n",
    "      kmers = [seq[i:i+k] if i < len(seq)-(k-1) else 0 for i,el in enumerate(seq)]\n",
    "      kmers = kmers[0:-(k-1)]\n",
    "      kmers = [str(el) for el in kmers]\n",
    "      values, counts = np.unique(kmers, return_counts=True)\n",
    "      freqVector=np.zeros(allCombinations.shape)\n",
    "      for j,v in enumerate(values):\n",
    "          freqVector[positionDict[v]] = counts[j]\n",
    "      yield (freqVector,y[0])\n",
    "\n",
    "\n",
    "def generatorVal():\n",
    "  for i,seq in enumerate(valSeq):\n",
    "      entryId = valIds[i]\n",
    "      if entryId in soEntryIds:\n",
    "        labelData = df.loc[entryId]\n",
    "        # indices = labelData[\"termToken\"].to_numpy()\n",
    "        indices = labelData[\"term\"].to_numpy()\n",
    "      else:\n",
    "        indices=[]\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "          #supress the warnings for unknown classes\n",
    "          warnings.simplefilter(\"ignore\")\n",
    "          y = mlb.transform([indices])\n",
    "      \n",
    "      kmers = [seq[i:i+k] if i < len(seq)-(k-1) else 0 for i,el in enumerate(seq)]\n",
    "      kmers = kmers[0:-(k-1)]\n",
    "      kmers = [str(el) for el in kmers]\n",
    "      values, counts = np.unique(kmers, return_counts=True)\n",
    "      freqVector=np.zeros(allCombinations.shape)\n",
    "      for j,v in enumerate(values):\n",
    "          freqVector[positionDict[v]] = counts[j]\n",
    "      yield (freqVector,y[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = generator()\n",
    "test = next(g)\n",
    "print(\"The first sample: \\n{}\\n{}\".format(test[0].shape, test[0][0:100]))\n",
    "print(\"The first sample has {} classes\".format(np.count_nonzero(test[1])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10c4a51a",
   "metadata": {},
   "source": [
    "## Tensorflow Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xSize = allCombinations.shape[0]\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(generator, output_signature=(\n",
    "         tf.TensorSpec(shape=(xSize,), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(len(mlb.classes_),), dtype=tf.int32)))\n",
    "print(list(dataset.take(1)))\n",
    "\n",
    "datasetVal = tf.data.Dataset.from_generator(generatorVal, output_signature=(\n",
    "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(len(mlb.classes_),), dtype=tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a554f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98752d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "VOCAB_SIZE=len(aa_dict)\n",
    "EMBED_DIM=100\n",
    "\n",
    "def createModel():\n",
    "    inputs = tf.keras.Input(shape=(xSize,))\n",
    "\n",
    "    x=layers.Conv1D(8, 7,strides=2, activation=tf.keras.activations.relu)(tf.expand_dims(inputs,2))\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x=layers.Conv1D(8, 7,strides=2, activation=tf.keras.activations.relu)(x)\n",
    "    x=layers.Conv1D(8, 7, strides=2, activation=tf.keras.activations.relu)(x)\n",
    "    x=layers.Conv1D(8, 7, activation=tf.keras.activations.relu)(x)\n",
    "    x=layers.Conv1D(16, 7, activation=tf.keras.activations.relu)(x)\n",
    "    x=layers.Conv1D(16, 7, activation=tf.keras.activations.relu)(x)\n",
    "    x=layers.Conv1D(16, 7, activation=tf.keras.activations.relu)(x)\n",
    "    x=layers.Conv1D(16, 7, activation=tf.keras.activations.relu)(x)\n",
    "\n",
    "\n",
    "    # x=layers.Conv1D(32, 5, activation=tf.keras.activations.relu)(x)\n",
    "    # x=layers.Conv1D(32, 5, activation=tf.keras.activations.relu)(x)\n",
    "    # x=layers.Conv1D(32, 5, activation=tf.keras.activations.relu)(x)\n",
    "    x=layers.Flatten()(x)\n",
    "    x=layers.Dense(32)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x=layers.Dense(32)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    outputs=layers.Dense(len(mlb.classes_), activation=tf.keras.activations.sigmoid)(x)\n",
    "    # outputs=layers.Softmax()(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"embedConvModel\")\n",
    "\n",
    "model = createModel()\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73140569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VOCAB_SIZE=len(aa_dict)\n",
    "EMBED_DIM=500\n",
    "OUT_SEQ_LENGTH=10\n",
    "\n",
    "def createRnnModel():\n",
    "    inputs = tf.keras.Input(shape=(TRUNCATE,))\n",
    "    # x = tf.keras.layers.Masking(0)(inputs)\n",
    "    x=layers.Embedding(VOCAB_SIZE, EMBED_DIM, mask_zero=True, name=\"embedding\")(inputs)\n",
    "\n",
    "    # x = layers.RepeatVector(OUT_SEQ_LENGTH)(x)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(200, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(200))(x)\n",
    "    # x = layers.LSTM(64)(x)\n",
    "    x = layers.Dense(500)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dense(500)(x)\n",
    "    outputs=layers.Dense(len(mlb.classes_), activation=tf.keras.activations.sigmoid)(x)\n",
    "    # outputs=layers.Softmax()(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"embedRnnModel\")\n",
    "\n",
    "# model = createRnnModel()\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ddc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "decaySteps=5000\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate, first_decay_steps=decaySteps,\n",
    "                                                                t_mul=2.0, m_mul=0.7)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "#     initial_learning_rate, decay_steps=decaySteps, alpha=0.01)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate,decay_steps=decaySteps,decay_rate=0.9,staircase=False)\n",
    "step = np.linspace(0,decaySteps*3)\n",
    "lr = lr_schedule(step)\n",
    "plt.figure(figsize = (8,6))\n",
    "# plt.yscale(\"log\")\n",
    "plt.plot(step, lr)\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.xlabel('step')\n",
    "_ = plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4760b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "class WeightedBinaryCE(Loss):\n",
    "    # initialize instance attributes\n",
    "    def __init__(self, classWeights, labelSmoothing=0.0):\n",
    "        super(WeightedBinaryCE, self).__init__()\n",
    "        self.labelSmoothing = tf.constant(labelSmoothing, dtype=tf.dtypes.float32)\n",
    "        self.classWeights= tf.constant(classWeights, dtype=tf.dtypes.float32)\n",
    "        \n",
    "    # Compute loss\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        if(self.labelSmoothing>0):\n",
    "            y_true = y_true*(1-self.labelSmoothing)+self.labelSmoothing/2\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred,tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        term_0 = tf.math.multiply(self.classWeights * tf.math.subtract(1.0 , y_true), tf.math.log(tf.math.subtract(1.0, y_pred) + tf.keras.backend.epsilon()))\n",
    "        term_1 = tf.math.multiply(self.classWeights * y_true, tf.math.log(y_pred + tf.keras.backend.epsilon()))\n",
    "        losses = term_0 + term_1\n",
    "        return -tf.reduce_mean(losses, axis=0)\n",
    "    \n",
    "\n",
    "testLoss = WeightedBinaryCE([1,1,1])\n",
    "labelsTrue = tf.constant([[1,0,1], [1,1,0]])\n",
    "labelsPred = tf.constant([[0.42,0.111,0.957], [0.877,0.121,0.544]])\n",
    "\n",
    "print(testLoss(labelsTrue, labelsPred))\n",
    "kerasCE = tf.keras.losses.BinaryCrossentropy()\n",
    "print(kerasCE(labelsTrue, labelsPred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DiceLoss(Loss):\n",
    "    # initialize instance attributes\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        \n",
    "    # Compute loss\n",
    "    def call(self, y_true, y_pred, smooth=1e-6):\n",
    "      inputs = tf.squeeze(tf.cast(y_pred, tf.dtypes.float32))\n",
    "      targets = tf.squeeze(tf.cast(y_true, tf.dtypes.float32))\n",
    "    \n",
    "      intersection = tf.math.reduce_sum(tf.math.multiply(targets, inputs))\n",
    "      dice = (2*intersection + smooth) / (tf.math.reduce_sum(targets) + tf.math.reduce_sum(inputs) + smooth)\n",
    "      return 1 - dice\n",
    "    \n",
    "\n",
    "testLoss = DiceLoss()\n",
    "labelsTrue = tf.constant([[1,0,1], [1,1,0]])\n",
    "labelsPred = tf.constant([[0.999,0.0001,0.99], [0.9,0.921,0.01]])\n",
    "\n",
    "print(testLoss(labelsTrue, labelsPred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# α controls the amount of Dice term contribution in the loss f\n",
    "# β ∈ [0, 1] controls the level of model penalization for false positives/negatives: when β is set to\n",
    "# a value smaller than 0.5, FP are penalized more than FN\n",
    "\n",
    "\n",
    "class WeightedComboLoss(Loss):\n",
    "    # initialize instance attributes\n",
    "    def __init__(self, labelWeights, alpha=0.5, beta=0.5):\n",
    "        super(WeightedComboLoss, self).__init__()\n",
    "        self.classWeights = tf.constant(labelWeights, dtype=tf.dtypes.float32)\n",
    "        self.alpha=tf.constant(alpha, dtype=tf.dtypes.float32)\n",
    "        self.beta=tf.constant(beta, dtype=tf.dtypes.float32)\n",
    "        \n",
    "    # Compute loss\n",
    "    def call(self, y_true, y_pred, smooth=1e-6):\n",
    "      inputs = tf.squeeze(tf.cast(y_pred, tf.dtypes.float32))\n",
    "      targets = tf.squeeze(tf.cast(y_true, tf.dtypes.float32))\n",
    "    \n",
    "      intersection = tf.math.reduce_sum(tf.math.multiply(targets, inputs))\n",
    "      dice = (2*intersection + smooth) / (tf.math.reduce_sum(targets) + tf.math.reduce_sum(inputs) + smooth)\n",
    "     \n",
    "      y_true = tf.cast(y_true, tf.float32)\n",
    "      y_pred = tf.cast(y_pred, tf.float32)\n",
    "      y_pred = tf.clip_by_value(y_pred,tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "      term_0 = tf.math.multiply((1-self.beta) * self.classWeights * tf.math.subtract(1.0 , y_true), tf.math.log(tf.math.subtract(1.0, y_pred) + tf.keras.backend.epsilon()))\n",
    "      term_1 = tf.math.multiply(self.beta * self.classWeights * y_true, tf.math.log(y_pred + tf.keras.backend.epsilon()))\n",
    "      losses = term_0 + term_1\n",
    "      weightedCE = -tf.reduce_mean(losses, axis=0)\n",
    "\n",
    "      combo = (self.alpha * weightedCE) - ((1 - self.alpha) * dice)\n",
    "\n",
    "      return combo\n",
    "    \n",
    "\n",
    "testLoss = WeightedComboLoss([1,1,1])\n",
    "labelsTrue = tf.constant([[1,0,1], [1,1,0]])\n",
    "labelsPred = tf.constant([[0.9,0.1,0.9], [0.8,0.9,0.05]])\n",
    "\n",
    "print(testLoss(labelsTrue, labelsPred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7880ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WeightedF1(tf.keras.metrics.Metric):\n",
    "    def __init__(self, classWeights, threshold=0.5):\n",
    "        super(WeightedF1, self).__init__()\n",
    "        self.classWeights= tf.constant(classWeights, dtype=tf.dtypes.float32)\n",
    "        self.threshold= tf.constant(threshold, dtype=tf.dtypes.float32)\n",
    "        self.f1 = self.add_weight(name='f1', initializer='zeros')\n",
    "        self.total = self.add_weight('total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        m = tf.math.count_nonzero(tf.reduce_max(y_pred, axis=1))\n",
    "        n = tf.shape(y_pred)[0]\n",
    "        y_true = tf.cast(y_true, tf.bool)\n",
    "        y_pred = tf.math.greater_equal(y_pred,self.threshold)\n",
    "   \n",
    "\n",
    "        tp = tf.math.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n",
    "        tp = tf.cast(tp, self.dtype)\n",
    "        tp = tf.math.multiply(tp, self.classWeights)\n",
    "        tp = tf.cast(tp, self.dtype)\n",
    "\n",
    "        tn = tf.math.logical_and(tf.equal(y_true, False), tf.equal(y_pred, False))\n",
    "        tn = tf.cast(tn, self.dtype)\n",
    "        tn = tf.math.multiply(tn, self.classWeights)\n",
    "        tn = tf.cast(tn, self.dtype)\n",
    "\n",
    "        fp = tf.math.logical_and(tf.equal(y_true, True), tf.equal(y_pred, False))\n",
    "        fp = tf.cast(fp, self.dtype)\n",
    "        fp = tf.math.multiply(fp, self.classWeights)\n",
    "        fp = tf.cast(fp, self.dtype)\n",
    "\n",
    "        fn = tf.math.logical_and(tf.equal(y_true, False), tf.equal(y_pred, True))\n",
    "        fn = tf.cast(fn, self.dtype)\n",
    "        fn = tf.math.multiply(fn, self.classWeights)\n",
    "        fn = tf.cast(fn, self.dtype)\n",
    "\n",
    "        pr = tf.math.divide(tf.math.reduce_sum(tp, axis=1),tf.math.reduce_sum(tp+fp+tf.keras.backend.epsilon(), axis=1))\n",
    "        m = tf.cast(m, pr.dtype)\n",
    "        if(m>0):\n",
    "            wPr = tf.math.reduce_sum(pr)/m\n",
    "        else:\n",
    "            wPr = tf.constant(0, tf.dtypes.float32)\n",
    "\n",
    "        re = tf.math.divide(tf.math.reduce_sum(tp, axis=1),tf.math.reduce_sum(tp+fn+tf.keras.backend.epsilon(), axis=1))\n",
    "        n = tf.cast(n, re.dtype)\n",
    "        wRe = tf.math.reduce_sum(re)/n\n",
    "\n",
    "        res = tf.math.divide(2*wPr*wRe, wPr + wRe + tf.keras.backend.epsilon())\n",
    "\n",
    "        self.f1.assign_add(res)\n",
    "        self.total.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide(self.f1, self.total)\n",
    "\n",
    "testM = WeightedF1(classWeights=[1,1,2])\n",
    "labelsTrue = tf.constant([[1,0,1], [1,1,0]])\n",
    "labelsPred = tf.constant([[0.8,0.1,0.9], [0,0,0]])\n",
    "\n",
    "testM.update_state(labelsTrue, labelsPred)\n",
    "testM.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedPrecision(tf.keras.metrics.Metric):\n",
    "    def __init__(self, classWeights, threshold=0.5):\n",
    "        super(WeightedPrecision, self).__init__()\n",
    "        self.classWeights= tf.constant(classWeights, dtype=tf.dtypes.float32)\n",
    "        self.threshold= tf.constant(threshold, dtype=tf.dtypes.float32)\n",
    "        self.prec = self.add_weight(name='prec', initializer='zeros')\n",
    "        self.total = self.add_weight('total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        m = tf.math.count_nonzero(tf.reduce_max(y_pred, axis=1))\n",
    "        y_true = tf.cast(y_true, tf.bool)\n",
    "        y_pred = tf.math.greater_equal(y_pred,self.threshold)\n",
    "   \n",
    "\n",
    "        tp = tf.math.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n",
    "        tp = tf.cast(tp, self.dtype)\n",
    "        tp = tf.math.multiply(tp, self.classWeights)\n",
    "        tp = tf.cast(tp, self.dtype)\n",
    "\n",
    "        fp = tf.math.logical_and(tf.equal(y_true, True), tf.equal(y_pred, False))\n",
    "        fp = tf.cast(fp, self.dtype)\n",
    "        fp = tf.math.multiply(fp, self.classWeights)\n",
    "        fp = tf.cast(fp, self.dtype)\n",
    "\n",
    "        pr = tf.math.divide(tf.math.reduce_sum(tp, axis=1),tf.math.reduce_sum(tp+fp+tf.keras.backend.epsilon(), axis=1))\n",
    "        m = tf.cast(m, pr.dtype)\n",
    "        if(m>0):\n",
    "            wPr = tf.math.reduce_sum(pr)/m\n",
    "        else:\n",
    "            wPr = tf.constant(0, tf.dtypes.float32)\n",
    "\n",
    "\n",
    "        self.prec.assign_add(wPr)\n",
    "        self.total.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide(self.prec, self.total)\n",
    "\n",
    "testM = WeightedPrecision(classWeights=[1,1,1])\n",
    "labelsTrue = tf.constant([[1,0,1], [1,1,0]])\n",
    "labelsPred = tf.constant([[0.8,0.1,0.9], [0,0,0]])\n",
    "\n",
    "testM.update_state(labelsTrue, labelsPred)\n",
    "testM.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2886c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedRecall(tf.keras.metrics.Metric):\n",
    "    def __init__(self, classWeights, threshold=0.5):\n",
    "        super(WeightedRecall, self).__init__()\n",
    "        self.classWeights= tf.constant(classWeights, dtype=tf.dtypes.float32)\n",
    "        self.threshold= tf.constant(threshold, dtype=tf.dtypes.float32)\n",
    "        self.rec = self.add_weight(name='rec', initializer='zeros')\n",
    "        self.total = self.add_weight('total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        n = tf.shape(y_pred)[0]\n",
    "        y_true = tf.cast(y_true, tf.bool)\n",
    "        y_pred = tf.math.greater_equal(y_pred,self.threshold)\n",
    "   \n",
    "\n",
    "        tp = tf.math.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n",
    "        tp = tf.cast(tp, self.dtype)\n",
    "        tp = tf.math.multiply(tp, self.classWeights)\n",
    "        tp = tf.cast(tp, self.dtype)\n",
    "\n",
    "        fn = tf.math.logical_and(tf.equal(y_true, False), tf.equal(y_pred, True))\n",
    "        fn = tf.cast(fn, self.dtype)\n",
    "        fn = tf.math.multiply(fn, self.classWeights)\n",
    "        fn = tf.cast(fn, self.dtype)\n",
    "\n",
    "        re = tf.math.divide(tf.math.reduce_sum(tp, axis=1),tf.math.reduce_sum(tp+fn+tf.keras.backend.epsilon(), axis=1))\n",
    "        n = tf.cast(n, re.dtype)\n",
    "        wRe = tf.math.reduce_sum(re)/n\n",
    "\n",
    "        self.rec.assign_add(wRe)\n",
    "        self.total.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide(self.rec, self.total)\n",
    "\n",
    "testM = WeightedRecall(classWeights=[1,1,1])\n",
    "labelsTrue = tf.constant([[1,0,1], [1,1,0]])\n",
    "labelsPred = tf.constant([[0.8,0.1,0.9], [0,0,0.2]])\n",
    "\n",
    "testM.update_state(labelsTrue, labelsPred)\n",
    "testM.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb5924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class WeightedAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self, classWeights, threshold=0.5):\n",
    "        super(WeightedAccuracy, self).__init__()\n",
    "        self.classWeights= tf.constant(classWeights, dtype=tf.dtypes.float32)\n",
    "        self.threshold= tf.constant(threshold, dtype=tf.dtypes.float32)\n",
    "        self.acc = self.add_weight(name='acc', initializer='zeros')\n",
    "        self.total = self.add_weight('total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        n = tf.shape(y_pred)[0]\n",
    "        y_true = tf.cast(y_true, tf.bool)\n",
    "        y_pred = tf.math.greater_equal(y_pred,self.threshold)\n",
    "   \n",
    "\n",
    "        tp = tf.math.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n",
    "        tp = tf.cast(tp, self.dtype)\n",
    "        tp = tf.math.multiply(tp, self.classWeights)\n",
    "        tp = tf.cast(tp, self.dtype)\n",
    "\n",
    "        tn = tf.math.logical_and(tf.equal(y_true, False), tf.equal(y_pred, False))\n",
    "        tn = tf.cast(tn, self.dtype)\n",
    "        tn = tf.math.multiply(tn, self.classWeights)\n",
    "        tn = tf.cast(tn, self.dtype)\n",
    "\n",
    "        fp = tf.math.logical_and(tf.equal(y_true, True), tf.equal(y_pred, False))\n",
    "        fp = tf.cast(fp, self.dtype)\n",
    "        fp = tf.math.multiply(fp, self.classWeights)\n",
    "        fp = tf.cast(fp, self.dtype)\n",
    "\n",
    "        fn = tf.math.logical_and(tf.equal(y_true, False), tf.equal(y_pred, True))\n",
    "        fn = tf.cast(fn, self.dtype)\n",
    "        fn = tf.math.multiply(fn, self.classWeights)\n",
    "        fn = tf.cast(fn, self.dtype)\n",
    "\n",
    "        acc = tf.math.divide(tf.math.reduce_sum(tp+tn, axis=1),tf.math.reduce_sum(tp+fn+tn+fp+tf.keras.backend.epsilon(), axis=1))\n",
    "        n = tf.cast(n, acc.dtype)\n",
    "        wAcc = tf.math.reduce_sum(acc)/n\n",
    "\n",
    "        self.acc.assign_add(wAcc)\n",
    "        self.total.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide(self.acc, self.total)\n",
    "\n",
    "testM = WeightedAccuracy(classWeights=[1,1,1])\n",
    "labelsTrue = tf.constant([[1,0,1], [1,1,0]])\n",
    "labelsPred = tf.constant([[0.8,0.1,0.9], [0,0,0]])\n",
    "\n",
    "testM.update_state(labelsTrue, labelsPred)\n",
    "testM.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f43e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE=32\n",
    "\n",
    "# batchedDataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "# batchedDatasetVal = datasetVal.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "# batchedDataset = batchedDataset.cache(os.path.join(DATA_PATH, \"datasetCache\"+SO))\n",
    "# batchedDatasetVal = batchedDatasetVal.cache(os.path.join(DATA_PATH, \"datasetCacheVal\"+SO))\n",
    "\n",
    "# optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule)\n",
    "\n",
    "# loss_fn = WeightedBinaryCE(labelWeights)\n",
    "\n",
    "# train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "# train_f1_metric = WeightedF1(classWeights=labelWeights, threshold=0.5)\n",
    "# train_prec = tf.keras.metrics.Precision()\n",
    "# train_rec = tf.keras.metrics.Recall()\n",
    "\n",
    "# model.compile(optimizer, loss_fn, metrics=[train_acc_metric,train_prec,train_f1_metric,train_rec ])\n",
    "\n",
    "# model.fit(batchedDataset, epochs=1, validation_data=batchedDatasetVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_addons as tfa\n",
    "\n",
    "BATCH_SIZE=64\n",
    "LOG_INTERVAL=20\n",
    "epochs = 15\n",
    "saveModel=False\n",
    "\n",
    "\n",
    "log_dir = \"./logs/\"+model.name+\"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_\"+SO\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,\n",
    "                                                      write_graph=True, update_freq=5)\n",
    "\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# Instantiate an optimizer .\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4)\n",
    "\n",
    "# Instantiate a loss function.\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "# loss_fn = WeightedBinaryCE(np.ones(len(mlb.classes_)))\n",
    "# loss_fn = WeightedBinaryCE(labelWeights)\n",
    "loss_fn = WeightedComboLoss(labelWeights, alpha=0.3, beta=0.7)\n",
    "\n",
    "train_acc_metric = WeightedAccuracy(classWeights=labelWeights)\n",
    "train_f1_metric = WeightedF1(classWeights=labelWeights, threshold=0.5)\n",
    "train_prec = WeightedPrecision(classWeights=labelWeights)\n",
    "train_rec = WeightedRecall(classWeights=labelWeights)\n",
    "\n",
    "val_acc_metric = WeightedAccuracy(classWeights=labelWeights)\n",
    "val_f1_metric = WeightedF1(classWeights=labelWeights, threshold=0.5)\n",
    "val_prec = WeightedPrecision(classWeights=labelWeights)\n",
    "val_rec = WeightedRecall(classWeights=labelWeights)\n",
    "\n",
    "batchedDataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "batchedDatasetVal = datasetVal.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "# batchedDataset = batchedDataset.cache(os.path.join(DATA_PATH, \"datasetCache\"+SO))\n",
    "# batchedDatasetVal = batchedDatasetVal.cache(os.path.join(DATA_PATH, \"datasetCacheVal\"+SO))\n",
    "\n",
    "@tf.function()\n",
    "def trainStep(x_batch_train, y_batch_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        probs = model(x_batch_train, training=True) \n",
    "        loss_value = loss_fn(y_batch_train, probs)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    #Gradient clipping\n",
    "    # grads = [tf.clip_by_norm(g, 2.0) for g in grads]\n",
    "\n",
    "    train_acc_metric.update_state(y_batch_train, probs)\n",
    "    train_f1_metric.update_state(y_batch_train, probs)\n",
    "    train_prec.update_state(y_batch_train, probs)\n",
    "    train_rec.update_state(y_batch_train, probs)\n",
    "\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "    return loss_value\n",
    "\n",
    "@tf.function()\n",
    "def valStep(x_batch_val, y_batch_val):\n",
    "    valProbs = model(x_batch_val, training=False)\n",
    "    # Update val metrics\n",
    "    val_acc_metric.update_state(y_batch_val, valProbs)\n",
    "    val_f1_metric.update_state(y_batch_val, valProbs)\n",
    "    val_prec.update_state(y_batch_val, valProbs)\n",
    "    val_rec.update_state(y_batch_val, valProbs)\n",
    "\n",
    "maxStep=0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch+1,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(batchedDataset):\n",
    "\n",
    "        loss_value =trainStep(x_batch_train,y_batch_train)\n",
    "\n",
    "        # Log \n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            template = 'Epoch {}/Step {}, Loss: {:.5f}, Accuracy: {:.5f}, F1: {:.4f}, Prec: {:.4f}, Rec: {:.4f} lr: {:.5f}'\n",
    "            print(template.format(epoch+1, step,loss_value.numpy(), \n",
    "                                    train_acc_metric.result(),train_f1_metric.result(),\n",
    "                                    train_prec.result(), train_rec.result(), optimizer.learning_rate.numpy()))\n",
    "            \n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', loss_value, step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('accuracy', train_acc_metric.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('f1', train_f1_metric.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('prec', train_prec.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('rec', train_rec.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('learning rate', optimizer.learning_rate.numpy(), step=maxStep*epoch+step)\n",
    "                summary_writer.flush()\n",
    "\n",
    "    \n",
    "    train_acc_metric.reset_states()\n",
    "    train_f1_metric.reset_states()\n",
    "    train_prec.reset_states()\n",
    "    train_rec.reset_states()\n",
    "\n",
    "    maxStep=step\n",
    "\n",
    "    print(\"Epoch finished. Start validation\")\n",
    "    for x_batch_val, y_batch_val in batchedDatasetVal:\n",
    "        valStep(x_batch_val, y_batch_val)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    val_f1 = val_f1_metric.result()\n",
    "    val_f1_metric.reset_states()\n",
    "    val_precision = val_prec.result()\n",
    "    val_prec.reset_states()\n",
    "    val_recall = val_rec.result()\n",
    "    val_rec.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Validation f1: %.4f\" % (float(val_f1),))\n",
    "    print(\"Validation precision: %.4f\" % (float(val_precision),))\n",
    "    print(\"Validation recall: %.4f\" % (float(val_recall),))\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('valAcc', float(val_acc), step=epoch)\n",
    "        tf.summary.scalar('valF1', float(val_f1), step=epoch)\n",
    "        tf.summary.scalar('valPrecision', float(val_precision), step=epoch)\n",
    "        tf.summary.scalar('valRecall', float(val_recall), step=epoch)\n",
    "        summary_writer.flush()\n",
    "    if saveModel:\n",
    "      model.save(os.path.join(DATA_PATH, \"model_\"+SO+\"_epoch_{}_valF1Score{:.3f}\".format(epoch, float(val_f1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.save(os.path.join(DATA_PATH, \"model_\"+SO+\"_epoch_{}_valf1Score{:.3f}\".format(epoch, float(val_f1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probs= model.predict(tf.expand_dims(list(datasetVal.take(32))[10][0], 0))\n",
    "prediction= [1 if p > 0.5 else 0 for p in probs[0]]\n",
    "probabilities= probs[probs>0.5]\n",
    "# classes = np.argwhere(prediction)\n",
    "print(mlb.inverse_transform(np.array([prediction])))\n",
    "print(probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
