{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1d77e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 17:36:05.882095: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-04 17:36:06.374122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "/mnt/e/ML/cafa-5-protein-function-prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 17:36:07.212896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-04 17:36:07.253031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-04 17:36:07.253343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "# try:\n",
    "#   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# except:\n",
    "#   # Invalid device or cannot modify virtual devices once initialized.\n",
    "#   pass\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "print(DATA_PATH)\n",
    "\n",
    "# Choose subontology (CCO, MFO or BPO)\n",
    "SO = 'BPO'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3544f8a",
   "metadata": {},
   "source": [
    "## Reading fasta, obo and tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b515f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "sequences = [rec.seq for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Train/train_sequences.fasta\"),\"fasta\")]\n",
    "ids = [rec.id for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Train/train_sequences.fasta\"),\"fasta\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2898414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import obonet\n",
    "\n",
    "# Read the taxrank ontology\n",
    "url = os.path.join(DATA_PATH, \"Train/go-basic.obo\")\n",
    "graph = obonet.read_obo(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4bf949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3497732, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "dfSO = df.loc[df[\"aspect\"]==SO]\n",
    "uniqueTerms = dfSO[\"term\"].unique()\n",
    "termsArr = list(dfSO[\"term\"].to_numpy())\n",
    "\n",
    "uniqueTermsDict={}\n",
    "for i,el in enumerate(uniqueTerms):\n",
    "    uniqueTermsDict[el] = i\n",
    "    \n",
    "print(dfSO.shape)\n",
    "df=dfSO\n",
    "\n",
    "df.set_index(\"EntryID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4dec705",
   "metadata": {},
   "outputs": [],
   "source": [
    "testID = df.index.to_list()[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5338b7f8",
   "metadata": {},
   "source": [
    "## GO analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1058ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = df[\"term\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb35584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name = {id_: data.get('name') for id_, data in graph.nodes(data=True)}\n",
    "name_to_id = {data['name']: id_ for id_, data in graph.nodes(data=True) if 'name' in data}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c099fc2",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cae9a3b0",
   "metadata": {},
   "source": [
    "The task is a multilabel classification: The output has several possible targets (Gene Ontologies) but each can only be 1 (existing) or 0 (non existing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8e63ed9",
   "metadata": {},
   "source": [
    "Extract label weights from IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e3c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found GOs: 0 (set to 0)\n"
     ]
    }
   ],
   "source": [
    "dfIa = pd.read_csv(os.path.join(DATA_PATH, \"IA.txt\"), sep='\\t', header=None)\n",
    "\n",
    "dfIa.set_index(0, inplace=True)\n",
    "\n",
    "labelWeights=[]\n",
    "allIndices = dfIa.index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "notFound=0\n",
    "for go in item_counts.index.to_list():\n",
    "    if go in allIndices:\n",
    "        labelWeights.append(dfIa.loc[go].to_numpy()[0])\n",
    "    else:\n",
    "        notFound += 1\n",
    "        labelWeights.append(0)\n",
    "\n",
    "print(\"Not found GOs: {} (set to 0)\".format(notFound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f6a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GO:0008152' 'GO:0034655' 'GO:0072523' 'GO:0044270' 'GO:0006753'\n",
      " 'GO:1901292' 'GO:0044237' 'GO:1901360' 'GO:0008150' 'GO:1901564'\n",
      " 'GO:1901565' 'GO:0009117' 'GO:0006139' 'GO:0044281' 'GO:0046496'\n",
      " 'GO:0019362' 'GO:0046483' 'GO:0055086' 'GO:0044248' 'GO:0019439'\n",
      " 'GO:0019637' 'GO:0006807' 'GO:0019677' 'GO:1901361' 'GO:0006163'\n",
      " 'GO:0046700' 'GO:0009987' 'GO:0006725' 'GO:0006796' 'GO:0034641'\n",
      " 'GO:0072521' 'GO:0071704' 'GO:0019364' 'GO:1901575' 'GO:0072526'\n",
      " 'GO:0046434' 'GO:0009166' 'GO:0072524' 'GO:0006195' 'GO:0009056'\n",
      " 'GO:0044238' 'GO:0006793' 'GO:0019674']\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "14109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:895: UserWarning: unknown class(es) ['GO:0006195', 'GO:0008150', 'GO:0019364', 'GO:1901292'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pickle\n",
    "\n",
    "topGOs= item_counts\n",
    "topGOs=topGOs.index.to_list()\n",
    "\n",
    "#Reduce possible GOs by label weight\n",
    "threshold=0\n",
    "labelWeights=np.array(labelWeights)\n",
    "selection = labelWeights>threshold\n",
    "topGOs=np.array(topGOs)[selection]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([topGOs])\n",
    "\n",
    "dftest=df.loc[testID]\n",
    "indices = dftest[\"term\"].to_numpy()\n",
    "print(indices)\n",
    "print(mlb.transform([indices]))\n",
    "print(len(mlb.classes_))\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'MLB_'+SO+'.pkl'), 'wb') as f:\n",
    "    pickle.dump(mlb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb189ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found GOs: 0 (set to 0)\n"
     ]
    }
   ],
   "source": [
    "labelWeightsCorr=[]\n",
    "\n",
    "notFound=0\n",
    "for go in mlb.classes_:\n",
    "    if go in allIndices:\n",
    "        labelWeightsCorr.append(dfIa.loc[go].to_numpy()[0])\n",
    "    else:\n",
    "        notFound += 1\n",
    "        labelWeightsCorr.append(0)\n",
    "\n",
    "print(\"Not found GOs: {} (set to 0)\".format(notFound))\n",
    "labelWeightsCorr=np.array(labelWeightsCorr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92019f8c",
   "metadata": {},
   "source": [
    "## Amino acids encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e42462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_dict = {'A': 1, 'B':24, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'O': 21, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'U': 22, 'V': 18, 'W': 19, 'Y': 20, 'X':30, 'Z':23}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfd823ba",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "431f9df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max. length of the sequences is 35375\n"
     ]
    }
   ],
   "source": [
    "seqLengths = [len(seq) for seq in sequences]\n",
    "maxLen = max(seqLengths)\n",
    "print(\"The max. length of the sequences is {}\".format(maxLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e581e06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A0A009IHW8' 'A0A021WW32' 'A0A023FFD0' ... 'X5L1L5' 'X5L565' 'X5M5N0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfAll=pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "soEntries = dfAll.loc[dfAll[\"aspect\"]==SO]\n",
    "soEntryIds = soEntries[\"EntryID\"].unique()\n",
    "\n",
    "# print(len(seqEntries))\n",
    "print(soEntryIds)\n",
    "\n",
    "# SoSequences = []\n",
    "# for entry in soEntryIds:\n",
    "#     SoSequences.append(sequences[ids.index(entry)])\n",
    "\n",
    "# print(len(SoSequences))\n",
    "dfAll.set_index(\"EntryID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44f722e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99572\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "TRAIN_VAL_SPLIT = 0.7\n",
    "nonRelevantThreshold=0.01\n",
    "k = 3\n",
    "\n",
    "allAA = list(aa_dict.keys())\n",
    "allAA.sort()\n",
    "allCombinations= list(product(*(allAA for i in range(k))))\n",
    "allCombinations=np.array([''.join(el) for el in allCombinations])\n",
    "\n",
    "positionDict = dict(zip(allCombinations, np.arange(0,allCombinations.size).T))\n",
    "\n",
    "#Use numpy vectorize to speed up the mapping (hopefully)\n",
    "mapping = lambda x: aa_dict[x]\n",
    "vectMapping = np.vectorize(mapping)\n",
    "\n",
    "# Shuffle the data\n",
    "import random\n",
    "random.seed(516213)\n",
    "c = list(zip(sequences, ids))\n",
    "random.shuffle(c)\n",
    "sequencesShuffle, idsShuffle = zip(*c)\n",
    "\n",
    "\n",
    "#Train Validation Split\n",
    "split = int(np.floor(len(sequencesShuffle)*TRAIN_VAL_SPLIT))\n",
    "print(split)\n",
    "trainSeq = sequencesShuffle[0:split]\n",
    "valSeq = sequencesShuffle[split+1:]\n",
    "trainIds = idsShuffle[0:split]\n",
    "valIds = idsShuffle[split+1:]\n",
    "\n",
    "\n",
    "def generator():\n",
    "  for i,seq in enumerate(trainSeq):\n",
    "      entryId = trainIds[i]\n",
    "      if entryId in soEntryIds:\n",
    "        labelData = df.loc[entryId]\n",
    "        # indices = labelData[\"termToken\"].to_numpy()\n",
    "        indices = labelData[\"term\"].to_numpy()\n",
    "      else: \n",
    "        indices=[]\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "          #supress the warnings for unknown classes\n",
    "          warnings.simplefilter(\"ignore\")\n",
    "          y = mlb.transform([indices])\n",
    "\n",
    "      # if np.count_nonzero(y)==0 and np.random.random()>nonRelevantThreshold:\n",
    "      #   continue\n",
    "\n",
    "      \n",
    "      kmers = [seq[j:j+k] if j < len(seq)-(k-1) else 0 for j,el in enumerate(seq)]\n",
    "      kmers = kmers[0:-(k-1)]\n",
    "      kmers = [str(el) for el in kmers]\n",
    "      values, counts = np.unique(kmers, return_counts=True)\n",
    "      freqVector=np.zeros(allCombinations.shape)\n",
    "      for l,v in enumerate(values):\n",
    "          freqVector[positionDict[v]] = counts[l]\n",
    "      yield (freqVector,y[0])\n",
    "\n",
    "\n",
    "def generatorVal():\n",
    "  for i,seq in enumerate(valSeq):\n",
    "      entryId = valIds[i]\n",
    "      if entryId in soEntryIds:\n",
    "        labelData = df.loc[entryId]\n",
    "        # indices = labelData[\"termToken\"].to_numpy()\n",
    "        indices = labelData[\"term\"].to_numpy()\n",
    "      else:\n",
    "        indices=[]\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "          #supress the warnings for unknown classes\n",
    "          warnings.simplefilter(\"ignore\")\n",
    "          y = mlb.transform([indices])\n",
    "\n",
    "      # if np.count_nonzero(y)==0 and np.random.random()>nonRelevantThreshold:\n",
    "      #   continue\n",
    "      \n",
    "      kmers = [seq[j:j+k] if j < len(seq)-(k-1) else 0 for j,el in enumerate(seq)]\n",
    "      kmers = kmers[0:-(k-1)]\n",
    "      kmers = [str(el) for el in kmers]\n",
    "      values, counts = np.unique(kmers, return_counts=True)\n",
    "      freqVector=np.zeros(allCombinations.shape)\n",
    "      for l,v in enumerate(values):\n",
    "          freqVector[positionDict[v]] = counts[l]\n",
    "      yield (freqVector,y[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2727338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sample: \n",
      "(15625,)\n",
      "[0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0.]\n",
      "The first sample has 0 classes\n"
     ]
    }
   ],
   "source": [
    "g = generator()\n",
    "test = next(g)\n",
    "print(\"The first sample: \\n{}\\n{}\".format(test[0].shape, test[0][0:100]))\n",
    "print(\"The first sample has {} classes\".format(np.count_nonzero(test[1])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10c4a51a",
   "metadata": {},
   "source": [
    "## Tensorflow Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63f0d2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 17:36:22.742787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-04 17:36:22.743151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-04 17:36:22.743459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-04 17:36:23.329895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-04 17:36:23.330319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-04 17:36:23.330333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-07-04 17:36:23.330659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-04 17:36:23.330762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6569 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:2b:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<tf.Tensor: shape=(15625,), dtype=int32, numpy=array([0, 0, 0, ..., 0, 0, 0], dtype=int32)>, <tf.Tensor: shape=(14109,), dtype=int32, numpy=array([0, 0, 0, ..., 0, 0, 0], dtype=int32)>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 17:36:23.629952: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xSize = allCombinations.shape[0]\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(generator, output_signature=(\n",
    "         tf.TensorSpec(shape=(xSize,), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(len(mlb.classes_),), dtype=tf.int32)))\n",
    "print(list(dataset.take(1)))\n",
    "\n",
    "datasetVal = tf.data.Dataset.from_generator(generatorVal, output_signature=(\n",
    "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(len(mlb.classes_),), dtype=tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0a554f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98752d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedConvModel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 15625)]           0         \n",
      "                                                                 \n",
      " tf.expand_dims (TFOpLambda)  (None, 15625, 1)         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 15619, 8)          64        \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 15619, 8)          0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 15619, 8)         32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 15613, 8)          456       \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 15613, 8)          0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 15613, 8)         32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 15607, 8)          456       \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 15607, 8)          0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 15607, 8)         32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 15601, 8)          456       \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 15601, 8)          0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 15601, 8)         32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 7798, 8)           456       \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 7798, 8)           0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 7798, 8)          32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 3899, 8)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 1947, 8)           456       \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 1947, 8)           0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 1947, 8)          32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 973, 8)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 484, 8)            456       \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 484, 8)            0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 484, 8)           32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 242, 8)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 118, 8)            456       \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 118, 8)            0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 118, 8)           32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 944)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                60480     \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 14109)             917085    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 993,557\n",
      "Trainable params: 993,429\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "VOCAB_SIZE=len(aa_dict)\n",
    "EMBED_DIM=10\n",
    "\n",
    "def createModel():\n",
    "    inputs = tf.keras.Input(shape=(xSize,))\n",
    "    # x=layers.Embedding(VOCAB_SIZE, EMBED_DIM, name=\"embedding\")(inputs)\n",
    "\n",
    "    x=layers.Conv1D(8, 7)(tf.expand_dims(inputs,2))\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x=layers.Conv1D(8, 7)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x=layers.Conv1D(8, 7)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x=layers.Conv1D(8, 7)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x=layers.Conv1D(8, 7,strides=2)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x=layers.MaxPool1D()(x)\n",
    "\n",
    "    x=layers.Conv1D(8, 7,strides=2)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x=layers.MaxPool1D()(x)\n",
    "    \n",
    "    x=layers.Conv1D(8, 7, strides=2)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x=layers.MaxPool1D()(x)\n",
    "    \n",
    "    x=layers.Conv1D(8, 7, strides=2)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "    # x=layers.Conv1D(32, 5, activation=tf.keras.activations.relu)(x)\n",
    "    # x=layers.Conv1D(32, 5, activation=tf.keras.activations.relu)(x)\n",
    "    # x=layers.Conv1D(32, 5, activation=tf.keras.activations.relu)(x)\n",
    "    x=layers.Flatten()(x)\n",
    "    x=layers.Dense(64)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x=layers.Dense(64)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x=layers.Dense(64)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x=layers.Dense(64)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    outputs=layers.Dense(len(mlb.classes_), activation=tf.keras.activations.sigmoid)(x)\n",
    "    # outputs=layers.Softmax()(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"embedConvModel\")\n",
    "\n",
    "model = createModel()\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a3ddc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "decaySteps=5000\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate, first_decay_steps=decaySteps,\n",
    "                                                                t_mul=2.0, m_mul=0.7)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "#     initial_learning_rate, decay_steps=decaySteps, alpha=0.01)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate,decay_steps=decaySteps,decay_rate=0.9,staircase=False)\n",
    "step = np.linspace(0,decaySteps*3)\n",
    "lr = lr_schedule(step)\n",
    "# plt.figure(figsize = (8,6))\n",
    "# # plt.yscale(\"log\")\n",
    "# plt.plot(step, lr)\n",
    "# plt.ylim([0,max(plt.ylim())])\n",
    "# plt.xlabel('step')\n",
    "# _ = plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2f43e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9113c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 17:36:24.271624: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-07-04 17:36:27.589565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-07-04 17:36:28.247273: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f77a342c9d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-04 17:36:28.247314: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1\n",
      "2023-07-04 17:36:28.269456: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-07-04 17:36:28.491197: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/Step 0, Loss: -0.00115, Accuracy: 0.49953, F1: 0.0014, Prec: 0.3646, Rec: 0.0007 lr: 0.00080\n",
      "Epoch 1/Step 20, Loss: -0.00134, Accuracy: 0.54550, F1: 0.0022, Prec: 0.4478, Rec: 0.0011 lr: 0.00080\n",
      "Epoch 1/Step 40, Loss: -0.01757, Accuracy: 0.69833, F1: 0.0081, Prec: 0.4723, Rec: 0.0042 lr: 0.00080\n",
      "Epoch 1/Step 60, Loss: -0.03511, Accuracy: 0.79251, F1: 0.0215, Prec: 0.4452, Rec: 0.0114 lr: 0.00080\n",
      "Epoch 1/Step 80, Loss: -0.03862, Accuracy: 0.84139, F1: 0.0328, Prec: 0.4310, Rec: 0.0177 lr: 0.00080\n",
      "Epoch 1/Step 100, Loss: -0.03879, Accuracy: 0.87101, F1: 0.0397, Prec: 0.4187, Rec: 0.0216 lr: 0.00080\n",
      "Epoch 1/Step 120, Loss: -0.04715, Accuracy: 0.89090, F1: 0.0442, Prec: 0.4096, Rec: 0.0242 lr: 0.00080\n",
      "Epoch 1/Step 140, Loss: -0.03930, Accuracy: 0.90520, F1: 0.0471, Prec: 0.3970, Rec: 0.0258 lr: 0.00080\n",
      "Epoch 1/Step 160, Loss: -0.04150, Accuracy: 0.91599, F1: 0.0498, Prec: 0.3895, Rec: 0.0274 lr: 0.00080\n",
      "Epoch 1/Step 180, Loss: -0.03260, Accuracy: 0.92442, F1: 0.0524, Prec: 0.3836, Rec: 0.0289 lr: 0.00080\n",
      "Epoch 1/Step 200, Loss: -0.04524, Accuracy: 0.93119, F1: 0.0545, Prec: 0.3789, Rec: 0.0302 lr: 0.00080\n",
      "Epoch 1/Step 220, Loss: -0.04641, Accuracy: 0.93674, F1: 0.0567, Prec: 0.3744, Rec: 0.0315 lr: 0.00080\n",
      "Epoch 1/Step 240, Loss: -0.03779, Accuracy: 0.94139, F1: 0.0585, Prec: 0.3712, Rec: 0.0326 lr: 0.00080\n",
      "Epoch 1/Step 260, Loss: -0.03766, Accuracy: 0.94533, F1: 0.0597, Prec: 0.3678, Rec: 0.0333 lr: 0.00080\n",
      "Epoch 1/Step 280, Loss: -0.05662, Accuracy: 0.94872, F1: 0.0613, Prec: 0.3655, Rec: 0.0343 lr: 0.00080\n",
      "Epoch 1/Step 300, Loss: -0.03386, Accuracy: 0.95168, F1: 0.0629, Prec: 0.3633, Rec: 0.0353 lr: 0.00080\n",
      "Epoch 1/Step 320, Loss: -0.03508, Accuracy: 0.95429, F1: 0.0640, Prec: 0.3606, Rec: 0.0360 lr: 0.00080\n",
      "Epoch 1/Step 340, Loss: -0.03775, Accuracy: 0.95662, F1: 0.0649, Prec: 0.3569, Rec: 0.0366 lr: 0.00080\n",
      "Epoch 1/Step 360, Loss: -0.08545, Accuracy: 0.95883, F1: 0.0671, Prec: 0.3500, Rec: 0.0385 lr: 0.00080\n",
      "Epoch 1/Step 380, Loss: -0.05326, Accuracy: 0.96087, F1: 0.0698, Prec: 0.3418, Rec: 0.0410 lr: 0.00080\n",
      "Epoch 1/Step 400, Loss: -0.06354, Accuracy: 0.96270, F1: 0.0727, Prec: 0.3348, Rec: 0.0438 lr: 0.00080\n",
      "Epoch 1/Step 420, Loss: -0.07300, Accuracy: 0.96437, F1: 0.0750, Prec: 0.3280, Rec: 0.0459 lr: 0.00080\n",
      "Epoch 1/Step 440, Loss: -0.07275, Accuracy: 0.96589, F1: 0.0773, Prec: 0.3221, Rec: 0.0480 lr: 0.00080\n",
      "Epoch 1/Step 460, Loss: -0.06386, Accuracy: 0.96727, F1: 0.0795, Prec: 0.3167, Rec: 0.0501 lr: 0.00080\n",
      "Epoch 1/Step 480, Loss: -0.08220, Accuracy: 0.96854, F1: 0.0817, Prec: 0.3117, Rec: 0.0522 lr: 0.00080\n",
      "Epoch 1/Step 500, Loss: -0.07113, Accuracy: 0.96971, F1: 0.0834, Prec: 0.3067, Rec: 0.0539 lr: 0.00080\n",
      "Epoch 1/Step 520, Loss: -0.10302, Accuracy: 0.97080, F1: 0.0851, Prec: 0.3024, Rec: 0.0555 lr: 0.00080\n",
      "Epoch 1/Step 540, Loss: -0.07508, Accuracy: 0.97180, F1: 0.0869, Prec: 0.2986, Rec: 0.0572 lr: 0.00080\n",
      "Epoch 1/Step 560, Loss: -0.06573, Accuracy: 0.97273, F1: 0.0883, Prec: 0.2948, Rec: 0.0586 lr: 0.00080\n",
      "Epoch 1/Step 580, Loss: -0.09510, Accuracy: 0.97359, F1: 0.0894, Prec: 0.2910, Rec: 0.0597 lr: 0.00080\n",
      "Epoch 1/Step 600, Loss: -0.09367, Accuracy: 0.97440, F1: 0.0906, Prec: 0.2877, Rec: 0.0608 lr: 0.00080\n",
      "Epoch 1/Step 620, Loss: -0.07779, Accuracy: 0.97516, F1: 0.0915, Prec: 0.2846, Rec: 0.0617 lr: 0.00080\n",
      "Epoch 1/Step 640, Loss: -0.06701, Accuracy: 0.97587, F1: 0.0925, Prec: 0.2818, Rec: 0.0627 lr: 0.00080\n",
      "Epoch 1/Step 660, Loss: -0.09647, Accuracy: 0.97653, F1: 0.0934, Prec: 0.2789, Rec: 0.0636 lr: 0.00080\n",
      "Epoch 1/Step 680, Loss: -0.07165, Accuracy: 0.97716, F1: 0.0946, Prec: 0.2765, Rec: 0.0647 lr: 0.00080\n",
      "Epoch 1/Step 700, Loss: -0.09498, Accuracy: 0.97775, F1: 0.0954, Prec: 0.2739, Rec: 0.0655 lr: 0.00080\n",
      "Epoch 1/Step 720, Loss: -0.08171, Accuracy: 0.97831, F1: 0.0962, Prec: 0.2715, Rec: 0.0663 lr: 0.00080\n",
      "Epoch 1/Step 740, Loss: -0.05208, Accuracy: 0.97884, F1: 0.0967, Prec: 0.2693, Rec: 0.0668 lr: 0.00080\n",
      "Epoch 1/Step 760, Loss: -0.06872, Accuracy: 0.97934, F1: 0.0973, Prec: 0.2673, Rec: 0.0673 lr: 0.00080\n",
      "Epoch 1/Step 780, Loss: -0.07898, Accuracy: 0.97982, F1: 0.0978, Prec: 0.2651, Rec: 0.0678 lr: 0.00080\n",
      "Epoch 1/Step 800, Loss: -0.07588, Accuracy: 0.98027, F1: 0.0984, Prec: 0.2628, Rec: 0.0684 lr: 0.00080\n",
      "Epoch 1/Step 820, Loss: -0.09373, Accuracy: 0.98070, F1: 0.0987, Prec: 0.2606, Rec: 0.0689 lr: 0.00080\n",
      "Epoch 1/Step 840, Loss: -0.08123, Accuracy: 0.98111, F1: 0.0992, Prec: 0.2589, Rec: 0.0694 lr: 0.00080\n",
      "Epoch 1/Step 860, Loss: -0.08156, Accuracy: 0.98150, F1: 0.0997, Prec: 0.2573, Rec: 0.0698 lr: 0.00080\n",
      "Epoch 1/Step 880, Loss: -0.06140, Accuracy: 0.98187, F1: 0.1004, Prec: 0.2557, Rec: 0.0705 lr: 0.00080\n",
      "Epoch 1/Step 900, Loss: -0.06191, Accuracy: 0.98222, F1: 0.1010, Prec: 0.2544, Rec: 0.0710 lr: 0.00080\n",
      "Epoch 1/Step 920, Loss: -0.06966, Accuracy: 0.98256, F1: 0.1012, Prec: 0.2527, Rec: 0.0713 lr: 0.00080\n",
      "Epoch 1/Step 940, Loss: -0.06112, Accuracy: 0.98289, F1: 0.1017, Prec: 0.2515, Rec: 0.0717 lr: 0.00080\n",
      "Epoch 1/Step 960, Loss: -0.08001, Accuracy: 0.98320, F1: 0.1022, Prec: 0.2504, Rec: 0.0722 lr: 0.00080\n",
      "Epoch 1/Step 980, Loss: -0.07256, Accuracy: 0.98350, F1: 0.1028, Prec: 0.2493, Rec: 0.0727 lr: 0.00080\n",
      "Epoch 1/Step 1000, Loss: -0.08530, Accuracy: 0.98379, F1: 0.1031, Prec: 0.2481, Rec: 0.0730 lr: 0.00080\n",
      "Epoch 1/Step 1020, Loss: -0.06651, Accuracy: 0.98406, F1: 0.1035, Prec: 0.2472, Rec: 0.0733 lr: 0.00080\n",
      "Epoch 1/Step 1040, Loss: -0.07291, Accuracy: 0.98433, F1: 0.1039, Prec: 0.2460, Rec: 0.0738 lr: 0.00080\n",
      "Epoch 1/Step 1060, Loss: -0.07901, Accuracy: 0.98458, F1: 0.1042, Prec: 0.2447, Rec: 0.0740 lr: 0.00080\n",
      "Epoch 1/Step 1080, Loss: -0.07703, Accuracy: 0.98483, F1: 0.1046, Prec: 0.2438, Rec: 0.0745 lr: 0.00080\n",
      "Epoch 1/Step 1100, Loss: -0.06763, Accuracy: 0.98507, F1: 0.1050, Prec: 0.2428, Rec: 0.0748 lr: 0.00080\n",
      "Epoch 1/Step 1120, Loss: -0.07486, Accuracy: 0.98530, F1: 0.1053, Prec: 0.2421, Rec: 0.0751 lr: 0.00080\n",
      "Epoch 1/Step 1140, Loss: -0.07442, Accuracy: 0.98552, F1: 0.1056, Prec: 0.2412, Rec: 0.0754 lr: 0.00080\n",
      "Epoch 1/Step 1160, Loss: -0.08744, Accuracy: 0.98573, F1: 0.1060, Prec: 0.2403, Rec: 0.0758 lr: 0.00080\n",
      "Epoch 1/Step 1180, Loss: -0.06551, Accuracy: 0.98593, F1: 0.1063, Prec: 0.2393, Rec: 0.0761 lr: 0.00080\n",
      "Epoch 1/Step 1200, Loss: -0.05546, Accuracy: 0.98613, F1: 0.1066, Prec: 0.2384, Rec: 0.0765 lr: 0.00080\n",
      "Epoch 1/Step 1220, Loss: -0.08166, Accuracy: 0.98633, F1: 0.1069, Prec: 0.2374, Rec: 0.0768 lr: 0.00080\n",
      "Epoch 1/Step 1240, Loss: -0.05800, Accuracy: 0.98651, F1: 0.1073, Prec: 0.2367, Rec: 0.0771 lr: 0.00080\n",
      "Epoch 1/Step 1260, Loss: -0.06360, Accuracy: 0.98669, F1: 0.1076, Prec: 0.2362, Rec: 0.0773 lr: 0.00080\n",
      "Epoch 1/Step 1280, Loss: -0.09038, Accuracy: 0.98687, F1: 0.1078, Prec: 0.2356, Rec: 0.0776 lr: 0.00080\n",
      "Epoch 1/Step 1300, Loss: -0.07934, Accuracy: 0.98704, F1: 0.1081, Prec: 0.2349, Rec: 0.0778 lr: 0.00080\n",
      "Epoch 1/Step 1320, Loss: -0.08267, Accuracy: 0.98720, F1: 0.1084, Prec: 0.2343, Rec: 0.0781 lr: 0.00080\n",
      "Epoch 1/Step 1340, Loss: -0.04601, Accuracy: 0.98736, F1: 0.1087, Prec: 0.2339, Rec: 0.0783 lr: 0.00080\n",
      "Epoch 1/Step 1360, Loss: -0.06872, Accuracy: 0.98752, F1: 0.1088, Prec: 0.2332, Rec: 0.0785 lr: 0.00080\n",
      "Epoch 1/Step 1380, Loss: -0.09779, Accuracy: 0.98767, F1: 0.1090, Prec: 0.2325, Rec: 0.0787 lr: 0.00080\n",
      "Epoch 1/Step 1400, Loss: -0.06603, Accuracy: 0.98781, F1: 0.1092, Prec: 0.2319, Rec: 0.0788 lr: 0.00080\n",
      "Epoch 1/Step 1420, Loss: -0.08562, Accuracy: 0.98795, F1: 0.1094, Prec: 0.2313, Rec: 0.0790 lr: 0.00080\n",
      "Epoch 1/Step 1440, Loss: -0.06817, Accuracy: 0.98809, F1: 0.1096, Prec: 0.2309, Rec: 0.0792 lr: 0.00080\n",
      "Epoch 1/Step 1460, Loss: -0.07636, Accuracy: 0.98823, F1: 0.1098, Prec: 0.2303, Rec: 0.0794 lr: 0.00080\n",
      "Epoch 1/Step 1480, Loss: -0.06287, Accuracy: 0.98836, F1: 0.1100, Prec: 0.2298, Rec: 0.0796 lr: 0.00080\n",
      "Epoch 1/Step 1500, Loss: -0.07236, Accuracy: 0.98848, F1: 0.1102, Prec: 0.2293, Rec: 0.0798 lr: 0.00080\n",
      "Epoch 1/Step 1520, Loss: -0.08215, Accuracy: 0.98861, F1: 0.1104, Prec: 0.2288, Rec: 0.0799 lr: 0.00080\n",
      "Epoch 1/Step 1540, Loss: -0.06018, Accuracy: 0.98873, F1: 0.1104, Prec: 0.2282, Rec: 0.0800 lr: 0.00080\n",
      "Epoch 1/Step 1560, Loss: -0.09169, Accuracy: 0.98885, F1: 0.1106, Prec: 0.2278, Rec: 0.0802 lr: 0.00080\n",
      "Epoch 1/Step 1580, Loss: -0.08733, Accuracy: 0.98896, F1: 0.1108, Prec: 0.2273, Rec: 0.0804 lr: 0.00080\n",
      "Epoch 1/Step 1600, Loss: -0.08860, Accuracy: 0.98907, F1: 0.1110, Prec: 0.2268, Rec: 0.0806 lr: 0.00080\n",
      "Epoch 1/Step 1620, Loss: -0.07039, Accuracy: 0.98918, F1: 0.1111, Prec: 0.2263, Rec: 0.0807 lr: 0.00080\n",
      "Epoch 1/Step 1640, Loss: -0.07636, Accuracy: 0.98929, F1: 0.1112, Prec: 0.2259, Rec: 0.0808 lr: 0.00080\n",
      "Epoch 1/Step 1660, Loss: -0.06874, Accuracy: 0.98939, F1: 0.1114, Prec: 0.2256, Rec: 0.0810 lr: 0.00080\n",
      "Epoch 1/Step 1680, Loss: -0.06510, Accuracy: 0.98949, F1: 0.1116, Prec: 0.2251, Rec: 0.0811 lr: 0.00080\n",
      "Epoch 1/Step 1700, Loss: -0.06131, Accuracy: 0.98959, F1: 0.1117, Prec: 0.2247, Rec: 0.0813 lr: 0.00080\n",
      "Epoch 1/Step 1720, Loss: -0.07828, Accuracy: 0.98969, F1: 0.1119, Prec: 0.2244, Rec: 0.0815 lr: 0.00080\n",
      "Epoch 1/Step 1740, Loss: -0.08513, Accuracy: 0.98978, F1: 0.1121, Prec: 0.2241, Rec: 0.0816 lr: 0.00080\n",
      "Epoch 1/Step 1760, Loss: -0.07654, Accuracy: 0.98987, F1: 0.1123, Prec: 0.2237, Rec: 0.0818 lr: 0.00080\n",
      "Epoch 1/Step 1780, Loss: -0.07635, Accuracy: 0.98996, F1: 0.1124, Prec: 0.2233, Rec: 0.0819 lr: 0.00080\n",
      "Epoch 1/Step 1800, Loss: -0.08809, Accuracy: 0.99005, F1: 0.1126, Prec: 0.2230, Rec: 0.0821 lr: 0.00080\n",
      "Epoch 1/Step 1820, Loss: -0.06439, Accuracy: 0.99013, F1: 0.1127, Prec: 0.2227, Rec: 0.0822 lr: 0.00080\n",
      "Epoch 1/Step 1840, Loss: -0.07194, Accuracy: 0.99022, F1: 0.1130, Prec: 0.2226, Rec: 0.0824 lr: 0.00080\n",
      "Epoch 1/Step 1860, Loss: -0.05187, Accuracy: 0.99030, F1: 0.1131, Prec: 0.2222, Rec: 0.0825 lr: 0.00080\n",
      "Epoch 1/Step 1880, Loss: -0.06481, Accuracy: 0.99038, F1: 0.1131, Prec: 0.2219, Rec: 0.0826 lr: 0.00080\n",
      "Epoch 1/Step 1900, Loss: -0.06756, Accuracy: 0.99046, F1: 0.1133, Prec: 0.2216, Rec: 0.0827 lr: 0.00080\n",
      "Epoch 1/Step 1920, Loss: -0.08100, Accuracy: 0.99054, F1: 0.1134, Prec: 0.2213, Rec: 0.0828 lr: 0.00080\n",
      "Epoch 1/Step 1940, Loss: -0.06541, Accuracy: 0.99061, F1: 0.1135, Prec: 0.2209, Rec: 0.0829 lr: 0.00080\n",
      "Epoch 1/Step 1960, Loss: -0.08146, Accuracy: 0.99069, F1: 0.1136, Prec: 0.2206, Rec: 0.0830 lr: 0.00080\n",
      "Epoch 1/Step 1980, Loss: -0.07953, Accuracy: 0.99076, F1: 0.1137, Prec: 0.2203, Rec: 0.0831 lr: 0.00080\n",
      "Epoch 1/Step 2000, Loss: -0.09989, Accuracy: 0.99083, F1: 0.1137, Prec: 0.2199, Rec: 0.0832 lr: 0.00080\n",
      "Epoch 1/Step 2020, Loss: -0.07051, Accuracy: 0.99090, F1: 0.1140, Prec: 0.2198, Rec: 0.0833 lr: 0.00080\n",
      "Epoch 1/Step 2040, Loss: -0.05716, Accuracy: 0.99097, F1: 0.1140, Prec: 0.2195, Rec: 0.0834 lr: 0.00080\n",
      "Epoch 1/Step 2060, Loss: -0.06417, Accuracy: 0.99104, F1: 0.1141, Prec: 0.2193, Rec: 0.0835 lr: 0.00080\n",
      "Epoch 1/Step 2080, Loss: -0.09384, Accuracy: 0.99110, F1: 0.1142, Prec: 0.2190, Rec: 0.0836 lr: 0.00080\n",
      "Epoch 1/Step 2100, Loss: -0.07698, Accuracy: 0.99117, F1: 0.1143, Prec: 0.2187, Rec: 0.0837 lr: 0.00080\n",
      "Epoch 1/Step 2120, Loss: -0.07736, Accuracy: 0.99123, F1: 0.1144, Prec: 0.2185, Rec: 0.0837 lr: 0.00080\n",
      "Epoch 1/Step 2140, Loss: -0.07460, Accuracy: 0.99129, F1: 0.1145, Prec: 0.2184, Rec: 0.0838 lr: 0.00080\n",
      "Epoch 1/Step 2160, Loss: -0.09105, Accuracy: 0.99136, F1: 0.1146, Prec: 0.2181, Rec: 0.0840 lr: 0.00080\n",
      "Epoch 1/Step 2180, Loss: -0.09202, Accuracy: 0.99142, F1: 0.1147, Prec: 0.2178, Rec: 0.0841 lr: 0.00080\n",
      "Epoch 1/Step 2200, Loss: -0.06718, Accuracy: 0.99147, F1: 0.1148, Prec: 0.2176, Rec: 0.0841 lr: 0.00080\n",
      "Epoch 1/Step 2220, Loss: -0.06361, Accuracy: 0.99153, F1: 0.1149, Prec: 0.2173, Rec: 0.0842 lr: 0.00080\n",
      "Epoch 1/Step 2240, Loss: -0.06325, Accuracy: 0.99159, F1: 0.1149, Prec: 0.2169, Rec: 0.0843 lr: 0.00080\n",
      "Epoch 1/Step 2260, Loss: -0.07104, Accuracy: 0.99164, F1: 0.1150, Prec: 0.2168, Rec: 0.0843 lr: 0.00080\n",
      "Epoch 1/Step 2280, Loss: -0.06451, Accuracy: 0.99170, F1: 0.1151, Prec: 0.2166, Rec: 0.0844 lr: 0.00080\n",
      "Epoch 1/Step 2300, Loss: -0.06926, Accuracy: 0.99175, F1: 0.1152, Prec: 0.2164, Rec: 0.0845 lr: 0.00080\n",
      "Epoch 1/Step 2320, Loss: -0.08987, Accuracy: 0.99181, F1: 0.1153, Prec: 0.2162, Rec: 0.0846 lr: 0.00080\n",
      "Epoch 1/Step 2340, Loss: -0.08314, Accuracy: 0.99186, F1: 0.1154, Prec: 0.2161, Rec: 0.0847 lr: 0.00080\n",
      "Epoch 1/Step 2360, Loss: -0.09119, Accuracy: 0.99191, F1: 0.1155, Prec: 0.2160, Rec: 0.0848 lr: 0.00080\n",
      "Epoch 1/Step 2380, Loss: -0.07257, Accuracy: 0.99196, F1: 0.1156, Prec: 0.2157, Rec: 0.0849 lr: 0.00080\n",
      "Epoch 1/Step 2400, Loss: -0.06097, Accuracy: 0.99201, F1: 0.1157, Prec: 0.2155, Rec: 0.0849 lr: 0.00080\n",
      "Epoch 1/Step 2420, Loss: -0.06038, Accuracy: 0.99206, F1: 0.1158, Prec: 0.2153, Rec: 0.0850 lr: 0.00080\n",
      "Epoch 1/Step 2440, Loss: -0.07186, Accuracy: 0.99211, F1: 0.1158, Prec: 0.2150, Rec: 0.0850 lr: 0.00080\n",
      "Epoch 1/Step 2460, Loss: -0.08766, Accuracy: 0.99215, F1: 0.1158, Prec: 0.2148, Rec: 0.0851 lr: 0.00080\n",
      "Epoch 1/Step 2480, Loss: -0.06312, Accuracy: 0.99220, F1: 0.1159, Prec: 0.2147, Rec: 0.0852 lr: 0.00080\n",
      "Epoch 1/Step 2500, Loss: -0.06296, Accuracy: 0.99224, F1: 0.1160, Prec: 0.2145, Rec: 0.0853 lr: 0.00080\n",
      "Epoch 1/Step 2520, Loss: -0.04769, Accuracy: 0.99229, F1: 0.1160, Prec: 0.2144, Rec: 0.0853 lr: 0.00080\n",
      "Epoch 1/Step 2540, Loss: -0.05396, Accuracy: 0.99233, F1: 0.1161, Prec: 0.2141, Rec: 0.0853 lr: 0.00080\n",
      "Epoch 1/Step 2560, Loss: -0.08611, Accuracy: 0.99238, F1: 0.1162, Prec: 0.2141, Rec: 0.0854 lr: 0.00080\n",
      "Epoch 1/Step 2580, Loss: -0.09389, Accuracy: 0.99242, F1: 0.1162, Prec: 0.2138, Rec: 0.0855 lr: 0.00080\n",
      "Epoch 1/Step 2600, Loss: -0.06900, Accuracy: 0.99246, F1: 0.1163, Prec: 0.2137, Rec: 0.0855 lr: 0.00080\n",
      "Epoch 1/Step 2620, Loss: -0.08471, Accuracy: 0.99250, F1: 0.1164, Prec: 0.2137, Rec: 0.0856 lr: 0.00080\n",
      "Epoch 1/Step 2640, Loss: -0.08948, Accuracy: 0.99254, F1: 0.1164, Prec: 0.2135, Rec: 0.0857 lr: 0.00080\n",
      "Epoch 1/Step 2660, Loss: -0.07210, Accuracy: 0.99258, F1: 0.1165, Prec: 0.2133, Rec: 0.0857 lr: 0.00080\n",
      "Epoch 1/Step 2680, Loss: -0.07008, Accuracy: 0.99262, F1: 0.1166, Prec: 0.2132, Rec: 0.0858 lr: 0.00080\n",
      "Epoch 1/Step 2700, Loss: -0.08408, Accuracy: 0.99266, F1: 0.1167, Prec: 0.2131, Rec: 0.0859 lr: 0.00080\n",
      "Epoch 1/Step 2720, Loss: -0.06905, Accuracy: 0.99270, F1: 0.1168, Prec: 0.2130, Rec: 0.0860 lr: 0.00080\n",
      "Epoch 1/Step 2740, Loss: -0.08345, Accuracy: 0.99274, F1: 0.1169, Prec: 0.2129, Rec: 0.0861 lr: 0.00080\n",
      "Epoch 1/Step 2760, Loss: -0.09918, Accuracy: 0.99278, F1: 0.1169, Prec: 0.2127, Rec: 0.0861 lr: 0.00080\n",
      "Epoch 1/Step 2780, Loss: -0.08435, Accuracy: 0.99281, F1: 0.1169, Prec: 0.2124, Rec: 0.0861 lr: 0.00080\n",
      "Epoch 1/Step 2800, Loss: -0.06750, Accuracy: 0.99285, F1: 0.1169, Prec: 0.2123, Rec: 0.0861 lr: 0.00080\n",
      "Epoch 1/Step 2820, Loss: -0.08501, Accuracy: 0.99288, F1: 0.1170, Prec: 0.2122, Rec: 0.0862 lr: 0.00080\n",
      "Epoch 1/Step 2840, Loss: -0.08418, Accuracy: 0.99292, F1: 0.1170, Prec: 0.2121, Rec: 0.0862 lr: 0.00080\n",
      "Epoch 1/Step 2860, Loss: -0.08286, Accuracy: 0.99295, F1: 0.1171, Prec: 0.2120, Rec: 0.0863 lr: 0.00080\n",
      "Epoch 1/Step 2880, Loss: -0.07198, Accuracy: 0.99299, F1: 0.1172, Prec: 0.2118, Rec: 0.0863 lr: 0.00080\n",
      "Epoch 1/Step 2900, Loss: -0.08575, Accuracy: 0.99302, F1: 0.1172, Prec: 0.2115, Rec: 0.0864 lr: 0.00080\n",
      "Epoch 1/Step 2920, Loss: -0.05999, Accuracy: 0.99306, F1: 0.1173, Prec: 0.2114, Rec: 0.0865 lr: 0.00080\n",
      "Epoch 1/Step 2940, Loss: -0.09324, Accuracy: 0.99309, F1: 0.1173, Prec: 0.2112, Rec: 0.0865 lr: 0.00080\n",
      "Epoch 1/Step 2960, Loss: -0.10254, Accuracy: 0.99312, F1: 0.1173, Prec: 0.2110, Rec: 0.0865 lr: 0.00080\n",
      "Epoch 1/Step 2980, Loss: -0.06739, Accuracy: 0.99315, F1: 0.1174, Prec: 0.2108, Rec: 0.0866 lr: 0.00080\n",
      "Epoch 1/Step 3000, Loss: -0.05730, Accuracy: 0.99318, F1: 0.1173, Prec: 0.2106, Rec: 0.0866 lr: 0.00080\n",
      "Epoch 1/Step 3020, Loss: -0.08959, Accuracy: 0.99322, F1: 0.1174, Prec: 0.2104, Rec: 0.0866 lr: 0.00080\n",
      "Epoch 1/Step 3040, Loss: -0.08438, Accuracy: 0.99325, F1: 0.1174, Prec: 0.2103, Rec: 0.0866 lr: 0.00080\n",
      "Epoch 1/Step 3060, Loss: -0.05621, Accuracy: 0.99328, F1: 0.1174, Prec: 0.2101, Rec: 0.0867 lr: 0.00080\n",
      "Epoch 1/Step 3080, Loss: -0.06121, Accuracy: 0.99331, F1: 0.1175, Prec: 0.2101, Rec: 0.0868 lr: 0.00080\n",
      "Epoch 1/Step 3100, Loss: -0.08041, Accuracy: 0.99334, F1: 0.1176, Prec: 0.2099, Rec: 0.0868 lr: 0.00080\n",
      "Epoch finished. Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 17:40:31.852034: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.9979\n",
      "Validation f1: 0.1234\n",
      "Validation precision: 0.1914\n",
      "Validation recall: 0.0923\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch 2/Step 0, Loss: -0.07839, Accuracy: 0.99788, F1: 0.1318, Prec: 0.2224, Rec: 0.0936 lr: 0.00080\n",
      "Epoch 2/Step 20, Loss: -0.07374, Accuracy: 0.99785, F1: 0.1313, Prec: 0.1972, Rec: 0.0991 lr: 0.00080\n",
      "Epoch 2/Step 40, Loss: -0.07163, Accuracy: 0.99788, F1: 0.1273, Prec: 0.1963, Rec: 0.0952 lr: 0.00080\n",
      "Epoch 2/Step 60, Loss: -0.07821, Accuracy: 0.99789, F1: 0.1243, Prec: 0.1930, Rec: 0.0927 lr: 0.00080\n",
      "Epoch 2/Step 80, Loss: -0.07871, Accuracy: 0.99789, F1: 0.1255, Prec: 0.1958, Rec: 0.0934 lr: 0.00080\n",
      "Epoch 2/Step 100, Loss: -0.08266, Accuracy: 0.99790, F1: 0.1257, Prec: 0.1966, Rec: 0.0935 lr: 0.00080\n",
      "Epoch 2/Step 120, Loss: -0.09499, Accuracy: 0.99790, F1: 0.1254, Prec: 0.1966, Rec: 0.0931 lr: 0.00080\n",
      "Epoch 2/Step 140, Loss: -0.07821, Accuracy: 0.99790, F1: 0.1239, Prec: 0.1941, Rec: 0.0921 lr: 0.00080\n",
      "Epoch 2/Step 160, Loss: -0.07985, Accuracy: 0.99790, F1: 0.1232, Prec: 0.1929, Rec: 0.0915 lr: 0.00080\n",
      "Epoch 2/Step 180, Loss: -0.07011, Accuracy: 0.99790, F1: 0.1231, Prec: 0.1919, Rec: 0.0916 lr: 0.00080\n",
      "Epoch 2/Step 200, Loss: -0.08947, Accuracy: 0.99790, F1: 0.1232, Prec: 0.1921, Rec: 0.0917 lr: 0.00080\n",
      "Epoch 2/Step 220, Loss: -0.09045, Accuracy: 0.99790, F1: 0.1233, Prec: 0.1916, Rec: 0.0919 lr: 0.00080\n",
      "Epoch 2/Step 240, Loss: -0.07559, Accuracy: 0.99789, F1: 0.1235, Prec: 0.1918, Rec: 0.0922 lr: 0.00080\n",
      "Epoch 2/Step 260, Loss: -0.07604, Accuracy: 0.99790, F1: 0.1233, Prec: 0.1912, Rec: 0.0921 lr: 0.00080\n",
      "Epoch 2/Step 280, Loss: -0.09225, Accuracy: 0.99790, F1: 0.1236, Prec: 0.1907, Rec: 0.0926 lr: 0.00080\n",
      "Epoch 2/Step 300, Loss: -0.07309, Accuracy: 0.99790, F1: 0.1243, Prec: 0.1905, Rec: 0.0934 lr: 0.00080\n",
      "Epoch 2/Step 320, Loss: -0.07654, Accuracy: 0.99791, F1: 0.1243, Prec: 0.1901, Rec: 0.0935 lr: 0.00080\n",
      "Epoch 2/Step 340, Loss: -0.06152, Accuracy: 0.99791, F1: 0.1242, Prec: 0.1898, Rec: 0.0935 lr: 0.00080\n",
      "Epoch 2/Step 360, Loss: -0.09286, Accuracy: 0.99792, F1: 0.1242, Prec: 0.1890, Rec: 0.0937 lr: 0.00080\n",
      "Epoch 2/Step 380, Loss: -0.05951, Accuracy: 0.99792, F1: 0.1241, Prec: 0.1885, Rec: 0.0938 lr: 0.00080\n",
      "Epoch 2/Step 400, Loss: -0.06944, Accuracy: 0.99792, F1: 0.1246, Prec: 0.1883, Rec: 0.0944 lr: 0.00080\n",
      "Epoch 2/Step 420, Loss: -0.07378, Accuracy: 0.99792, F1: 0.1246, Prec: 0.1881, Rec: 0.0945 lr: 0.00080\n",
      "Epoch 2/Step 440, Loss: -0.07275, Accuracy: 0.99793, F1: 0.1248, Prec: 0.1882, Rec: 0.0946 lr: 0.00080\n",
      "Epoch 2/Step 460, Loss: -0.06494, Accuracy: 0.99793, F1: 0.1250, Prec: 0.1883, Rec: 0.0948 lr: 0.00080\n",
      "Epoch 2/Step 480, Loss: -0.08272, Accuracy: 0.99793, F1: 0.1253, Prec: 0.1883, Rec: 0.0951 lr: 0.00080\n",
      "Epoch 2/Step 500, Loss: -0.07155, Accuracy: 0.99793, F1: 0.1253, Prec: 0.1880, Rec: 0.0952 lr: 0.00080\n",
      "Epoch 2/Step 520, Loss: -0.10281, Accuracy: 0.99793, F1: 0.1254, Prec: 0.1880, Rec: 0.0954 lr: 0.00080\n",
      "Epoch 2/Step 540, Loss: -0.07489, Accuracy: 0.99793, F1: 0.1257, Prec: 0.1883, Rec: 0.0957 lr: 0.00080\n",
      "Epoch 2/Step 560, Loss: -0.06508, Accuracy: 0.99793, F1: 0.1258, Prec: 0.1881, Rec: 0.0959 lr: 0.00080\n",
      "Epoch 2/Step 580, Loss: -0.09542, Accuracy: 0.99794, F1: 0.1256, Prec: 0.1878, Rec: 0.0957 lr: 0.00080\n",
      "Epoch 2/Step 600, Loss: -0.09457, Accuracy: 0.99794, F1: 0.1256, Prec: 0.1877, Rec: 0.0958 lr: 0.00080\n",
      "Epoch 2/Step 620, Loss: -0.07862, Accuracy: 0.99794, F1: 0.1255, Prec: 0.1877, Rec: 0.0956 lr: 0.00080\n",
      "Epoch 2/Step 640, Loss: -0.06740, Accuracy: 0.99794, F1: 0.1255, Prec: 0.1877, Rec: 0.0956 lr: 0.00080\n",
      "Epoch 2/Step 660, Loss: -0.09678, Accuracy: 0.99794, F1: 0.1254, Prec: 0.1874, Rec: 0.0956 lr: 0.00080\n",
      "Epoch 2/Step 680, Loss: -0.07210, Accuracy: 0.99794, F1: 0.1257, Prec: 0.1875, Rec: 0.0959 lr: 0.00080\n",
      "Epoch 2/Step 700, Loss: -0.09469, Accuracy: 0.99794, F1: 0.1256, Prec: 0.1873, Rec: 0.0958 lr: 0.00080\n",
      "Epoch 2/Step 720, Loss: -0.08224, Accuracy: 0.99794, F1: 0.1257, Prec: 0.1871, Rec: 0.0959 lr: 0.00080\n",
      "Epoch 2/Step 740, Loss: -0.05267, Accuracy: 0.99795, F1: 0.1254, Prec: 0.1870, Rec: 0.0956 lr: 0.00080\n",
      "Epoch 2/Step 760, Loss: -0.06913, Accuracy: 0.99795, F1: 0.1253, Prec: 0.1870, Rec: 0.0955 lr: 0.00080\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mStart of epoch \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,))\n\u001b[1;32m     75\u001b[0m \u001b[39m# Iterate over the batches of the dataset.\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[39mfor\u001b[39;00m step, (x_batch_train, y_batch_train) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batchedDataset):\n\u001b[1;32m     78\u001b[0m     loss_value \u001b[39m=\u001b[39mtrainStep(x_batch_train,y_batch_train)\n\u001b[1;32m     80\u001b[0m     \u001b[39m# Log \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:797\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    796\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 797\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[1;32m    798\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    799\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:780\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[0;32m--> 780\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[1;32m    781\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[1;32m    782\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[1;32m    783\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[1;32m    785\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3011\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3009\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3010\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3011\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3012\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mIteratorGetNext\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, iterator, \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types,\n\u001b[1;32m   3013\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes)\n\u001b[1;32m   3014\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3015\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE=32\n",
    "LOG_INTERVAL=20\n",
    "epochs = 15\n",
    "saveModel=False\n",
    "\n",
    "\n",
    "log_dir = \"./logs/\"+model.name+\"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_\"+SO\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,\n",
    "                                                      write_graph=True, update_freq=5)\n",
    "\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# Instantiate an optimizer .\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=8e-4)\n",
    "\n",
    "# Instantiate a loss function.\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "# loss_fn = WeightedBinaryCE(np.ones(len(mlb.classes_)))\n",
    "# loss_fn = WeightedBinaryCE(labelWeightsCorr)\n",
    "# loss_fn = WeightedComboLoss(labelWeightsCorr, alpha=0.5, beta=0.5, labelSmoothing=0.05)\n",
    "loss_fn = WeightedComboLoss(labelWeightsCorr, alpha=0.5, beta=0.5)\n",
    "\n",
    "train_acc_metric = WeightedAccuracy(classWeights=labelWeightsCorr)\n",
    "train_f1_metric = WeightedF1(classWeights=labelWeightsCorr, threshold=0.5)\n",
    "train_prec = WeightedPrecision(classWeights=labelWeightsCorr)\n",
    "train_rec = WeightedRecall(classWeights=labelWeightsCorr)\n",
    "\n",
    "val_acc_metric = WeightedAccuracy(classWeights=labelWeightsCorr)\n",
    "val_f1_metric = WeightedF1(classWeights=labelWeightsCorr, threshold=0.5)\n",
    "val_prec = WeightedPrecision(classWeights=labelWeightsCorr)\n",
    "val_rec = WeightedRecall(classWeights=labelWeightsCorr)\n",
    "\n",
    "batchedDataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "batchedDatasetVal = datasetVal.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "# batchedDataset = batchedDataset.cache(os.path.join(DATA_PATH, \"datasetCache\"+SO))\n",
    "# batchedDatasetVal = batchedDatasetVal.cache(os.path.join(DATA_PATH, \"datasetCacheVal\"+SO))\n",
    "\n",
    "@tf.function()\n",
    "def trainStep(x_batch_train, y_batch_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        probs = model(x_batch_train, training=True) \n",
    "        loss_value = loss_fn(y_batch_train, probs)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    #Gradient clipping\n",
    "    # grads = [tf.clip_by_norm(g, 2.0) for g in grads]\n",
    "\n",
    "    train_acc_metric.update_state(y_batch_train, probs)\n",
    "    train_f1_metric.update_state(y_batch_train, probs)\n",
    "    train_prec.update_state(y_batch_train, probs)\n",
    "    train_rec.update_state(y_batch_train, probs)\n",
    "\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "    return loss_value\n",
    "\n",
    "@tf.function()\n",
    "def valStep(x_batch_val, y_batch_val):\n",
    "    valProbs = model(x_batch_val, training=False)\n",
    "    # Update val metrics\n",
    "    val_acc_metric.update_state(y_batch_val, valProbs)\n",
    "    val_f1_metric.update_state(y_batch_val, valProbs)\n",
    "    val_prec.update_state(y_batch_val, valProbs)\n",
    "    val_rec.update_state(y_batch_val, valProbs)\n",
    "\n",
    "maxStep=0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch+1,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(batchedDataset):\n",
    "\n",
    "        loss_value =trainStep(x_batch_train,y_batch_train)\n",
    "\n",
    "        # Log \n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            template = 'Epoch {}/Step {}, Loss: {:.5f}, Accuracy: {:.5f}, F1: {:.4f}, Prec: {:.4f}, Rec: {:.4f} lr: {:.5f}'\n",
    "            print(template.format(epoch+1, step,loss_value.numpy(), \n",
    "                                    train_acc_metric.result(),train_f1_metric.result(),\n",
    "                                    train_prec.result(), train_rec.result(), optimizer.learning_rate.numpy()))\n",
    "            \n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', loss_value, step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('accuracy', train_acc_metric.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('f1', train_f1_metric.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('prec', train_prec.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('rec', train_rec.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('learning rate', optimizer.learning_rate.numpy(), step=maxStep*epoch+step)\n",
    "                summary_writer.flush()\n",
    "\n",
    "    \n",
    "    train_acc_metric.reset_states()\n",
    "    train_f1_metric.reset_states()\n",
    "    train_prec.reset_states()\n",
    "    train_rec.reset_states()\n",
    "\n",
    "    maxStep=step\n",
    "\n",
    "    print(\"Epoch finished. Start validation\")\n",
    "    for x_batch_val, y_batch_val in batchedDatasetVal:\n",
    "        valStep(x_batch_val, y_batch_val)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    val_f1 = val_f1_metric.result()\n",
    "    val_f1_metric.reset_states()\n",
    "    val_precision = val_prec.result()\n",
    "    val_prec.reset_states()\n",
    "    val_recall = val_rec.result()\n",
    "    val_rec.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Validation f1: %.4f\" % (float(val_f1),))\n",
    "    print(\"Validation precision: %.4f\" % (float(val_precision),))\n",
    "    print(\"Validation recall: %.4f\" % (float(val_recall),))\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('valAcc', float(val_acc), step=epoch)\n",
    "        tf.summary.scalar('valF1', float(val_f1), step=epoch)\n",
    "        tf.summary.scalar('valPrecision', float(val_precision), step=epoch)\n",
    "        tf.summary.scalar('valRecall', float(val_recall), step=epoch)\n",
    "        summary_writer.flush()\n",
    "    if saveModel:\n",
    "      model.save(os.path.join(DATA_PATH, \"model_\"+SO+\"_epoch_{}_valF1Score{:.3f}\".format(epoch, float(val_f1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.save(os.path.join(DATA_PATH, \"model_\"+SO+\"_epoch_{}_valf1Score{:.3f}\".format(epoch, float(val_f1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probs= model.predict(tf.expand_dims(list(datasetVal.take(32))[10][0], 0))\n",
    "prediction= [1 if p > 0.5 else 0 for p in probs[0]]\n",
    "probabilities= probs[probs>0.5]\n",
    "# classes = np.argwhere(prediction)\n",
    "print(mlb.inverse_transform(np.array([prediction])))\n",
    "print(probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
