{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1d77e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 16:10:38.764939: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-27 16:10:39.253556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "/mnt/e/ML/cafa-5-protein-function-prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 16:10:39.998953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-27 16:10:40.015686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-27 16:10:40.016062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "# try:\n",
    "#   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# except:\n",
    "#   # Invalid device or cannot modify virtual devices once initialized.\n",
    "#   pass\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "print(DATA_PATH)\n",
    "\n",
    "# Choose subontology (CCO, MFO or BPO)\n",
    "SO = 'MFO'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3544f8a",
   "metadata": {},
   "source": [
    "## Reading fasta, obo and tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b515f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "sequences = [rec.seq for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Train/train_sequences.fasta\"),\"fasta\")]\n",
    "ids = [rec.id for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Train/train_sequences.fasta\"),\"fasta\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2898414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import obonet\n",
    "\n",
    "# Read the taxrank ontology\n",
    "url = os.path.join(DATA_PATH, \"Train/go-basic.obo\")\n",
    "graph = obonet.read_obo(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4bf949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670114, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "dfSO = df.loc[df[\"aspect\"]==SO]\n",
    "uniqueTerms = dfSO[\"term\"].unique()\n",
    "termsArr = list(dfSO[\"term\"].to_numpy())\n",
    "\n",
    "uniqueTermsDict={}\n",
    "for i,el in enumerate(uniqueTerms):\n",
    "    uniqueTermsDict[el] = i\n",
    "    \n",
    "print(dfSO.shape)\n",
    "df=dfSO\n",
    "\n",
    "df.set_index(\"EntryID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4dec705",
   "metadata": {},
   "outputs": [],
   "source": [
    "testID = df.index.to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fff067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGo = pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "dfGo = dfGo.loc[dfGo[\"aspect\"]==SO]\n",
    "\n",
    "dfGo.set_index(\"term\", inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5338b7f8",
   "metadata": {},
   "source": [
    "## GO analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1058ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = df[\"term\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb35584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name = {id_: data.get('name') for id_, data in graph.nodes(data=True)}\n",
    "name_to_id = {data['name']: id_ for id_, data in graph.nodes(data=True) if 'name' in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7651a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantNodes=[]\n",
    "for layer, nodes in enumerate(networkx.topological_generations(graph)):\n",
    "    if layer<20:\n",
    "        relevantNodes = relevantNodes + nodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c099fc2",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cae9a3b0",
   "metadata": {},
   "source": [
    "The task is a multilabel classification: The output has several possible targets (Gene Ontologies) but each can only be 1 (existing) or 0 (non existing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8e63ed9",
   "metadata": {},
   "source": [
    "Extract label weights from IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7e3c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found GOs: 0 (set to 0)\n"
     ]
    }
   ],
   "source": [
    "dfIa = pd.read_csv(os.path.join(DATA_PATH, \"IA.txt\"), sep='\\t', header=None)\n",
    "\n",
    "dfIa.set_index(0, inplace=True)\n",
    "\n",
    "labelWeights=[]\n",
    "allIndices = dfIa.index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "notFound=0\n",
    "for go in item_counts.index.to_list():\n",
    "    if go in allIndices:\n",
    "        labelWeights.append(dfIa.loc[go].to_numpy()[0])\n",
    "    else:\n",
    "        notFound += 1\n",
    "        labelWeights.append(0)\n",
    "\n",
    "print(\"Not found GOs: {} (set to 0)\".format(notFound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caeee590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading presaved data\n"
     ]
    }
   ],
   "source": [
    "topGOs=item_counts.index.to_list()\n",
    "\n",
    "threshold=0\n",
    "labelWeights=np.array(labelWeights)\n",
    "selection = labelWeights>threshold\n",
    "topGOs=np.array(topGOs)[selection]\n",
    "\n",
    "if os.path.exists(os.path.join(DATA_PATH, \"GODataSizes_\"+SO+\".npy\")):\n",
    "    print(\"Loading presaved data\")\n",
    "    GODataSizes = np.load(os.path.join(DATA_PATH, \"GODataSizes_\"+SO+\".npy\"))\n",
    "else:\n",
    "    GODataSizes= [dfGo.loc[g].size for g in topGOs]\n",
    "    np.save(os.path.join(DATA_PATH, \"GODataSizes_\"+SO), GODataSizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e87f301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2099\n"
     ]
    }
   ],
   "source": [
    "#At least 10 samples\n",
    "print(np.count_nonzero(np.array(GODataSizes)>20))\n",
    "GODataSizes= np.array(GODataSizes)\n",
    "GOsWithSufficientData = topGOs[GODataSizes>20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61e2ef95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2099\n"
     ]
    }
   ],
   "source": [
    "#Reduce to relevant GOs (see Data Exploration.ipynb)\n",
    "\n",
    "finalGOSet =[]\n",
    "for g in GOsWithSufficientData:\n",
    "    if g in relevantNodes:\n",
    "        finalGOSet.append(g)\n",
    "\n",
    "print(len(finalGOSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99f6a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GO:0003674' 'GO:0003953' 'GO:0016787' 'GO:0016799' 'GO:0016798'\n",
      " 'GO:0003824']\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "2099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:895: UserWarning: unknown class(es) ['GO:0003674'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([finalGOSet])\n",
    "\n",
    "dftest=df.loc[testID]\n",
    "indices = dftest[\"term\"].to_numpy()\n",
    "print(indices)\n",
    "print(mlb.transform([indices]))\n",
    "print(len(mlb.classes_))\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'MLB_'+SO+'.pkl'), 'wb') as f:\n",
    "    pickle.dump(mlb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb189ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found GOs: 0 (set to 0)\n"
     ]
    }
   ],
   "source": [
    "labelWeightsCorr=[]\n",
    "\n",
    "notFound=0\n",
    "for go in mlb.classes_:\n",
    "    if go in allIndices:\n",
    "        labelWeightsCorr.append(dfIa.loc[go].to_numpy()[0])\n",
    "    else:\n",
    "        notFound += 1\n",
    "        labelWeightsCorr.append(0)\n",
    "\n",
    "print(\"Not found GOs: {} (set to 0)\".format(notFound))\n",
    "labelWeightsCorr=np.array(labelWeightsCorr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92019f8c",
   "metadata": {},
   "source": [
    "## Amino acids encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e42462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_dict = {'A': 1, 'B':24, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'O': 21, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'U': 22, 'V': 18, 'W': 19, 'Y': 20, 'X':30, 'Z':23}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfd823ba",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "431f9df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max. length of the sequences is 35375\n"
     ]
    }
   ],
   "source": [
    "seqLengths = [len(seq) for seq in sequences]\n",
    "maxLen = max(seqLengths)\n",
    "print(\"The max. length of the sequences is {}\".format(maxLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e581e06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A0A009IHW8' 'A0A023FBW4' 'A0A023FBW7' ... 'X5L1L5' 'X5L565' 'X5M5N0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfAll=pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "soEntries = dfAll.loc[dfAll[\"aspect\"]==SO]\n",
    "soEntryIds = soEntries[\"EntryID\"].unique()\n",
    "\n",
    "# print(len(seqEntries))\n",
    "print(soEntryIds)\n",
    "\n",
    "# SoSequences = []\n",
    "# for entry in soEntryIds:\n",
    "#     SoSequences.append(sequences[ids.index(entry)])\n",
    "\n",
    "# print(len(SoSequences))\n",
    "dfAll.set_index(\"EntryID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44f722e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99572\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "TRAIN_VAL_SPLIT = 0.7\n",
    "nonRelevantThreshold=0.01\n",
    "k = 3\n",
    "\n",
    "allAA = list(aa_dict.keys())\n",
    "allAA.sort()\n",
    "allCombinations= list(product(*(allAA for i in range(k))))\n",
    "allCombinations=np.array([''.join(el) for el in allCombinations])\n",
    "\n",
    "positionDict = dict(zip(allCombinations, np.arange(0,allCombinations.size).T))\n",
    "\n",
    "#Use numpy vectorize to speed up the mapping (hopefully)\n",
    "mapping = lambda x: aa_dict[x]\n",
    "vectMapping = np.vectorize(mapping)\n",
    "\n",
    "# Shuffle the data\n",
    "import random\n",
    "random.seed(516213)\n",
    "c = list(zip(sequences, ids))\n",
    "random.shuffle(c)\n",
    "sequencesShuffle, idsShuffle = zip(*c)\n",
    "\n",
    "\n",
    "#Train Validation Split\n",
    "split = int(np.floor(len(sequencesShuffle)*TRAIN_VAL_SPLIT))\n",
    "print(split)\n",
    "trainSeq = sequencesShuffle[0:split]\n",
    "valSeq = sequencesShuffle[split+1:]\n",
    "trainIds = idsShuffle[0:split]\n",
    "valIds = idsShuffle[split+1:]\n",
    "\n",
    "\n",
    "def generator():\n",
    "  for i,seq in enumerate(trainSeq):\n",
    "      entryId = trainIds[i]\n",
    "      if entryId in soEntryIds:\n",
    "        labelData = df.loc[entryId]\n",
    "        # indices = labelData[\"termToken\"].to_numpy()\n",
    "        indices = labelData[\"term\"].to_numpy()\n",
    "      else: \n",
    "        indices=[]\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "          #supress the warnings for unknown classes\n",
    "          warnings.simplefilter(\"ignore\")\n",
    "          y = mlb.transform([indices])\n",
    "\n",
    "      # if np.count_nonzero(y)==0 and np.random.random()>nonRelevantThreshold:\n",
    "      #   continue\n",
    "\n",
    "      \n",
    "      kmers = [seq[j:j+k] if j < len(seq)-(k-1) else 0 for j,el in enumerate(seq)]\n",
    "      kmers = kmers[0:-(k-1)]\n",
    "      kmers = [str(el) for el in kmers]\n",
    "      values, counts = np.unique(kmers, return_counts=True)\n",
    "      freqVector=np.zeros(allCombinations.shape)\n",
    "      for l,v in enumerate(values):\n",
    "          freqVector[positionDict[v]] = counts[l]\n",
    "      yield (freqVector,y[0])\n",
    "\n",
    "\n",
    "def generatorVal():\n",
    "  for i,seq in enumerate(valSeq):\n",
    "      entryId = valIds[i]\n",
    "      if entryId in soEntryIds:\n",
    "        labelData = df.loc[entryId]\n",
    "        # indices = labelData[\"termToken\"].to_numpy()\n",
    "        indices = labelData[\"term\"].to_numpy()\n",
    "      else:\n",
    "        indices=[]\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "          #supress the warnings for unknown classes\n",
    "          warnings.simplefilter(\"ignore\")\n",
    "          y = mlb.transform([indices])\n",
    "\n",
    "      # if np.count_nonzero(y)==0 and np.random.random()>nonRelevantThreshold:\n",
    "      #   continue\n",
    "      \n",
    "      kmers = [seq[j:j+k] if j < len(seq)-(k-1) else 0 for j,el in enumerate(seq)]\n",
    "      kmers = kmers[0:-(k-1)]\n",
    "      kmers = [str(el) for el in kmers]\n",
    "      values, counts = np.unique(kmers, return_counts=True)\n",
    "      freqVector=np.zeros(allCombinations.shape)\n",
    "      for l,v in enumerate(values):\n",
    "          freqVector[positionDict[v]] = counts[l]\n",
    "      yield (freqVector,y[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2727338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sample: \n",
      "(15625,)\n",
      "[0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0.]\n",
      "The first sample has 5 classes\n"
     ]
    }
   ],
   "source": [
    "g = generator()\n",
    "test = next(g)\n",
    "print(\"The first sample: \\n{}\\n{}\".format(test[0].shape, test[0][0:100]))\n",
    "print(\"The first sample has {} classes\".format(np.count_nonzero(test[1])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10c4a51a",
   "metadata": {},
   "source": [
    "## Tensorflow Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63f0d2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 16:10:55.826014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-27 16:10:55.826439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-27 16:10:55.826753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-27 16:10:56.301940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-27 16:10:56.302390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-27 16:10:56.302403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-07-27 16:10:56.302727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-27 16:10:56.302760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6569 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:2b:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<tf.Tensor: shape=(15625,), dtype=int32, numpy=array([0, 0, 0, ..., 0, 0, 0], dtype=int32)>, <tf.Tensor: shape=(2099,), dtype=int32, numpy=array([0, 0, 0, ..., 0, 0, 0], dtype=int32)>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 16:10:56.620867: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xSize = allCombinations.shape[0]\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(generator, output_signature=(\n",
    "         tf.TensorSpec(shape=(xSize,), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(len(mlb.classes_),), dtype=tf.int32)))\n",
    "print(list(dataset.take(1)))\n",
    "\n",
    "datasetVal = tf.data.Dataset.from_generator(generatorVal, output_signature=(\n",
    "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(len(mlb.classes_),), dtype=tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0a554f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98752d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedConvModel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 15625)]           0         \n",
      "                                                                 \n",
      " tf.expand_dims (TFOpLambda)  (None, 15625, 1)         0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 15625, 1)         4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 15619, 16)         128       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 15619, 16)         0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 15619, 16)        64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 15613, 16)         1808      \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 15613, 16)         0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 15613, 16)        64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 15607, 16)         1808      \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 15607, 16)         0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 15607, 16)        64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 15601, 32)         3616      \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 15601, 32)         0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 15601, 32)        128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 7798, 32)          7200      \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 7798, 32)          0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 7798, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 3896, 32)          7200      \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 3896, 32)          0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 3896, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 1945, 32)          7200      \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 1945, 32)          0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 1945, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 970, 32)           7200      \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 970, 32)           0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 970, 32)          128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 31040)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1986624   \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2099)              136435    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,168,375\n",
      "Trainable params: 2,167,957\n",
      "Non-trainable params: 418\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "VOCAB_SIZE=len(aa_dict)\n",
    "EMBED_DIM=10\n",
    "\n",
    "def createModel():\n",
    "    inputs = tf.keras.Input(shape=(xSize,))\n",
    "    # x=layers.Embedding(VOCAB_SIZE, EMBED_DIM, name=\"embedding\")(inputs)\n",
    "    x = layers.BatchNormalization()(tf.expand_dims(inputs,2))\n",
    "\n",
    "    x=layers.Conv1D(16, 7)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x=layers.Conv1D(16, 7)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x=layers.Conv1D(16, 7)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x=layers.Conv1D(32, 7)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x=layers.Conv1D(32, 7,strides=2)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x=layers.Conv1D(32, 7,strides=2)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x=layers.Conv1D(32, 7, strides=2)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x=layers.Conv1D(32, 7, strides=2)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    # x=layers.Conv1D(32, 5, activation=tf.keras.activations.relu)(x)\n",
    "    # x=layers.Conv1D(32, 5, activation=tf.keras.activations.relu)(x)\n",
    "    # x=layers.Conv1D(32, 5, activation=tf.keras.activations.relu)(x)\n",
    "    x=layers.Flatten()(x)\n",
    "    x=layers.Dense(64)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x=layers.Dense(64)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x=layers.Dense(64)(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    outputs=layers.Dense(len(mlb.classes_), activation=tf.keras.activations.sigmoid)(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"embedConvModel\")\n",
    "\n",
    "model = createModel()\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a3ddc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "decaySteps=5000\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate, first_decay_steps=decaySteps,\n",
    "                                                                t_mul=2.0, m_mul=0.7)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "#     initial_learning_rate, decay_steps=decaySteps, alpha=0.01)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate,decay_steps=decaySteps,decay_rate=0.9,staircase=False)\n",
    "step = np.linspace(0,decaySteps*3)\n",
    "lr = lr_schedule(step)\n",
    "# plt.figure(figsize = (8,6))\n",
    "# # plt.yscale(\"log\")\n",
    "# plt.plot(step, lr)\n",
    "# plt.ylim([0,max(plt.ylim())])\n",
    "# plt.xlabel('step')\n",
    "# _ = plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2f43e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9113c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 16:10:57.163314: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-07-27 16:11:00.397726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-07-27 16:11:01.217357: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f8761554ad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-27 16:11:01.217386: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1\n",
      "2023-07-27 16:11:01.222321: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-07-27 16:11:01.329855: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/Step 0, Loss: 0.00352, Accuracy: 0.49599, F1: 0.0019, Prec: 0.3178, Rec: 0.0010, lr: 0.00060\n",
      "Epoch 1/Step 20, Loss: -0.00976, Accuracy: 0.74429, F1: 0.0073, Prec: 0.3422, Rec: 0.0037, lr: 0.00060\n",
      "Epoch 1/Step 40, Loss: -0.04904, Accuracy: 0.86318, F1: 0.0395, Prec: 0.3177, Rec: 0.0230, lr: 0.00060\n",
      "Epoch 1/Step 60, Loss: -0.06171, Accuracy: 0.90690, F1: 0.0672, Prec: 0.2967, Rec: 0.0433, lr: 0.00060\n",
      "Epoch 1/Step 80, Loss: -0.07428, Accuracy: 0.92937, F1: 0.0865, Prec: 0.2742, Rec: 0.0612, lr: 0.00060\n",
      "Epoch 1/Step 100, Loss: -0.07691, Accuracy: 0.94302, F1: 0.1003, Prec: 0.2562, Rec: 0.0765, lr: 0.00060\n",
      "Epoch 1/Step 120, Loss: -0.07211, Accuracy: 0.95217, F1: 0.1114, Prec: 0.2433, Rec: 0.0909, lr: 0.00060\n",
      "Epoch 1/Step 140, Loss: -0.08005, Accuracy: 0.95872, F1: 0.1188, Prec: 0.2350, Rec: 0.0996, lr: 0.00060\n",
      "Epoch 1/Step 160, Loss: -0.07806, Accuracy: 0.96365, F1: 0.1247, Prec: 0.2292, Rec: 0.1060, lr: 0.00060\n",
      "Epoch 1/Step 180, Loss: -0.10824, Accuracy: 0.96749, F1: 0.1305, Prec: 0.2257, Rec: 0.1127, lr: 0.00060\n",
      "Epoch 1/Step 200, Loss: -0.09089, Accuracy: 0.97056, F1: 0.1335, Prec: 0.2212, Rec: 0.1163, lr: 0.00060\n",
      "Epoch 1/Step 220, Loss: -0.10287, Accuracy: 0.97308, F1: 0.1376, Prec: 0.2187, Rec: 0.1212, lr: 0.00060\n",
      "Epoch 1/Step 240, Loss: -0.11187, Accuracy: 0.97518, F1: 0.1403, Prec: 0.2158, Rec: 0.1246, lr: 0.00060\n",
      "Epoch 1/Step 260, Loss: -0.09263, Accuracy: 0.97696, F1: 0.1429, Prec: 0.2134, Rec: 0.1281, lr: 0.00060\n",
      "Epoch 1/Step 280, Loss: -0.09233, Accuracy: 0.97848, F1: 0.1450, Prec: 0.2117, Rec: 0.1307, lr: 0.00060\n",
      "Epoch 1/Step 300, Loss: -0.10017, Accuracy: 0.97980, F1: 0.1462, Prec: 0.2094, Rec: 0.1324, lr: 0.00060\n",
      "Epoch 1/Step 320, Loss: -0.10338, Accuracy: 0.98096, F1: 0.1472, Prec: 0.2066, Rec: 0.1344, lr: 0.00060\n",
      "Epoch 1/Step 340, Loss: -0.08396, Accuracy: 0.98198, F1: 0.1487, Prec: 0.2056, Rec: 0.1363, lr: 0.00060\n",
      "Epoch 1/Step 360, Loss: -0.09638, Accuracy: 0.98289, F1: 0.1495, Prec: 0.2035, Rec: 0.1378, lr: 0.00060\n",
      "Epoch 1/Step 380, Loss: -0.10275, Accuracy: 0.98371, F1: 0.1504, Prec: 0.2021, Rec: 0.1390, lr: 0.00060\n",
      "Epoch 1/Step 400, Loss: -0.07412, Accuracy: 0.98445, F1: 0.1509, Prec: 0.2010, Rec: 0.1397, lr: 0.00060\n",
      "Epoch 1/Step 420, Loss: -0.09802, Accuracy: 0.98511, F1: 0.1520, Prec: 0.2005, Rec: 0.1409, lr: 0.00060\n",
      "Epoch 1/Step 440, Loss: -0.10208, Accuracy: 0.98572, F1: 0.1530, Prec: 0.1999, Rec: 0.1421, lr: 0.00060\n",
      "Epoch 1/Step 460, Loss: -0.09311, Accuracy: 0.98627, F1: 0.1537, Prec: 0.1992, Rec: 0.1430, lr: 0.00060\n",
      "Epoch 1/Step 480, Loss: -0.09100, Accuracy: 0.98677, F1: 0.1545, Prec: 0.1985, Rec: 0.1439, lr: 0.00060\n",
      "Epoch 1/Step 500, Loss: -0.10550, Accuracy: 0.98724, F1: 0.1553, Prec: 0.1984, Rec: 0.1447, lr: 0.00060\n",
      "Epoch 1/Step 520, Loss: -0.08578, Accuracy: 0.98766, F1: 0.1560, Prec: 0.1980, Rec: 0.1456, lr: 0.00060\n",
      "Epoch 1/Step 540, Loss: -0.11015, Accuracy: 0.98806, F1: 0.1569, Prec: 0.1979, Rec: 0.1464, lr: 0.00060\n",
      "Epoch 1/Step 560, Loss: -0.10998, Accuracy: 0.98843, F1: 0.1576, Prec: 0.1981, Rec: 0.1468, lr: 0.00060\n",
      "Epoch 1/Step 580, Loss: -0.10675, Accuracy: 0.98877, F1: 0.1581, Prec: 0.1980, Rec: 0.1475, lr: 0.00060\n",
      "Epoch 1/Step 600, Loss: -0.11305, Accuracy: 0.98909, F1: 0.1586, Prec: 0.1981, Rec: 0.1477, lr: 0.00060\n",
      "Epoch 1/Step 620, Loss: -0.12248, Accuracy: 0.98939, F1: 0.1597, Prec: 0.1986, Rec: 0.1488, lr: 0.00060\n",
      "Epoch 1/Step 640, Loss: -0.11906, Accuracy: 0.98967, F1: 0.1600, Prec: 0.1980, Rec: 0.1493, lr: 0.00060\n",
      "Epoch 1/Step 660, Loss: -0.10508, Accuracy: 0.98994, F1: 0.1602, Prec: 0.1971, Rec: 0.1500, lr: 0.00060\n",
      "Epoch 1/Step 680, Loss: -0.09651, Accuracy: 0.99018, F1: 0.1606, Prec: 0.1970, Rec: 0.1504, lr: 0.00060\n",
      "Epoch 1/Step 700, Loss: -0.10017, Accuracy: 0.99042, F1: 0.1608, Prec: 0.1964, Rec: 0.1507, lr: 0.00060\n",
      "Epoch 1/Step 720, Loss: -0.12667, Accuracy: 0.99064, F1: 0.1611, Prec: 0.1964, Rec: 0.1509, lr: 0.00060\n",
      "Epoch 1/Step 740, Loss: -0.09045, Accuracy: 0.99085, F1: 0.1613, Prec: 0.1960, Rec: 0.1512, lr: 0.00060\n",
      "Epoch 1/Step 760, Loss: -0.11029, Accuracy: 0.99105, F1: 0.1614, Prec: 0.1958, Rec: 0.1513, lr: 0.00060\n",
      "Epoch 1/Step 780, Loss: -0.10144, Accuracy: 0.99124, F1: 0.1617, Prec: 0.1956, Rec: 0.1515, lr: 0.00060\n",
      "Epoch 1/Step 800, Loss: -0.09923, Accuracy: 0.99142, F1: 0.1616, Prec: 0.1949, Rec: 0.1517, lr: 0.00060\n",
      "Epoch 1/Step 820, Loss: -0.09968, Accuracy: 0.99159, F1: 0.1619, Prec: 0.1948, Rec: 0.1520, lr: 0.00060\n",
      "Epoch 1/Step 840, Loss: -0.12897, Accuracy: 0.99175, F1: 0.1623, Prec: 0.1948, Rec: 0.1524, lr: 0.00060\n",
      "Epoch 1/Step 860, Loss: -0.10930, Accuracy: 0.99191, F1: 0.1623, Prec: 0.1944, Rec: 0.1524, lr: 0.00060\n",
      "Epoch 1/Step 880, Loss: -0.10773, Accuracy: 0.99206, F1: 0.1625, Prec: 0.1942, Rec: 0.1527, lr: 0.00060\n",
      "Epoch 1/Step 900, Loss: -0.11145, Accuracy: 0.99220, F1: 0.1627, Prec: 0.1941, Rec: 0.1529, lr: 0.00060\n",
      "Epoch 1/Step 920, Loss: -0.11591, Accuracy: 0.99233, F1: 0.1628, Prec: 0.1938, Rec: 0.1530, lr: 0.00060\n",
      "Epoch 1/Step 940, Loss: -0.08710, Accuracy: 0.99246, F1: 0.1630, Prec: 0.1937, Rec: 0.1533, lr: 0.00060\n",
      "Epoch 1/Step 960, Loss: -0.11560, Accuracy: 0.99258, F1: 0.1633, Prec: 0.1937, Rec: 0.1535, lr: 0.00060\n",
      "Epoch 1/Step 980, Loss: -0.11442, Accuracy: 0.99270, F1: 0.1635, Prec: 0.1935, Rec: 0.1538, lr: 0.00060\n",
      "Epoch 1/Step 1000, Loss: -0.11109, Accuracy: 0.99282, F1: 0.1635, Prec: 0.1932, Rec: 0.1537, lr: 0.00060\n",
      "Epoch 1/Step 1020, Loss: -0.10701, Accuracy: 0.99292, F1: 0.1635, Prec: 0.1927, Rec: 0.1540, lr: 0.00060\n",
      "Epoch 1/Step 1040, Loss: -0.10091, Accuracy: 0.99303, F1: 0.1637, Prec: 0.1928, Rec: 0.1541, lr: 0.00060\n",
      "Epoch 1/Step 1060, Loss: -0.09550, Accuracy: 0.99313, F1: 0.1637, Prec: 0.1926, Rec: 0.1542, lr: 0.00060\n",
      "Epoch 1/Step 1080, Loss: -0.10716, Accuracy: 0.99323, F1: 0.1638, Prec: 0.1925, Rec: 0.1543, lr: 0.00060\n",
      "Epoch 1/Step 1100, Loss: -0.11515, Accuracy: 0.99332, F1: 0.1640, Prec: 0.1927, Rec: 0.1543, lr: 0.00060\n",
      "Epoch 1/Step 1120, Loss: -0.10237, Accuracy: 0.99342, F1: 0.1640, Prec: 0.1925, Rec: 0.1543, lr: 0.00060\n",
      "Epoch 1/Step 1140, Loss: -0.10854, Accuracy: 0.99350, F1: 0.1644, Prec: 0.1926, Rec: 0.1547, lr: 0.00060\n",
      "Epoch 1/Step 1160, Loss: -0.08633, Accuracy: 0.99359, F1: 0.1644, Prec: 0.1926, Rec: 0.1546, lr: 0.00060\n",
      "Epoch 1/Step 1180, Loss: -0.10020, Accuracy: 0.99367, F1: 0.1644, Prec: 0.1923, Rec: 0.1546, lr: 0.00060\n",
      "Epoch 1/Step 1200, Loss: -0.08351, Accuracy: 0.99375, F1: 0.1644, Prec: 0.1923, Rec: 0.1546, lr: 0.00060\n",
      "Epoch 1/Step 1220, Loss: -0.10376, Accuracy: 0.99383, F1: 0.1646, Prec: 0.1923, Rec: 0.1548, lr: 0.00060\n",
      "Epoch 1/Step 1240, Loss: -0.13297, Accuracy: 0.99390, F1: 0.1646, Prec: 0.1921, Rec: 0.1548, lr: 0.00060\n",
      "Epoch 1/Step 1260, Loss: -0.08941, Accuracy: 0.99397, F1: 0.1646, Prec: 0.1922, Rec: 0.1548, lr: 0.00060\n",
      "Epoch 1/Step 1280, Loss: -0.11257, Accuracy: 0.99404, F1: 0.1648, Prec: 0.1920, Rec: 0.1550, lr: 0.00060\n",
      "Epoch 1/Step 1300, Loss: -0.09662, Accuracy: 0.99411, F1: 0.1649, Prec: 0.1922, Rec: 0.1550, lr: 0.00060\n",
      "Epoch 1/Step 1320, Loss: -0.08201, Accuracy: 0.99417, F1: 0.1650, Prec: 0.1921, Rec: 0.1551, lr: 0.00060\n",
      "Epoch 1/Step 1340, Loss: -0.11288, Accuracy: 0.99423, F1: 0.1653, Prec: 0.1923, Rec: 0.1553, lr: 0.00060\n",
      "Epoch 1/Step 1360, Loss: -0.09256, Accuracy: 0.99430, F1: 0.1654, Prec: 0.1921, Rec: 0.1555, lr: 0.00060\n",
      "Epoch 1/Step 1380, Loss: -0.10374, Accuracy: 0.99436, F1: 0.1655, Prec: 0.1921, Rec: 0.1557, lr: 0.00060\n",
      "Epoch 1/Step 1400, Loss: -0.08783, Accuracy: 0.99441, F1: 0.1656, Prec: 0.1920, Rec: 0.1557, lr: 0.00060\n",
      "Epoch 1/Step 1420, Loss: -0.09127, Accuracy: 0.99447, F1: 0.1655, Prec: 0.1919, Rec: 0.1556, lr: 0.00060\n",
      "Epoch 1/Step 1440, Loss: -0.10302, Accuracy: 0.99452, F1: 0.1656, Prec: 0.1917, Rec: 0.1557, lr: 0.00060\n",
      "Epoch 1/Step 1460, Loss: -0.10734, Accuracy: 0.99458, F1: 0.1656, Prec: 0.1917, Rec: 0.1557, lr: 0.00060\n",
      "Epoch 1/Step 1480, Loss: -0.11378, Accuracy: 0.99463, F1: 0.1656, Prec: 0.1916, Rec: 0.1557, lr: 0.00060\n",
      "Epoch 1/Step 1500, Loss: -0.12121, Accuracy: 0.99468, F1: 0.1657, Prec: 0.1916, Rec: 0.1558, lr: 0.00060\n",
      "Epoch 1/Step 1520, Loss: -0.09954, Accuracy: 0.99473, F1: 0.1657, Prec: 0.1915, Rec: 0.1559, lr: 0.00060\n",
      "Epoch 1/Step 1540, Loss: -0.10609, Accuracy: 0.99478, F1: 0.1658, Prec: 0.1914, Rec: 0.1559, lr: 0.00060\n",
      "Epoch finished. Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 16:15:28.962324: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.9984\n",
      "Validation f1: 0.1699\n",
      "Validation precision: 0.1878\n",
      "Validation recall: 0.1601\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/e/ML/cafa-5-protein-function-prediction/model_MFO_epoch_0_valF1Score0.170/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/e/ML/cafa-5-protein-function-prediction/model_MFO_epoch_0_valF1Score0.170/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 2\n",
      "Epoch 2/Step 0, Loss: -0.09881, Accuracy: 0.99851, F1: 0.1733, Prec: 0.2175, Rec: 0.1441, lr: 0.00060\n",
      "Epoch 2/Step 20, Loss: -0.10761, Accuracy: 0.99846, F1: 0.1640, Prec: 0.1865, Rec: 0.1508, lr: 0.00060\n",
      "Epoch 2/Step 40, Loss: -0.10501, Accuracy: 0.99844, F1: 0.1654, Prec: 0.1810, Rec: 0.1565, lr: 0.00060\n",
      "Epoch 2/Step 60, Loss: -0.10088, Accuracy: 0.99844, F1: 0.1691, Prec: 0.1875, Rec: 0.1577, lr: 0.00060\n",
      "Epoch 2/Step 80, Loss: -0.10540, Accuracy: 0.99844, F1: 0.1687, Prec: 0.1877, Rec: 0.1570, lr: 0.00060\n",
      "Epoch 2/Step 100, Loss: -0.10297, Accuracy: 0.99844, F1: 0.1679, Prec: 0.1856, Rec: 0.1568, lr: 0.00060\n",
      "Epoch 2/Step 120, Loss: -0.08351, Accuracy: 0.99843, F1: 0.1682, Prec: 0.1843, Rec: 0.1586, lr: 0.00060\n",
      "Epoch 2/Step 140, Loss: -0.09823, Accuracy: 0.99843, F1: 0.1677, Prec: 0.1843, Rec: 0.1580, lr: 0.00060\n",
      "Epoch 2/Step 160, Loss: -0.09434, Accuracy: 0.99843, F1: 0.1676, Prec: 0.1848, Rec: 0.1575, lr: 0.00060\n",
      "Epoch 2/Step 180, Loss: -0.12180, Accuracy: 0.99842, F1: 0.1688, Prec: 0.1861, Rec: 0.1588, lr: 0.00060\n",
      "Epoch 2/Step 200, Loss: -0.10099, Accuracy: 0.99842, F1: 0.1682, Prec: 0.1856, Rec: 0.1581, lr: 0.00060\n",
      "Epoch 2/Step 220, Loss: -0.11062, Accuracy: 0.99842, F1: 0.1691, Prec: 0.1863, Rec: 0.1592, lr: 0.00060\n",
      "Epoch 2/Step 240, Loss: -0.12012, Accuracy: 0.99841, F1: 0.1692, Prec: 0.1861, Rec: 0.1595, lr: 0.00060\n",
      "Epoch 2/Step 260, Loss: -0.10110, Accuracy: 0.99841, F1: 0.1696, Prec: 0.1860, Rec: 0.1603, lr: 0.00060\n",
      "Epoch 2/Step 280, Loss: -0.10015, Accuracy: 0.99841, F1: 0.1699, Prec: 0.1863, Rec: 0.1607, lr: 0.00060\n",
      "Epoch 2/Step 300, Loss: -0.10869, Accuracy: 0.99841, F1: 0.1695, Prec: 0.1856, Rec: 0.1604, lr: 0.00060\n",
      "Epoch 2/Step 320, Loss: -0.11141, Accuracy: 0.99841, F1: 0.1690, Prec: 0.1843, Rec: 0.1607, lr: 0.00060\n",
      "Epoch 2/Step 340, Loss: -0.09195, Accuracy: 0.99841, F1: 0.1693, Prec: 0.1846, Rec: 0.1610, lr: 0.00060\n",
      "Epoch 2/Step 360, Loss: -0.10298, Accuracy: 0.99841, F1: 0.1689, Prec: 0.1837, Rec: 0.1612, lr: 0.00060\n",
      "Epoch 2/Step 380, Loss: -0.11152, Accuracy: 0.99841, F1: 0.1685, Prec: 0.1835, Rec: 0.1606, lr: 0.00060\n",
      "Epoch 2/Step 400, Loss: -0.08159, Accuracy: 0.99841, F1: 0.1682, Prec: 0.1833, Rec: 0.1603, lr: 0.00060\n",
      "Epoch 2/Step 420, Loss: -0.10397, Accuracy: 0.99841, F1: 0.1685, Prec: 0.1835, Rec: 0.1606, lr: 0.00060\n",
      "Epoch 2/Step 440, Loss: -0.11158, Accuracy: 0.99841, F1: 0.1688, Prec: 0.1837, Rec: 0.1609, lr: 0.00060\n",
      "Epoch 2/Step 460, Loss: -0.10058, Accuracy: 0.99841, F1: 0.1688, Prec: 0.1837, Rec: 0.1610, lr: 0.00060\n",
      "Epoch 2/Step 480, Loss: -0.10014, Accuracy: 0.99841, F1: 0.1689, Prec: 0.1837, Rec: 0.1611, lr: 0.00060\n",
      "Epoch 2/Step 500, Loss: -0.11291, Accuracy: 0.99841, F1: 0.1692, Prec: 0.1842, Rec: 0.1612, lr: 0.00060\n",
      "Epoch 2/Step 520, Loss: -0.09478, Accuracy: 0.99841, F1: 0.1694, Prec: 0.1843, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 2/Step 540, Loss: -0.11015, Accuracy: 0.99841, F1: 0.1697, Prec: 0.1848, Rec: 0.1618, lr: 0.00060\n",
      "Epoch 2/Step 560, Loss: -0.10999, Accuracy: 0.99841, F1: 0.1699, Prec: 0.1855, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 2/Step 580, Loss: -0.10675, Accuracy: 0.99841, F1: 0.1701, Prec: 0.1857, Rec: 0.1618, lr: 0.00060\n",
      "Epoch 2/Step 600, Loss: -0.11305, Accuracy: 0.99841, F1: 0.1701, Prec: 0.1863, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 2/Step 620, Loss: -0.12248, Accuracy: 0.99841, F1: 0.1709, Prec: 0.1871, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 2/Step 640, Loss: -0.11906, Accuracy: 0.99841, F1: 0.1709, Prec: 0.1869, Rec: 0.1623, lr: 0.00060\n",
      "Epoch 2/Step 660, Loss: -0.10508, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1864, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 2/Step 680, Loss: -0.09651, Accuracy: 0.99841, F1: 0.1708, Prec: 0.1865, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 2/Step 700, Loss: -0.10017, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1862, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 2/Step 720, Loss: -0.12667, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1865, Rec: 0.1624, lr: 0.00060\n",
      "Epoch 2/Step 740, Loss: -0.09045, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1864, Rec: 0.1624, lr: 0.00060\n",
      "Epoch 2/Step 760, Loss: -0.11029, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1864, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 2/Step 780, Loss: -0.10144, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1865, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 2/Step 800, Loss: -0.09923, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1861, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 2/Step 820, Loss: -0.09968, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 2/Step 840, Loss: -0.12897, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1863, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 2/Step 860, Loss: -0.10930, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1862, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 2/Step 880, Loss: -0.10773, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 2/Step 900, Loss: -0.11145, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1862, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 2/Step 920, Loss: -0.11591, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 2/Step 940, Loss: -0.08710, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 2/Step 960, Loss: -0.11560, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1862, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 2/Step 980, Loss: -0.11442, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1863, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 2/Step 1000, Loss: -0.11109, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 2/Step 1020, Loss: -0.10700, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1858, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 2/Step 1040, Loss: -0.10091, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1860, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 2/Step 1060, Loss: -0.09550, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1859, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 2/Step 1080, Loss: -0.10716, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1859, Rec: 0.1619, lr: 0.00060\n",
      "Epoch 2/Step 1100, Loss: -0.11515, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1862, Rec: 0.1619, lr: 0.00060\n",
      "Epoch 2/Step 1120, Loss: -0.10236, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1861, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 2/Step 1140, Loss: -0.10854, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1864, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 2/Step 1160, Loss: -0.08633, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1864, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 2/Step 1180, Loss: -0.10020, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1863, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 2/Step 1200, Loss: -0.08351, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1864, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 2/Step 1220, Loss: -0.10376, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1865, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 2/Step 1240, Loss: -0.13297, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1864, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 2/Step 1260, Loss: -0.08941, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1865, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 2/Step 1280, Loss: -0.11257, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1865, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 2/Step 1300, Loss: -0.09662, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1867, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 2/Step 1320, Loss: -0.08201, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 2/Step 1340, Loss: -0.11288, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1869, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 2/Step 1360, Loss: -0.09256, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1869, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 2/Step 1380, Loss: -0.10374, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1869, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 2/Step 1400, Loss: -0.08783, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1870, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 2/Step 1420, Loss: -0.09127, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1869, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 2/Step 1440, Loss: -0.10302, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1868, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 2/Step 1460, Loss: -0.10734, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 2/Step 1480, Loss: -0.11378, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 2/Step 1500, Loss: -0.12121, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 2/Step 1520, Loss: -0.09954, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1613, lr: 0.00060\n",
      "Epoch 2/Step 1540, Loss: -0.10609, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1613, lr: 0.00060\n",
      "Epoch finished. Start validation\n",
      "Validation acc: 0.9984\n",
      "Validation f1: 0.1699\n",
      "Validation precision: 0.1878\n",
      "Validation recall: 0.1601\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/e/ML/cafa-5-protein-function-prediction/model_MFO_epoch_1_valF1Score0.170/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/e/ML/cafa-5-protein-function-prediction/model_MFO_epoch_1_valF1Score0.170/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 3\n",
      "Epoch 3/Step 0, Loss: -0.09881, Accuracy: 0.99851, F1: 0.1733, Prec: 0.2175, Rec: 0.1441, lr: 0.00060\n",
      "Epoch 3/Step 20, Loss: -0.10762, Accuracy: 0.99846, F1: 0.1640, Prec: 0.1865, Rec: 0.1508, lr: 0.00060\n",
      "Epoch 3/Step 40, Loss: -0.10501, Accuracy: 0.99844, F1: 0.1654, Prec: 0.1810, Rec: 0.1565, lr: 0.00060\n",
      "Epoch 3/Step 60, Loss: -0.10088, Accuracy: 0.99844, F1: 0.1691, Prec: 0.1875, Rec: 0.1577, lr: 0.00060\n",
      "Epoch 3/Step 80, Loss: -0.10540, Accuracy: 0.99844, F1: 0.1687, Prec: 0.1877, Rec: 0.1570, lr: 0.00060\n",
      "Epoch 3/Step 100, Loss: -0.10297, Accuracy: 0.99844, F1: 0.1679, Prec: 0.1856, Rec: 0.1568, lr: 0.00060\n",
      "Epoch 3/Step 120, Loss: -0.08350, Accuracy: 0.99843, F1: 0.1682, Prec: 0.1843, Rec: 0.1586, lr: 0.00060\n",
      "Epoch 3/Step 140, Loss: -0.09823, Accuracy: 0.99843, F1: 0.1677, Prec: 0.1843, Rec: 0.1580, lr: 0.00060\n",
      "Epoch 3/Step 160, Loss: -0.09434, Accuracy: 0.99843, F1: 0.1676, Prec: 0.1848, Rec: 0.1575, lr: 0.00060\n",
      "Epoch 3/Step 180, Loss: -0.12180, Accuracy: 0.99842, F1: 0.1688, Prec: 0.1861, Rec: 0.1588, lr: 0.00060\n",
      "Epoch 3/Step 200, Loss: -0.10099, Accuracy: 0.99842, F1: 0.1682, Prec: 0.1856, Rec: 0.1581, lr: 0.00060\n",
      "Epoch 3/Step 220, Loss: -0.11062, Accuracy: 0.99842, F1: 0.1691, Prec: 0.1863, Rec: 0.1592, lr: 0.00060\n",
      "Epoch 3/Step 240, Loss: -0.12012, Accuracy: 0.99841, F1: 0.1692, Prec: 0.1861, Rec: 0.1595, lr: 0.00060\n",
      "Epoch 3/Step 260, Loss: -0.10110, Accuracy: 0.99841, F1: 0.1696, Prec: 0.1860, Rec: 0.1603, lr: 0.00060\n",
      "Epoch 3/Step 280, Loss: -0.10015, Accuracy: 0.99841, F1: 0.1699, Prec: 0.1863, Rec: 0.1607, lr: 0.00060\n",
      "Epoch 3/Step 300, Loss: -0.10870, Accuracy: 0.99841, F1: 0.1695, Prec: 0.1856, Rec: 0.1604, lr: 0.00060\n",
      "Epoch 3/Step 320, Loss: -0.11141, Accuracy: 0.99841, F1: 0.1690, Prec: 0.1843, Rec: 0.1607, lr: 0.00060\n",
      "Epoch 3/Step 340, Loss: -0.09195, Accuracy: 0.99841, F1: 0.1693, Prec: 0.1846, Rec: 0.1610, lr: 0.00060\n",
      "Epoch 3/Step 360, Loss: -0.10298, Accuracy: 0.99841, F1: 0.1689, Prec: 0.1837, Rec: 0.1612, lr: 0.00060\n",
      "Epoch 3/Step 380, Loss: -0.11152, Accuracy: 0.99841, F1: 0.1685, Prec: 0.1835, Rec: 0.1606, lr: 0.00060\n",
      "Epoch 3/Step 400, Loss: -0.08159, Accuracy: 0.99841, F1: 0.1682, Prec: 0.1833, Rec: 0.1603, lr: 0.00060\n",
      "Epoch 3/Step 420, Loss: -0.10397, Accuracy: 0.99841, F1: 0.1685, Prec: 0.1835, Rec: 0.1606, lr: 0.00060\n",
      "Epoch 3/Step 440, Loss: -0.11158, Accuracy: 0.99841, F1: 0.1688, Prec: 0.1837, Rec: 0.1609, lr: 0.00060\n",
      "Epoch 3/Step 460, Loss: -0.10058, Accuracy: 0.99841, F1: 0.1688, Prec: 0.1837, Rec: 0.1610, lr: 0.00060\n",
      "Epoch 3/Step 480, Loss: -0.10014, Accuracy: 0.99841, F1: 0.1689, Prec: 0.1837, Rec: 0.1611, lr: 0.00060\n",
      "Epoch 3/Step 500, Loss: -0.11291, Accuracy: 0.99841, F1: 0.1692, Prec: 0.1842, Rec: 0.1612, lr: 0.00060\n",
      "Epoch 3/Step 520, Loss: -0.09478, Accuracy: 0.99841, F1: 0.1694, Prec: 0.1843, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 3/Step 540, Loss: -0.11015, Accuracy: 0.99841, F1: 0.1697, Prec: 0.1848, Rec: 0.1618, lr: 0.00060\n",
      "Epoch 3/Step 560, Loss: -0.10998, Accuracy: 0.99841, F1: 0.1699, Prec: 0.1855, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 3/Step 580, Loss: -0.10675, Accuracy: 0.99841, F1: 0.1701, Prec: 0.1857, Rec: 0.1618, lr: 0.00060\n",
      "Epoch 3/Step 600, Loss: -0.11305, Accuracy: 0.99841, F1: 0.1701, Prec: 0.1863, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 3/Step 620, Loss: -0.12248, Accuracy: 0.99841, F1: 0.1709, Prec: 0.1871, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 3/Step 640, Loss: -0.11906, Accuracy: 0.99841, F1: 0.1709, Prec: 0.1869, Rec: 0.1623, lr: 0.00060\n",
      "Epoch 3/Step 660, Loss: -0.10508, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1864, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 3/Step 680, Loss: -0.09651, Accuracy: 0.99841, F1: 0.1708, Prec: 0.1865, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 3/Step 700, Loss: -0.10017, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1862, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 3/Step 720, Loss: -0.12667, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1865, Rec: 0.1624, lr: 0.00060\n",
      "Epoch 3/Step 740, Loss: -0.09045, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1864, Rec: 0.1624, lr: 0.00060\n",
      "Epoch 3/Step 760, Loss: -0.11029, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1864, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 3/Step 780, Loss: -0.10144, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1865, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 3/Step 800, Loss: -0.09923, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1861, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 3/Step 820, Loss: -0.09968, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 3/Step 840, Loss: -0.12897, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1863, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 3/Step 860, Loss: -0.10930, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1862, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 3/Step 880, Loss: -0.10773, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 3/Step 900, Loss: -0.11145, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1862, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 3/Step 920, Loss: -0.11591, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 3/Step 940, Loss: -0.08710, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 3/Step 960, Loss: -0.11560, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1862, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 3/Step 980, Loss: -0.11442, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1863, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 3/Step 1000, Loss: -0.11109, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 3/Step 1020, Loss: -0.10700, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1858, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 3/Step 1040, Loss: -0.10091, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1860, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 3/Step 1060, Loss: -0.09550, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1859, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 3/Step 1080, Loss: -0.10716, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1859, Rec: 0.1619, lr: 0.00060\n",
      "Epoch 3/Step 1100, Loss: -0.11515, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1862, Rec: 0.1619, lr: 0.00060\n",
      "Epoch 3/Step 1120, Loss: -0.10236, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1861, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 3/Step 1140, Loss: -0.10854, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1864, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 3/Step 1160, Loss: -0.08633, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1864, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 3/Step 1180, Loss: -0.10020, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1863, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 3/Step 1200, Loss: -0.08351, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1864, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 3/Step 1220, Loss: -0.10375, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1865, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 3/Step 1240, Loss: -0.13297, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1864, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 3/Step 1260, Loss: -0.08941, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1865, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 3/Step 1280, Loss: -0.11257, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1865, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 3/Step 1300, Loss: -0.09662, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1867, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 3/Step 1320, Loss: -0.08201, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 3/Step 1340, Loss: -0.11288, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1869, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 3/Step 1360, Loss: -0.09256, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1869, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 3/Step 1380, Loss: -0.10374, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1869, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 3/Step 1400, Loss: -0.08783, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1870, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 3/Step 1420, Loss: -0.09127, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1869, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 3/Step 1440, Loss: -0.10302, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1868, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 3/Step 1460, Loss: -0.10734, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 3/Step 1480, Loss: -0.11378, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 3/Step 1500, Loss: -0.12121, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 3/Step 1520, Loss: -0.09954, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1613, lr: 0.00060\n",
      "Epoch 3/Step 1540, Loss: -0.10609, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1613, lr: 0.00060\n",
      "Epoch finished. Start validation\n",
      "Validation acc: 0.9984\n",
      "Validation f1: 0.1699\n",
      "Validation precision: 0.1878\n",
      "Validation recall: 0.1601\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/e/ML/cafa-5-protein-function-prediction/model_MFO_epoch_2_valF1Score0.170/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/e/ML/cafa-5-protein-function-prediction/model_MFO_epoch_2_valF1Score0.170/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 4\n",
      "Epoch 4/Step 0, Loss: -0.09881, Accuracy: 0.99851, F1: 0.1733, Prec: 0.2175, Rec: 0.1441, lr: 0.00060\n",
      "Epoch 4/Step 20, Loss: -0.10761, Accuracy: 0.99846, F1: 0.1640, Prec: 0.1865, Rec: 0.1508, lr: 0.00060\n",
      "Epoch 4/Step 40, Loss: -0.10501, Accuracy: 0.99844, F1: 0.1654, Prec: 0.1810, Rec: 0.1565, lr: 0.00060\n",
      "Epoch 4/Step 60, Loss: -0.10088, Accuracy: 0.99844, F1: 0.1691, Prec: 0.1875, Rec: 0.1577, lr: 0.00060\n",
      "Epoch 4/Step 80, Loss: -0.10540, Accuracy: 0.99844, F1: 0.1687, Prec: 0.1877, Rec: 0.1570, lr: 0.00060\n",
      "Epoch 4/Step 100, Loss: -0.10297, Accuracy: 0.99844, F1: 0.1679, Prec: 0.1856, Rec: 0.1568, lr: 0.00060\n",
      "Epoch 4/Step 120, Loss: -0.08350, Accuracy: 0.99843, F1: 0.1682, Prec: 0.1843, Rec: 0.1586, lr: 0.00060\n",
      "Epoch 4/Step 140, Loss: -0.09823, Accuracy: 0.99843, F1: 0.1677, Prec: 0.1843, Rec: 0.1580, lr: 0.00060\n",
      "Epoch 4/Step 160, Loss: -0.09434, Accuracy: 0.99843, F1: 0.1676, Prec: 0.1848, Rec: 0.1575, lr: 0.00060\n",
      "Epoch 4/Step 180, Loss: -0.12180, Accuracy: 0.99842, F1: 0.1688, Prec: 0.1861, Rec: 0.1588, lr: 0.00060\n",
      "Epoch 4/Step 200, Loss: -0.10099, Accuracy: 0.99842, F1: 0.1682, Prec: 0.1856, Rec: 0.1581, lr: 0.00060\n",
      "Epoch 4/Step 220, Loss: -0.11062, Accuracy: 0.99842, F1: 0.1691, Prec: 0.1863, Rec: 0.1592, lr: 0.00060\n",
      "Epoch 4/Step 240, Loss: -0.12012, Accuracy: 0.99841, F1: 0.1692, Prec: 0.1861, Rec: 0.1595, lr: 0.00060\n",
      "Epoch 4/Step 260, Loss: -0.10110, Accuracy: 0.99841, F1: 0.1696, Prec: 0.1860, Rec: 0.1603, lr: 0.00060\n",
      "Epoch 4/Step 280, Loss: -0.10015, Accuracy: 0.99841, F1: 0.1699, Prec: 0.1863, Rec: 0.1607, lr: 0.00060\n",
      "Epoch 4/Step 300, Loss: -0.10869, Accuracy: 0.99841, F1: 0.1695, Prec: 0.1856, Rec: 0.1604, lr: 0.00060\n",
      "Epoch 4/Step 320, Loss: -0.11141, Accuracy: 0.99841, F1: 0.1690, Prec: 0.1843, Rec: 0.1607, lr: 0.00060\n",
      "Epoch 4/Step 340, Loss: -0.09195, Accuracy: 0.99841, F1: 0.1693, Prec: 0.1846, Rec: 0.1610, lr: 0.00060\n",
      "Epoch 4/Step 360, Loss: -0.10298, Accuracy: 0.99841, F1: 0.1689, Prec: 0.1837, Rec: 0.1612, lr: 0.00060\n",
      "Epoch 4/Step 380, Loss: -0.11152, Accuracy: 0.99841, F1: 0.1685, Prec: 0.1835, Rec: 0.1606, lr: 0.00060\n",
      "Epoch 4/Step 400, Loss: -0.08159, Accuracy: 0.99841, F1: 0.1682, Prec: 0.1833, Rec: 0.1603, lr: 0.00060\n",
      "Epoch 4/Step 420, Loss: -0.10397, Accuracy: 0.99841, F1: 0.1685, Prec: 0.1835, Rec: 0.1606, lr: 0.00060\n",
      "Epoch 4/Step 440, Loss: -0.11158, Accuracy: 0.99841, F1: 0.1688, Prec: 0.1837, Rec: 0.1609, lr: 0.00060\n",
      "Epoch 4/Step 460, Loss: -0.10058, Accuracy: 0.99841, F1: 0.1688, Prec: 0.1837, Rec: 0.1610, lr: 0.00060\n",
      "Epoch 4/Step 480, Loss: -0.10014, Accuracy: 0.99841, F1: 0.1689, Prec: 0.1837, Rec: 0.1611, lr: 0.00060\n",
      "Epoch 4/Step 500, Loss: -0.11291, Accuracy: 0.99841, F1: 0.1692, Prec: 0.1842, Rec: 0.1612, lr: 0.00060\n",
      "Epoch 4/Step 520, Loss: -0.09478, Accuracy: 0.99841, F1: 0.1694, Prec: 0.1843, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 4/Step 540, Loss: -0.11015, Accuracy: 0.99841, F1: 0.1697, Prec: 0.1848, Rec: 0.1618, lr: 0.00060\n",
      "Epoch 4/Step 560, Loss: -0.10998, Accuracy: 0.99841, F1: 0.1699, Prec: 0.1855, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 4/Step 580, Loss: -0.10675, Accuracy: 0.99841, F1: 0.1701, Prec: 0.1857, Rec: 0.1618, lr: 0.00060\n",
      "Epoch 4/Step 600, Loss: -0.11305, Accuracy: 0.99841, F1: 0.1701, Prec: 0.1863, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 4/Step 620, Loss: -0.12248, Accuracy: 0.99841, F1: 0.1709, Prec: 0.1871, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 4/Step 640, Loss: -0.11906, Accuracy: 0.99841, F1: 0.1709, Prec: 0.1869, Rec: 0.1623, lr: 0.00060\n",
      "Epoch 4/Step 660, Loss: -0.10508, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1864, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 4/Step 680, Loss: -0.09651, Accuracy: 0.99841, F1: 0.1708, Prec: 0.1865, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 4/Step 700, Loss: -0.10017, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1862, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 4/Step 720, Loss: -0.12667, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1865, Rec: 0.1624, lr: 0.00060\n",
      "Epoch 4/Step 740, Loss: -0.09045, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1864, Rec: 0.1624, lr: 0.00060\n",
      "Epoch 4/Step 760, Loss: -0.11029, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1864, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 4/Step 780, Loss: -0.10144, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1865, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 4/Step 800, Loss: -0.09923, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1861, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 4/Step 820, Loss: -0.09968, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 4/Step 840, Loss: -0.12897, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1863, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 4/Step 860, Loss: -0.10930, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1862, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 4/Step 880, Loss: -0.10773, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 4/Step 900, Loss: -0.11145, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1862, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 4/Step 920, Loss: -0.11591, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 4/Step 940, Loss: -0.08710, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 4/Step 960, Loss: -0.11560, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1862, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 4/Step 980, Loss: -0.11442, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1863, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 4/Step 1000, Loss: -0.11109, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 4/Step 1020, Loss: -0.10700, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1858, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 4/Step 1040, Loss: -0.10091, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1860, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 4/Step 1060, Loss: -0.09550, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1859, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 4/Step 1080, Loss: -0.10716, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1859, Rec: 0.1619, lr: 0.00060\n",
      "Epoch 4/Step 1100, Loss: -0.11515, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1862, Rec: 0.1619, lr: 0.00060\n",
      "Epoch 4/Step 1120, Loss: -0.10236, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1861, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 4/Step 1140, Loss: -0.10854, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1864, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 4/Step 1160, Loss: -0.08633, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1864, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 4/Step 1180, Loss: -0.10020, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1863, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 4/Step 1200, Loss: -0.08351, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1864, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 4/Step 1220, Loss: -0.10375, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1865, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 4/Step 1240, Loss: -0.13297, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1864, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 4/Step 1260, Loss: -0.08941, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1865, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 4/Step 1280, Loss: -0.11257, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1865, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 4/Step 1300, Loss: -0.09662, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1867, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 4/Step 1320, Loss: -0.08201, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 4/Step 1340, Loss: -0.11288, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1869, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 4/Step 1360, Loss: -0.09256, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1869, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 4/Step 1380, Loss: -0.10374, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1869, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 4/Step 1400, Loss: -0.08783, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1870, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 4/Step 1420, Loss: -0.09127, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1869, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 4/Step 1440, Loss: -0.10302, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1868, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 4/Step 1460, Loss: -0.10734, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 4/Step 1480, Loss: -0.11378, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 4/Step 1500, Loss: -0.12121, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 4/Step 1520, Loss: -0.09954, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1613, lr: 0.00060\n",
      "Epoch 4/Step 1540, Loss: -0.10609, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1613, lr: 0.00060\n",
      "Epoch finished. Start validation\n",
      "Validation acc: 0.9984\n",
      "Validation f1: 0.1699\n",
      "Validation precision: 0.1878\n",
      "Validation recall: 0.1601\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/e/ML/cafa-5-protein-function-prediction/model_MFO_epoch_3_valF1Score0.170/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/e/ML/cafa-5-protein-function-prediction/model_MFO_epoch_3_valF1Score0.170/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 5\n",
      "Epoch 5/Step 0, Loss: -0.09881, Accuracy: 0.99851, F1: 0.1733, Prec: 0.2175, Rec: 0.1441, lr: 0.00060\n",
      "Epoch 5/Step 20, Loss: -0.10761, Accuracy: 0.99846, F1: 0.1640, Prec: 0.1865, Rec: 0.1508, lr: 0.00060\n",
      "Epoch 5/Step 40, Loss: -0.10501, Accuracy: 0.99844, F1: 0.1654, Prec: 0.1810, Rec: 0.1565, lr: 0.00060\n",
      "Epoch 5/Step 60, Loss: -0.10088, Accuracy: 0.99844, F1: 0.1691, Prec: 0.1875, Rec: 0.1577, lr: 0.00060\n",
      "Epoch 5/Step 80, Loss: -0.10540, Accuracy: 0.99844, F1: 0.1687, Prec: 0.1877, Rec: 0.1570, lr: 0.00060\n",
      "Epoch 5/Step 100, Loss: -0.10297, Accuracy: 0.99844, F1: 0.1679, Prec: 0.1856, Rec: 0.1568, lr: 0.00060\n",
      "Epoch 5/Step 120, Loss: -0.08350, Accuracy: 0.99843, F1: 0.1682, Prec: 0.1843, Rec: 0.1586, lr: 0.00060\n",
      "Epoch 5/Step 140, Loss: -0.09823, Accuracy: 0.99843, F1: 0.1677, Prec: 0.1843, Rec: 0.1580, lr: 0.00060\n",
      "Epoch 5/Step 160, Loss: -0.09434, Accuracy: 0.99843, F1: 0.1676, Prec: 0.1848, Rec: 0.1575, lr: 0.00060\n",
      "Epoch 5/Step 180, Loss: -0.12180, Accuracy: 0.99842, F1: 0.1688, Prec: 0.1861, Rec: 0.1588, lr: 0.00060\n",
      "Epoch 5/Step 200, Loss: -0.10099, Accuracy: 0.99842, F1: 0.1682, Prec: 0.1856, Rec: 0.1581, lr: 0.00060\n",
      "Epoch 5/Step 220, Loss: -0.11062, Accuracy: 0.99842, F1: 0.1691, Prec: 0.1863, Rec: 0.1592, lr: 0.00060\n",
      "Epoch 5/Step 240, Loss: -0.12012, Accuracy: 0.99841, F1: 0.1692, Prec: 0.1861, Rec: 0.1595, lr: 0.00060\n",
      "Epoch 5/Step 260, Loss: -0.10110, Accuracy: 0.99841, F1: 0.1696, Prec: 0.1860, Rec: 0.1603, lr: 0.00060\n",
      "Epoch 5/Step 280, Loss: -0.10015, Accuracy: 0.99841, F1: 0.1699, Prec: 0.1863, Rec: 0.1607, lr: 0.00060\n",
      "Epoch 5/Step 300, Loss: -0.10869, Accuracy: 0.99841, F1: 0.1695, Prec: 0.1856, Rec: 0.1604, lr: 0.00060\n",
      "Epoch 5/Step 320, Loss: -0.11141, Accuracy: 0.99841, F1: 0.1690, Prec: 0.1843, Rec: 0.1607, lr: 0.00060\n",
      "Epoch 5/Step 340, Loss: -0.09195, Accuracy: 0.99841, F1: 0.1693, Prec: 0.1846, Rec: 0.1610, lr: 0.00060\n",
      "Epoch 5/Step 360, Loss: -0.10298, Accuracy: 0.99841, F1: 0.1689, Prec: 0.1837, Rec: 0.1612, lr: 0.00060\n",
      "Epoch 5/Step 380, Loss: -0.11152, Accuracy: 0.99841, F1: 0.1685, Prec: 0.1835, Rec: 0.1606, lr: 0.00060\n",
      "Epoch 5/Step 400, Loss: -0.08159, Accuracy: 0.99841, F1: 0.1682, Prec: 0.1833, Rec: 0.1603, lr: 0.00060\n",
      "Epoch 5/Step 420, Loss: -0.10397, Accuracy: 0.99841, F1: 0.1685, Prec: 0.1835, Rec: 0.1606, lr: 0.00060\n",
      "Epoch 5/Step 440, Loss: -0.11158, Accuracy: 0.99841, F1: 0.1688, Prec: 0.1837, Rec: 0.1609, lr: 0.00060\n",
      "Epoch 5/Step 460, Loss: -0.10058, Accuracy: 0.99841, F1: 0.1688, Prec: 0.1837, Rec: 0.1610, lr: 0.00060\n",
      "Epoch 5/Step 480, Loss: -0.10014, Accuracy: 0.99841, F1: 0.1689, Prec: 0.1837, Rec: 0.1611, lr: 0.00060\n",
      "Epoch 5/Step 500, Loss: -0.11291, Accuracy: 0.99841, F1: 0.1692, Prec: 0.1842, Rec: 0.1612, lr: 0.00060\n",
      "Epoch 5/Step 520, Loss: -0.09478, Accuracy: 0.99841, F1: 0.1694, Prec: 0.1843, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 5/Step 540, Loss: -0.11015, Accuracy: 0.99841, F1: 0.1697, Prec: 0.1848, Rec: 0.1618, lr: 0.00060\n",
      "Epoch 5/Step 560, Loss: -0.10998, Accuracy: 0.99841, F1: 0.1699, Prec: 0.1855, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 5/Step 580, Loss: -0.10675, Accuracy: 0.99841, F1: 0.1701, Prec: 0.1857, Rec: 0.1618, lr: 0.00060\n",
      "Epoch 5/Step 600, Loss: -0.11305, Accuracy: 0.99841, F1: 0.1701, Prec: 0.1863, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 5/Step 620, Loss: -0.12248, Accuracy: 0.99841, F1: 0.1709, Prec: 0.1871, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 5/Step 640, Loss: -0.11906, Accuracy: 0.99841, F1: 0.1709, Prec: 0.1869, Rec: 0.1623, lr: 0.00060\n",
      "Epoch 5/Step 660, Loss: -0.10508, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1864, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 5/Step 680, Loss: -0.09651, Accuracy: 0.99841, F1: 0.1708, Prec: 0.1865, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 5/Step 700, Loss: -0.10017, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1862, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 5/Step 720, Loss: -0.12667, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1865, Rec: 0.1624, lr: 0.00060\n",
      "Epoch 5/Step 740, Loss: -0.09045, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1864, Rec: 0.1624, lr: 0.00060\n",
      "Epoch 5/Step 760, Loss: -0.11029, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1864, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 5/Step 780, Loss: -0.10144, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1865, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 5/Step 800, Loss: -0.09923, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1861, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 5/Step 820, Loss: -0.09968, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 5/Step 840, Loss: -0.12897, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1863, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 5/Step 860, Loss: -0.10930, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1862, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 5/Step 880, Loss: -0.10773, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 5/Step 900, Loss: -0.11145, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1862, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 5/Step 920, Loss: -0.11591, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 5/Step 940, Loss: -0.08710, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1621, lr: 0.00060\n",
      "Epoch 5/Step 960, Loss: -0.11560, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1862, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 5/Step 980, Loss: -0.11442, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1863, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 5/Step 1000, Loss: -0.11109, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1861, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 5/Step 1020, Loss: -0.10700, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1858, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 5/Step 1040, Loss: -0.10091, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1860, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 5/Step 1060, Loss: -0.09550, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1859, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 5/Step 1080, Loss: -0.10716, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1859, Rec: 0.1619, lr: 0.00060\n",
      "Epoch 5/Step 1100, Loss: -0.11515, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1862, Rec: 0.1619, lr: 0.00060\n",
      "Epoch 5/Step 1120, Loss: -0.10236, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1861, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 5/Step 1140, Loss: -0.10854, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1864, Rec: 0.1620, lr: 0.00060\n",
      "Epoch 5/Step 1160, Loss: -0.08633, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1864, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 5/Step 1180, Loss: -0.10020, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1863, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 5/Step 1200, Loss: -0.08351, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1864, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 5/Step 1220, Loss: -0.10375, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1865, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 5/Step 1240, Loss: -0.13297, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1864, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 5/Step 1260, Loss: -0.08941, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1865, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 5/Step 1280, Loss: -0.11257, Accuracy: 0.99841, F1: 0.1702, Prec: 0.1865, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 5/Step 1300, Loss: -0.09662, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1867, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 5/Step 1320, Loss: -0.08201, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 5/Step 1340, Loss: -0.11288, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1869, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 5/Step 1360, Loss: -0.09256, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1869, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 5/Step 1380, Loss: -0.10374, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1869, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 5/Step 1400, Loss: -0.08783, Accuracy: 0.99841, F1: 0.1705, Prec: 0.1870, Rec: 0.1616, lr: 0.00060\n",
      "Epoch 5/Step 1420, Loss: -0.09127, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1869, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 5/Step 1440, Loss: -0.10302, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1868, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 5/Step 1460, Loss: -0.10734, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 5/Step 1480, Loss: -0.11378, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 5/Step 1500, Loss: -0.12121, Accuracy: 0.99841, F1: 0.1704, Prec: 0.1868, Rec: 0.1614, lr: 0.00060\n",
      "Epoch 5/Step 1520, Loss: -0.09954, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1613, lr: 0.00060\n",
      "Epoch 5/Step 1540, Loss: -0.10609, Accuracy: 0.99841, F1: 0.1703, Prec: 0.1868, Rec: 0.1613, lr: 0.00060\n",
      "Epoch finished. Start validation\n",
      "Validation acc: 0.9984\n",
      "Validation f1: 0.1699\n",
      "Validation precision: 0.1878\n",
      "Validation recall: 0.1601\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/e/ML/cafa-5-protein-function-prediction/model_MFO_epoch_4_valF1Score0.170/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/e/ML/cafa-5-protein-function-prediction/model_MFO_epoch_4_valF1Score0.170/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 6\n",
      "Epoch 6/Step 0, Loss: -0.09881, Accuracy: 0.99851, F1: 0.1733, Prec: 0.2175, Rec: 0.1441, lr: 0.00060\n",
      "Epoch 6/Step 20, Loss: -0.10761, Accuracy: 0.99846, F1: 0.1640, Prec: 0.1865, Rec: 0.1508, lr: 0.00060\n",
      "Epoch 6/Step 40, Loss: -0.10501, Accuracy: 0.99844, F1: 0.1654, Prec: 0.1810, Rec: 0.1565, lr: 0.00060\n",
      "Epoch 6/Step 60, Loss: -0.10088, Accuracy: 0.99844, F1: 0.1691, Prec: 0.1875, Rec: 0.1577, lr: 0.00060\n",
      "Epoch 6/Step 80, Loss: -0.10540, Accuracy: 0.99844, F1: 0.1687, Prec: 0.1877, Rec: 0.1570, lr: 0.00060\n",
      "Epoch 6/Step 100, Loss: -0.10297, Accuracy: 0.99844, F1: 0.1679, Prec: 0.1856, Rec: 0.1568, lr: 0.00060\n",
      "Epoch 6/Step 120, Loss: -0.08350, Accuracy: 0.99843, F1: 0.1682, Prec: 0.1843, Rec: 0.1586, lr: 0.00060\n",
      "Epoch 6/Step 140, Loss: -0.09823, Accuracy: 0.99843, F1: 0.1677, Prec: 0.1843, Rec: 0.1580, lr: 0.00060\n",
      "Epoch 6/Step 160, Loss: -0.09434, Accuracy: 0.99843, F1: 0.1676, Prec: 0.1848, Rec: 0.1575, lr: 0.00060\n",
      "Epoch 6/Step 180, Loss: -0.12180, Accuracy: 0.99842, F1: 0.1688, Prec: 0.1861, Rec: 0.1588, lr: 0.00060\n",
      "Epoch 6/Step 200, Loss: -0.10099, Accuracy: 0.99842, F1: 0.1682, Prec: 0.1856, Rec: 0.1581, lr: 0.00060\n",
      "Epoch 6/Step 220, Loss: -0.11062, Accuracy: 0.99842, F1: 0.1691, Prec: 0.1863, Rec: 0.1592, lr: 0.00060\n",
      "Epoch 6/Step 240, Loss: -0.12012, Accuracy: 0.99841, F1: 0.1692, Prec: 0.1861, Rec: 0.1595, lr: 0.00060\n",
      "Epoch 6/Step 260, Loss: -0.10110, Accuracy: 0.99841, F1: 0.1696, Prec: 0.1860, Rec: 0.1603, lr: 0.00060\n",
      "Epoch 6/Step 280, Loss: -0.10015, Accuracy: 0.99841, F1: 0.1699, Prec: 0.1863, Rec: 0.1607, lr: 0.00060\n",
      "Epoch 6/Step 300, Loss: -0.10869, Accuracy: 0.99841, F1: 0.1695, Prec: 0.1856, Rec: 0.1604, lr: 0.00060\n",
      "Epoch 6/Step 320, Loss: -0.11141, Accuracy: 0.99841, F1: 0.1690, Prec: 0.1843, Rec: 0.1607, lr: 0.00060\n",
      "Epoch 6/Step 340, Loss: -0.09195, Accuracy: 0.99841, F1: 0.1693, Prec: 0.1846, Rec: 0.1610, lr: 0.00060\n",
      "Epoch 6/Step 360, Loss: -0.10298, Accuracy: 0.99841, F1: 0.1689, Prec: 0.1837, Rec: 0.1612, lr: 0.00060\n",
      "Epoch 6/Step 380, Loss: -0.11152, Accuracy: 0.99841, F1: 0.1685, Prec: 0.1835, Rec: 0.1606, lr: 0.00060\n",
      "Epoch 6/Step 400, Loss: -0.08159, Accuracy: 0.99841, F1: 0.1682, Prec: 0.1833, Rec: 0.1603, lr: 0.00060\n",
      "Epoch 6/Step 420, Loss: -0.10397, Accuracy: 0.99841, F1: 0.1685, Prec: 0.1835, Rec: 0.1606, lr: 0.00060\n",
      "Epoch 6/Step 440, Loss: -0.11158, Accuracy: 0.99841, F1: 0.1688, Prec: 0.1837, Rec: 0.1609, lr: 0.00060\n",
      "Epoch 6/Step 460, Loss: -0.10058, Accuracy: 0.99841, F1: 0.1688, Prec: 0.1837, Rec: 0.1610, lr: 0.00060\n",
      "Epoch 6/Step 480, Loss: -0.10014, Accuracy: 0.99841, F1: 0.1689, Prec: 0.1837, Rec: 0.1611, lr: 0.00060\n",
      "Epoch 6/Step 500, Loss: -0.11291, Accuracy: 0.99841, F1: 0.1692, Prec: 0.1842, Rec: 0.1612, lr: 0.00060\n",
      "Epoch 6/Step 520, Loss: -0.09478, Accuracy: 0.99841, F1: 0.1694, Prec: 0.1843, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 6/Step 540, Loss: -0.11015, Accuracy: 0.99841, F1: 0.1697, Prec: 0.1848, Rec: 0.1618, lr: 0.00060\n",
      "Epoch 6/Step 560, Loss: -0.10998, Accuracy: 0.99841, F1: 0.1699, Prec: 0.1855, Rec: 0.1617, lr: 0.00060\n",
      "Epoch 6/Step 580, Loss: -0.10675, Accuracy: 0.99841, F1: 0.1701, Prec: 0.1857, Rec: 0.1618, lr: 0.00060\n",
      "Epoch 6/Step 600, Loss: -0.11305, Accuracy: 0.99841, F1: 0.1701, Prec: 0.1863, Rec: 0.1615, lr: 0.00060\n",
      "Epoch 6/Step 620, Loss: -0.12248, Accuracy: 0.99841, F1: 0.1709, Prec: 0.1871, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 6/Step 640, Loss: -0.11906, Accuracy: 0.99841, F1: 0.1709, Prec: 0.1869, Rec: 0.1623, lr: 0.00060\n",
      "Epoch 6/Step 660, Loss: -0.10508, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1864, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 6/Step 680, Loss: -0.09651, Accuracy: 0.99841, F1: 0.1708, Prec: 0.1865, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 6/Step 700, Loss: -0.10017, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1862, Rec: 0.1626, lr: 0.00060\n",
      "Epoch 6/Step 720, Loss: -0.12667, Accuracy: 0.99841, F1: 0.1707, Prec: 0.1865, Rec: 0.1624, lr: 0.00060\n",
      "Epoch 6/Step 740, Loss: -0.09045, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1864, Rec: 0.1624, lr: 0.00060\n",
      "Epoch 6/Step 760, Loss: -0.11029, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1864, Rec: 0.1622, lr: 0.00060\n",
      "Epoch 6/Step 780, Loss: -0.10144, Accuracy: 0.99841, F1: 0.1706, Prec: 0.1865, Rec: 0.1621, lr: 0.00060\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39m# Iterate over the batches of the dataset.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m step, (x_batch_train, y_batch_train) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batchedDataset):\n\u001b[0;32m---> 78\u001b[0m     loss_value \u001b[39m=\u001b[39mtrainStep(x_batch_train,y_batch_train)\n\u001b[1;32m     80\u001b[0m     \u001b[39m# Log \u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m LOG_INTERVAL \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE=64\n",
    "LOG_INTERVAL=20\n",
    "epochs = 15\n",
    "saveModel=True\n",
    "\n",
    "\n",
    "log_dir = \"./logs/\"+model.name+\"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_\"+SO\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,\n",
    "                                                      write_graph=True, update_freq=5)\n",
    "\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# Instantiate an optimizer .\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=6e-4)\n",
    "\n",
    "# Instantiate a loss function.\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "# loss_fn = WeightedBinaryCE(np.ones(len(mlb.classes_)))\n",
    "# loss_fn = WeightedBinaryCE(labelWeightsCorr)\n",
    "# loss_fn = WeightedComboLoss(labelWeightsCorr, alpha=0.5, beta=0.5, labelSmoothing=0.05)\n",
    "loss_fn = WeightedComboLoss(labelWeightsCorr, alpha=0.5, beta=0.5)\n",
    "\n",
    "train_acc_metric = WeightedAccuracy(classWeights=labelWeightsCorr)\n",
    "train_f1_metric = WeightedF1(classWeights=labelWeightsCorr, threshold=0.5)\n",
    "train_prec = WeightedPrecision(classWeights=labelWeightsCorr)\n",
    "train_rec = WeightedRecall(classWeights=labelWeightsCorr)\n",
    "\n",
    "val_acc_metric = WeightedAccuracy(classWeights=labelWeightsCorr)\n",
    "val_f1_metric = WeightedF1(classWeights=labelWeightsCorr, threshold=0.5)\n",
    "val_prec = WeightedPrecision(classWeights=labelWeightsCorr)\n",
    "val_rec = WeightedRecall(classWeights=labelWeightsCorr)\n",
    "\n",
    "batchedDataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "batchedDatasetVal = datasetVal.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "# batchedDataset = batchedDataset.cache(os.path.join(DATA_PATH, \"datasetCache\"+SO))\n",
    "# batchedDatasetVal = batchedDatasetVal.cache(os.path.join(DATA_PATH, \"datasetCacheVal\"+SO))\n",
    "\n",
    "@tf.function()\n",
    "def trainStep(x_batch_train, y_batch_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        probs = model(x_batch_train, training=True) \n",
    "        loss_value = loss_fn(y_batch_train, probs)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    #Gradient clipping\n",
    "    # grads = [tf.clip_by_norm(g, 2.0) for g in grads]\n",
    "\n",
    "    train_acc_metric.update_state(y_batch_train, probs)\n",
    "    train_f1_metric.update_state(y_batch_train, probs)\n",
    "    train_prec.update_state(y_batch_train, probs)\n",
    "    train_rec.update_state(y_batch_train, probs)\n",
    "\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "    return loss_value\n",
    "\n",
    "@tf.function()\n",
    "def valStep(x_batch_val, y_batch_val):\n",
    "    valProbs = model(x_batch_val, training=False)\n",
    "    # Update val metrics\n",
    "    val_acc_metric.update_state(y_batch_val, valProbs)\n",
    "    val_f1_metric.update_state(y_batch_val, valProbs)\n",
    "    val_prec.update_state(y_batch_val, valProbs)\n",
    "    val_rec.update_state(y_batch_val, valProbs)\n",
    "\n",
    "maxStep=0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch+1,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(batchedDataset):\n",
    "\n",
    "        loss_value =trainStep(x_batch_train,y_batch_train)\n",
    "\n",
    "        # Log \n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            template = 'Epoch {}/Step {}, Loss: {:.5f}, Accuracy: {:.5f}, F1: {:.4f}, Prec: {:.4f}, Rec: {:.4f}, lr: {:.5f}'\n",
    "            print(template.format(epoch+1, step,loss_value.numpy(), \n",
    "                                    train_acc_metric.result(),train_f1_metric.result(),\n",
    "                                    train_prec.result(), train_rec.result(), optimizer.learning_rate.numpy()))\n",
    "            \n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', loss_value, step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('accuracy', train_acc_metric.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('f1', train_f1_metric.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('prec', train_prec.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('rec', train_rec.result(), step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('learning rate', optimizer.learning_rate.numpy(), step=maxStep*epoch+step)\n",
    "                summary_writer.flush()\n",
    "\n",
    "    \n",
    "    train_acc_metric.reset_states()\n",
    "    train_f1_metric.reset_states()\n",
    "    train_prec.reset_states()\n",
    "    train_rec.reset_states()\n",
    "\n",
    "    maxStep=step\n",
    "\n",
    "    print(\"Epoch finished. Start validation\")\n",
    "    for x_batch_val, y_batch_val in batchedDatasetVal:\n",
    "        valStep(x_batch_val, y_batch_val)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    val_f1 = val_f1_metric.result()\n",
    "    val_f1_metric.reset_states()\n",
    "    val_precision = val_prec.result()\n",
    "    val_prec.reset_states()\n",
    "    val_recall = val_rec.result()\n",
    "    val_rec.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Validation f1: %.4f\" % (float(val_f1),))\n",
    "    print(\"Validation precision: %.4f\" % (float(val_precision),))\n",
    "    print(\"Validation recall: %.4f\" % (float(val_recall),))\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('valAcc', float(val_acc), step=epoch)\n",
    "        tf.summary.scalar('valF1', float(val_f1), step=epoch)\n",
    "        tf.summary.scalar('valPrecision', float(val_precision), step=epoch)\n",
    "        tf.summary.scalar('valRecall', float(val_recall), step=epoch)\n",
    "        summary_writer.flush()\n",
    "    if saveModel:\n",
    "      model.save(os.path.join(DATA_PATH, \"model_\"+SO+\"_epoch_{}_valF1Score{:.3f}\".format(epoch, float(val_f1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.save(os.path.join(DATA_PATH, \"model_\"+SO+\"_epoch_{}_valf1Score{:.3f}\".format(epoch, float(val_f1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc1f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 15:01:50.530641: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 295ms/step\n",
      "[('GO:0005622', 'GO:0005634', 'GO:0005737', 'GO:0005829', 'GO:0005886', 'GO:0031410', 'GO:0031981', 'GO:0043226', 'GO:0043227', 'GO:0043229', 'GO:0043231', 'GO:0071944', 'GO:0110165', 'GO:0120025')]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probs= model.predict(tf.expand_dims(list(datasetVal.take(32))[10][0], 0))\n",
    "prediction= [1 if p > 0.5 else 0 for p in probs[0]]\n",
    "probabilities= probs[probs>0.5]\n",
    "# classes = np.argwhere(prediction)\n",
    "print(mlb.inverse_transform(np.array([prediction])))\n",
    "print(probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
