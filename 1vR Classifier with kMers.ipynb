{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "# try:\n",
    "#   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# except:\n",
    "#   # Invalid device or cannot modify virtual devices once initialized.\n",
    "#   pass\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "print(DATA_PATH)\n",
    "\n",
    "# Choose subontology (CCO, MFO or BPO)\n",
    "SO = 'BPO'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3544f8a",
   "metadata": {},
   "source": [
    "## Reading fasta, obo and tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b515f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "sequences = [rec.seq for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Train/train_sequences.fasta\"),\"fasta\")]\n",
    "ids = [rec.id for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Train/train_sequences.fasta\"),\"fasta\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2898414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import obonet\n",
    "\n",
    "# Read the taxrank ontology\n",
    "url = os.path.join(DATA_PATH, \"Train/go-basic.obo\")\n",
    "graph = obonet.read_obo(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4bf949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "dfSO = df.loc[df[\"aspect\"]==SO]\n",
    "uniqueTerms = dfSO[\"term\"].unique()\n",
    "termsArr = list(dfSO[\"term\"].to_numpy())\n",
    "\n",
    "uniqueTermsDict={}\n",
    "for i,el in enumerate(uniqueTerms):\n",
    "    uniqueTermsDict[el] = i\n",
    "    \n",
    "print(dfSO.shape)\n",
    "df=dfSO\n",
    "\n",
    "df.set_index(\"EntryID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dec705",
   "metadata": {},
   "outputs": [],
   "source": [
    "testID = df.index.to_list()[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5338b7f8",
   "metadata": {},
   "source": [
    "## GO analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1058ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = df[\"term\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb35584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name = {id_: data.get('name') for id_, data in graph.nodes(data=True)}\n",
    "name_to_id = {data['name']: id_ for id_, data in graph.nodes(data=True) if 'name' in data}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c099fc2",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cae9a3b0",
   "metadata": {},
   "source": [
    "The task is a multilabel classification: The output has several possible targets (Gene Ontologies) but each can only be 1 (existing) or 0 (non existing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8e63ed9",
   "metadata": {},
   "source": [
    "Extract label weights from IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIa = pd.read_csv(os.path.join(DATA_PATH, \"IA.txt\"), sep='\\t', header=None)\n",
    "\n",
    "dfIa.set_index(0, inplace=True)\n",
    "\n",
    "labelWeights=[]\n",
    "allIndices = dfIa.index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "notFound=0\n",
    "for go in item_counts.index.to_list():\n",
    "    if go in allIndices:\n",
    "        labelWeights.append(dfIa.loc[go].to_numpy()[0])\n",
    "    else:\n",
    "        notFound += 1\n",
    "        labelWeights.append(0)\n",
    "\n",
    "print(\"Not found GOs: {} (set to 0)\".format(notFound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pickle\n",
    "\n",
    "topGOs= item_counts\n",
    "topGOs=topGOs.index.to_list()\n",
    "\n",
    "#Reduce possible GOs by label weight\n",
    "threshold=0\n",
    "labelWeights=np.array(labelWeights)\n",
    "selection = labelWeights>threshold\n",
    "topGOs=np.array(topGOs)[selection]\n",
    "labelWeights=labelWeights[selection]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([topGOs])\n",
    "\n",
    "dftest=df.loc[testID]\n",
    "indices = dftest[\"term\"].to_numpy()\n",
    "print(indices)\n",
    "print(mlb.transform([indices]))\n",
    "print(len(mlb.classes_))\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'MLB_'+SO+'.pkl'), 'wb') as f:\n",
    "    pickle.dump(mlb, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92019f8c",
   "metadata": {},
   "source": [
    "## Amino acids encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_dict = {'A': 1, 'B':24, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'O': 21, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'U': 22, 'V': 18, 'W': 19, 'Y': 20, 'X':30, 'Z':23}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfd823ba",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqLengths = [len(seq) for seq in sequences]\n",
    "maxLen = max(seqLengths)\n",
    "print(\"The max. length of the sequences is {}\".format(maxLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e581e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfAll=pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "soEntries = dfAll.loc[dfAll[\"aspect\"]==SO]\n",
    "soEntryIds = soEntries[\"EntryID\"].unique()\n",
    "\n",
    "# print(len(seqEntries))\n",
    "print(soEntryIds)\n",
    "\n",
    "# SoSequences = []\n",
    "# for entry in soEntryIds:\n",
    "#     SoSequences.append(sequences[ids.index(entry)])\n",
    "\n",
    "# print(len(SoSequences))\n",
    "dfAll.set_index(\"EntryID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f722e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "TRAIN_VAL_SPLIT = 0.7\n",
    "k = 3\n",
    "\n",
    "allAA = list(aa_dict.keys())\n",
    "allAA.sort()\n",
    "allCombinations= list(product(*(allAA for i in range(k))))\n",
    "allCombinations=np.array([''.join(el) for el in allCombinations])\n",
    "\n",
    "positionDict = dict(zip(allCombinations, np.arange(0,allCombinations.size).T))\n",
    "\n",
    "#Use numpy vectorize to speed up the mapping (hopefully)\n",
    "mapping = lambda x: aa_dict[x]\n",
    "vectMapping = np.vectorize(mapping)\n",
    "\n",
    "# Shuffle the data\n",
    "import random\n",
    "random.seed(516213)\n",
    "c = list(zip(sequences, ids))\n",
    "random.shuffle(c)\n",
    "sequencesShuffle, idsShuffle = zip(*c)\n",
    "\n",
    "\n",
    "#Train Validation Split\n",
    "split = int(np.floor(len(sequencesShuffle)*TRAIN_VAL_SPLIT))\n",
    "print(split)\n",
    "trainSeq = sequencesShuffle[0:split]\n",
    "valSeq = sequencesShuffle[split+1:]\n",
    "trainIds = idsShuffle[0:split]\n",
    "valIds = idsShuffle[split+1:]\n",
    "\n",
    "\n",
    "def generator():\n",
    "  for i,seq in enumerate(trainSeq):\n",
    "      entryId = trainIds[i]\n",
    "      if entryId in soEntryIds:\n",
    "        labelData = df.loc[entryId]\n",
    "        # indices = labelData[\"termToken\"].to_numpy()\n",
    "        indices = labelData[\"term\"].to_numpy()\n",
    "      else: \n",
    "        indices=[]\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "          #supress the warnings for unknown classes\n",
    "          warnings.simplefilter(\"ignore\")\n",
    "          y = mlb.transform([indices])\n",
    "\n",
    "      # if np.count_nonzero(y)==0 and np.random.random()>nonRelevantThreshold:\n",
    "      #   continue\n",
    "\n",
    "      \n",
    "      kmers = [seq[j:j+k] if j < len(seq)-(k-1) else 0 for j,el in enumerate(seq)]\n",
    "      kmers = kmers[0:-(k-1)]\n",
    "      kmers = [str(el) for el in kmers]\n",
    "      values, counts = np.unique(kmers, return_counts=True)\n",
    "      freqVector=np.zeros(allCombinations.shape)\n",
    "      for l,v in enumerate(values):\n",
    "          freqVector[positionDict[v]] = counts[l]\n",
    "      yield (freqVector,y[0])\n",
    "\n",
    "\n",
    "def generatorVal():\n",
    "  for i,seq in enumerate(valSeq):\n",
    "      entryId = valIds[i]\n",
    "      if entryId in soEntryIds:\n",
    "        labelData = df.loc[entryId]\n",
    "        # indices = labelData[\"termToken\"].to_numpy()\n",
    "        indices = labelData[\"term\"].to_numpy()\n",
    "      else:\n",
    "        indices=[]\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "          #supress the warnings for unknown classes\n",
    "          warnings.simplefilter(\"ignore\")\n",
    "          y = mlb.transform([indices])\n",
    "\n",
    "      # if np.count_nonzero(y)==0 and np.random.random()>nonRelevantThreshold:\n",
    "      #   continue\n",
    "      \n",
    "      kmers = [seq[j:j+k] if j < len(seq)-(k-1) else 0 for j,el in enumerate(seq)]\n",
    "      kmers = kmers[0:-(k-1)]\n",
    "      kmers = [str(el) for el in kmers]\n",
    "      values, counts = np.unique(kmers, return_counts=True)\n",
    "      freqVector=np.zeros(allCombinations.shape)\n",
    "      for l,v in enumerate(values):\n",
    "          freqVector[positionDict[v]] = counts[l]\n",
    "      yield (freqVector,y[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = generator()\n",
    "test = next(g)\n",
    "print(\"The first sample: \\n{}\\n{}\".format(test[0].shape, test[0][0:100]))\n",
    "print(\"The first sample has {} classes\".format(np.count_nonzero(test[1])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10c4a51a",
   "metadata": {},
   "source": [
    "## One-vs-Rest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b71bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = mlb.classes_[671]\n",
    "\n",
    "def getKmers(seq):\n",
    "    kmers = [seq[j:j+k] if j < len(seq)-(k-1) else 0 for j,el in enumerate(seq)]\n",
    "    kmers = kmers[0:-(k-1)]\n",
    "    kmers = [str(el) for el in kmers]\n",
    "    values, counts = np.unique(kmers, return_counts=True)\n",
    "    freqVector=np.zeros(allCombinations.shape)\n",
    "    for l,v in enumerate(values):\n",
    "        freqVector[positionDict[v]] = counts[l]\n",
    "    return freqVector\n",
    "\n",
    "\n",
    "X=[]\n",
    "y=[]\n",
    "positiveClassCount=0\n",
    "\n",
    "for i,seq in enumerate(tqdm(trainSeq)):\n",
    "    entryId = trainIds[i]\n",
    "    if entryId in soEntryIds:\n",
    "        labelData = df.loc[entryId]\n",
    "        indices = labelData[\"term\"].to_numpy()\n",
    "    else: \n",
    "        indices=[]\n",
    "    if target in indices:\n",
    "        freqVector= getKmers(seq)\n",
    "        X.append(freqVector)\n",
    "        y.append(1)\n",
    "        positiveClassCount += 1\n",
    "    elif 0.2*len(y) < positiveClassCount:\n",
    "        freqVector= getKmers(seq)\n",
    "        X.append(freqVector)\n",
    "        y.append(0)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb02e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xval=[]\n",
    "yval=[]\n",
    "positiveClassCountVal=0\n",
    "for i,seq in enumerate(tqdm(valSeq)):\n",
    "    entryId = valIds[i]\n",
    "    if entryId in soEntryIds:\n",
    "        labelData = df.loc[entryId]\n",
    "        indices = labelData[\"term\"].to_numpy()\n",
    "    else: \n",
    "        indices=[]\n",
    "    if target in indices:\n",
    "        freqVector= getKmers(seq)\n",
    "        Xval.append(freqVector)\n",
    "        yval.append(1)\n",
    "        positiveClassCountVal += 1\n",
    "    elif len(yval)/2 < positiveClassCountVal:\n",
    "        freqVector= getKmers(seq)\n",
    "        Xval.append(freqVector)\n",
    "        yval.append(0)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "Xval=np.array(Xval)\n",
    "yval=np.array(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.count_nonzero(y))\n",
    "print(Xval.shape)\n",
    "print(yval.shape)\n",
    "print(np.count_nonzero(yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b6936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# clf = LogisticRegression().fit(X, y)\n",
    "# clf = svm.SVC(probability=True).fit(X, y)\n",
    "# clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(4, 64)).fit(X, y)\n",
    "clf = make_pipeline(preprocessing.StandardScaler(), LogisticRegression(solver=\"liblinear\"))\n",
    "clf.fit(X, y)\n",
    "print(clf.predict(Xval))\n",
    "# print(clf.predict_proba(Xval))\n",
    "print(clf.score(Xval, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1Score(yTrue, yPred):\n",
    "    tp = np.count_nonzero(np.logical_and(np.array(yTrue, dtype=\"bool\") , np.array(yPred, dtype=\"bool\")))\n",
    "    fn = np.count_nonzero(np.logical_and(np.logical_not(yTrue) , np.array(yPred, dtype=\"bool\")))\n",
    "    fp = np.count_nonzero(np.logical_and(np.array(yTrue, dtype=\"bool\") , np.logical_not(yPred)))\n",
    "    prec = (tp)/(tp+fp+1e-20)\n",
    "    rec = tp/(tp+fn+1e-20)\n",
    "    f1 = 2*(prec*rec)/(prec+rec+1e-20)\n",
    "    return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f835da",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1Score(yval, clf.predict(Xval))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0a05488",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGo = pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "dfGo = dfGo.loc[dfGo[\"aspect\"]==SO]\n",
    "uniqueTerms = dfGo[\"term\"].unique()\n",
    "termsArr = list(dfGo[\"term\"].to_numpy())\n",
    "\n",
    "uniqueTermsDict={}\n",
    "for i,el in enumerate(uniqueTerms):\n",
    "    uniqueTermsDict[el] = i\n",
    "    \n",
    "print(dfGo.shape)\n",
    "\n",
    "dfGo.set_index(\"term\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b99443",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqDict = dict(zip(ids, sequences))\n",
    "classifiers = []\n",
    "scores=[]\n",
    "f1Scores=[]\n",
    "\n",
    "for l in tqdm(mlb.classes_):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    if dfGo.loc[l].size<3:\n",
    "        classifiers.append(None)\n",
    "        f1Scores.append(0)\n",
    "        scores.append(0)\n",
    "        continue\n",
    "\n",
    "    relevantSequenceIds = dfGo.loc[l][\"EntryID\"].unique()\n",
    "    for seqId in relevantSequenceIds:\n",
    "        seq = seqDict[seqId]\n",
    "        X.append(getKmers(seq))\n",
    "        y.append(1)\n",
    "\n",
    "    nonRelSeqIds = dfGo.sample(n=len(X))[\"EntryID\"].unique()\n",
    "    for seqId in nonRelSeqIds:\n",
    "        if seqId in relevantSequenceIds:\n",
    "            continue\n",
    "        seq = seqDict[seqId]\n",
    "        X.append(getKmers(seq))\n",
    "        y.append(0)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    try:\n",
    "        clf = make_pipeline(preprocessing.StandardScaler(), LogisticRegression(solver=\"liblinear\"))\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "    except:\n",
    "        classifiers.append(None)\n",
    "        f1Scores.append(0)\n",
    "        scores.append(0)\n",
    "    classifiers.append(clf)\n",
    "    f1Scores.append(f1Score(y_test, clf.predict(X_test)))\n",
    "    # print(clf.predict_proba(Xval))\n",
    "    scores.append(clf.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c9aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
