{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "DATA_PATH_INTERPRO = os.getenv('DATA_PATH_INTERPRO')\n",
    "print(DATA_PATH)\n",
    "print(DATA_PATH_INTERPRO)\n",
    "\n",
    "# Choose subontology (CCO, MFO or BPO)\n",
    "SO = 'CCO'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3544f8a",
   "metadata": {},
   "source": [
    "## Reading fasta, obo and tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b515f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "sequences = [rec.seq for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Train/train_sequences.fasta\"),\"fasta\")]\n",
    "ids = [rec.id for rec in SeqIO.parse(os.path.join(DATA_PATH, \"Train/train_sequences.fasta\"),\"fasta\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2898414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import obonet\n",
    "\n",
    "# Read the taxrank ontology\n",
    "url = os.path.join(DATA_PATH, \"Train/go-basic.obo\")\n",
    "graph = obonet.read_obo(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4bf949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "dfSO = df.loc[df[\"aspect\"]==SO]\n",
    "uniqueTerms = dfSO[\"term\"].unique()\n",
    "termsArr = list(dfSO[\"term\"].to_numpy())\n",
    "\n",
    "uniqueTermsDict={}\n",
    "for i,el in enumerate(uniqueTerms):\n",
    "    uniqueTermsDict[el] = i\n",
    "    \n",
    "print(dfSO.shape)\n",
    "df=dfSO\n",
    "\n",
    "df.set_index(\"EntryID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dec705",
   "metadata": {},
   "outputs": [],
   "source": [
    "testID = df.index.to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGo = pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "dfGo = dfGo.loc[dfGo[\"aspect\"]==SO]\n",
    "\n",
    "dfGo.set_index(\"term\", inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5338b7f8",
   "metadata": {},
   "source": [
    "## GO analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1058ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = df[\"term\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb35584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name = {id_: data.get('name') for id_, data in graph.nodes(data=True)}\n",
    "name_to_id = {data['name']: id_ for id_, data in graph.nodes(data=True) if 'name' in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356dbdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "layerGOs={}\n",
    "for layer, nodes in enumerate(networkx.topological_generations(graph)):\n",
    "    # layerGOs[layer] = nodes\n",
    "    for n in nodes:\n",
    "        layerGOs[n] = layer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c099fc2",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cae9a3b0",
   "metadata": {},
   "source": [
    "The task is a multilabel classification: The output has several possible targets (Gene Ontologies) but each can only be 1 (existing) or 0 (non existing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8e63ed9",
   "metadata": {},
   "source": [
    "Extract label weights from IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIa = pd.read_csv(os.path.join(DATA_PATH, \"IA.txt\"), sep='\\t', header=None)\n",
    "\n",
    "dfIa.set_index(0, inplace=True)\n",
    "\n",
    "labelWeights=[]\n",
    "allIndices = dfIa.index.tolist()\n",
    "\n",
    "notFound=0\n",
    "for go in item_counts.index.to_list():\n",
    "    if go in allIndices:\n",
    "        labelWeights.append(dfIa.loc[go].to_numpy()[0])\n",
    "    else:\n",
    "        notFound += 1\n",
    "        labelWeights.append(0)\n",
    "\n",
    "print(\"Not found GOs: {} (set to 0)\".format(notFound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeee590",
   "metadata": {},
   "outputs": [],
   "source": [
    "topGOs=item_counts.index.to_list()\n",
    "\n",
    "threshold=0\n",
    "labelWeights=np.array(labelWeights)\n",
    "selection = labelWeights>threshold\n",
    "topGOs=np.array(topGOs)[selection]\n",
    "\n",
    "if os.path.exists(os.path.join(DATA_PATH, \"GODataSizes_\"+SO+\".npy\")):\n",
    "    print(\"Loading presaved data\")\n",
    "    GODataSizes = np.load(os.path.join(DATA_PATH, \"GODataSizes_\"+SO+\".npy\"))\n",
    "else:\n",
    "    GODataSizes= [dfGo.loc[g].size for g in topGOs]\n",
    "    np.save(os.path.join(DATA_PATH, \"GODataSizes_\"+SO), GODataSizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#At least 10 samples\n",
    "print(np.count_nonzero(np.array(GODataSizes)>10))\n",
    "GODataSizes= np.array(GODataSizes)\n",
    "GOsWithSufficientData = topGOs[GODataSizes>10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pickle\n",
    "\n",
    "print(len(topGOs))\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([GOsWithSufficientData])\n",
    "\n",
    "dftest=df.loc[testID]\n",
    "indices = dftest[\"term\"].to_numpy()\n",
    "print(indices)\n",
    "print(mlb.transform([indices]))\n",
    "print(len(mlb.classes_))\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'MLB_'+SO+'.pkl'), 'wb') as f:\n",
    "    pickle.dump(mlb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569f67a",
   "metadata": {},
   "source": [
    "Get an order array that sorts the GOs by the depth in the GO graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlbLayers = []\n",
    "for c in mlb.classes_:\n",
    "    mlbLayers.append(layerGOs[c])\n",
    "\n",
    "GOSortIndices = np.argsort(np.max(mlbLayers)-mlbLayers) \n",
    "print(GOSortIndices[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb189ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelWeightsCorr=[]\n",
    "occurenceScores=[]\n",
    "termHist= df[\"term\"].value_counts()\n",
    "maxGoCount = termHist.max()\n",
    "\n",
    "notFound=0\n",
    "for go in mlb.classes_:\n",
    "    if go in allIndices:\n",
    "        occurenceScore = (maxGoCount-termHist[go])/maxGoCount\n",
    "        occurenceScores.append(occurenceScore)\n",
    "        labelWeightsCorr.append(dfIa.loc[go].to_numpy()[0])\n",
    "    else:\n",
    "        notFound += 1\n",
    "        labelWeightsCorr.append(0)\n",
    "\n",
    "print(\"Not found GOs: {} (set to 0)\".format(notFound))\n",
    "labelWeightsCorr=np.array(labelWeightsCorr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92019f8c",
   "metadata": {},
   "source": [
    "## Amino acids encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_dict = {'A': 1, 'B':24, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'O': 21, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'U': 22, 'V': 18, 'W': 19, 'Y': 20, 'X':30, 'Z':23}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d206203f",
   "metadata": {},
   "source": [
    "## Interpro Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43765061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "root = ET.parse(os.path.join(DATA_PATH, \"interpro.xml\")).getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e280b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "possibleDomains=[]\n",
    "for child in root:\n",
    "    if \"type\" in child.attrib:\n",
    "        if(child.attrib[\"type\"]==\"Domain\"):\n",
    "            # print(child.tag, child.attrib)\n",
    "            possibleDomains.append(child.attrib[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b46e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(possibleDomains)\n",
    "\n",
    "mlbInterPro = MultiLabelBinarizer()\n",
    "mlbInterPro.fit([possibleDomains])\n",
    "\n",
    "\n",
    "print(mlbInterPro.transform([[\"IPR000001\"]]))\n",
    "print(len(mlbInterPro.classes_))\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'MLB_InterPro_'+SO+'.pkl'), 'wb') as f:\n",
    "    pickle.dump(mlbInterPro, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd8d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(DATA_PATH_INTERPRO, \"train_sequences1.fasta.json\")) as f:\n",
    "    iprData1 = json.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_PATH_INTERPRO, \"train_sequences2.fasta.json\")) as f:\n",
    "    iprData2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iprIds = {}\n",
    "\n",
    "\n",
    "for entry in tqdm([*iprData1[\"results\"], *iprData2[\"results\"]]):\n",
    "    entryId = entry[\"xref\"][0][\"id\"]\n",
    "    matches=[]\n",
    "    for match in entry[\"matches\"]:\n",
    "        sigEntry = match[\"signature\"][\"entry\"]\n",
    "        if(sigEntry):\n",
    "            type = sigEntry[\"type\"]\n",
    "            if type==\"DOMAIN\":\n",
    "                iprId = match[\"signature\"][\"entry\"][\"accession\"]\n",
    "                matches.append(iprId)\n",
    "    iprIds[entryId] = matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iprIds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e073e2",
   "metadata": {},
   "source": [
    "## Physiochemical Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from Bio.Seq import MutableSeq, Seq\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "if os.path.exists(os.path.join(DATA_PATH, \"PCDict\"+\".pkl\")):\n",
    "    print(\"Loading presaved data\")\n",
    "    with open(os.path.join(DATA_PATH, \"PCDict\"+\".pkl\"), 'rb') as f:\n",
    "        PCDict = pickle.load(f)\n",
    "else:\n",
    "    PCDict = {}\n",
    "\n",
    "    for i,seq in enumerate(tqdm(sequences)):\n",
    "\n",
    "        index = ids[i]\n",
    "        \n",
    "        X =ProteinAnalysis(seq)\n",
    "\n",
    "        if \"X\" in seq or \"U\" in seq or \"O\" in seq or \"B\" in seq or \"Z\" in seq:\n",
    "            cleanedSeq = seq.replace(\"X\", \"A\")\n",
    "            cleanedSeq = cleanedSeq.replace(\"U\", \"A\")\n",
    "            cleanedSeq = cleanedSeq.replace(\"O\", \"A\")\n",
    "            cleanedSeq = cleanedSeq.replace(\"B\", \"A\")\n",
    "            cleanedSeq = cleanedSeq.replace(\"Z\", \"A\")\n",
    "            XClean =ProteinAnalysis(cleanedSeq)\n",
    "            flex = XClean.flexibility()\n",
    "            molW = XClean.molecular_weight()\n",
    "            instabIdx = XClean.instability_index()\n",
    "            gravy = XClean.gravy()\n",
    "        else:\n",
    "            flex= X.flexibility()\n",
    "            molW = X.molecular_weight()\n",
    "            instabIdx = X.instability_index()\n",
    "            gravy = X.gravy()\n",
    "\n",
    "        if len(flex)>10:\n",
    "            idx = np.round(np.linspace(0, len(flex) - 1, 10)).astype(int)\n",
    "            flex = np.array(flex)[idx]\n",
    "        elif len(flex)<10:\n",
    "            flex = np.pad(flex, (0,10-len(flex)))\n",
    "\n",
    "        protS= X.protein_scale(aa_dict,100)\n",
    "        if len(protS)>10:\n",
    "            idx = np.round(np.linspace(0, len(protS) - 1, 10)).astype(int)\n",
    "            protS = np.array(protS)[idx]\n",
    "        elif len(protS)<10:\n",
    "            protS = np.pad(protS, (0,10-len(protS)))\n",
    "\n",
    "        #Adding all the physiochemical properties (N = 53)\n",
    "        PCDict[index] = [ molW, X.aromaticity(), instabIdx, *list(X.get_amino_acids_percent().values()),\n",
    "                *flex, gravy, *protS, X.isoelectric_point(), X.charge_at_pH(7), X.charge_at_pH(3), X.charge_at_pH(10), *X.molar_extinction_coefficient(),\n",
    "                *X.secondary_structure_fraction()]\n",
    "        \n",
    "    with open(os.path.join(DATA_PATH, \"PCDict\"+\".pkl\"), 'wb') as f:\n",
    "        pickle.dump(PCDict, f)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfd823ba",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll=pd.read_csv(os.path.join(DATA_PATH, \"Train/train_terms.tsv\"), sep='\\t')\n",
    "\n",
    "soEntries = dfAll.loc[dfAll[\"aspect\"]==SO]\n",
    "soEntryIds = soEntries[\"EntryID\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f722e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "TRAIN_VAL_SPLIT = 0.7\n",
    "\n",
    "\n",
    "# Shuffle the data\n",
    "import random\n",
    "random.seed(516213)\n",
    "c = list(zip(sequences, ids))\n",
    "random.shuffle(c)\n",
    "sequencesShuffle, idsShuffle = zip(*c)\n",
    "\n",
    "\n",
    "#Train Validation Split\n",
    "split = int(np.floor(len(sequencesShuffle)*TRAIN_VAL_SPLIT))\n",
    "print(split)\n",
    "trainSeq = sequencesShuffle[0:split]\n",
    "valSeq = sequencesShuffle[split+1:]\n",
    "trainIds = idsShuffle[0:split]\n",
    "valIds = idsShuffle[split+1:]\n",
    "\n",
    "\n",
    "def generator():\n",
    "  for i,seq in enumerate(trainSeq):\n",
    "      entryId = trainIds[i]\n",
    "      if entryId in soEntryIds:\n",
    "        labelData = df.loc[entryId]\n",
    "        # indices = labelData[\"termToken\"].to_numpy()\n",
    "        indices = labelData[\"term\"].to_numpy()\n",
    "      else: \n",
    "        indices=[]\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "        #supress the warnings for unknown classes\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        y = mlb.transform([indices])\n",
    "\n",
    "      # with warnings.catch_warnings():\n",
    "      #   #supress the warnings for unknown classes\n",
    "      #   warnings.simplefilter(\"ignore\")\n",
    "      #   if entryId in iprIds:\n",
    "      #     x  = mlbInterPro.transform([iprIds[entryId]])\n",
    "      #   else:\n",
    "      #     x  = mlbInterPro.transform([[]])\n",
    "      \n",
    "\n",
    "      #Adding all the physiochemical properties (N = 53)\n",
    "      pcProps = PCDict[entryId]\n",
    "      # extended = [ *pcProps, *x[0]]\n",
    "     \n",
    "      yield (np.array(pcProps),y[0])\n",
    "\n",
    "\n",
    "def generatorVal():\n",
    "  for i,seq in enumerate(valSeq):\n",
    "      entryId = valIds[i]\n",
    "      if entryId in soEntryIds:\n",
    "        labelData = df.loc[entryId]\n",
    "        # indices = labelData[\"termToken\"].to_numpy()\n",
    "        indices = labelData[\"term\"].to_numpy()\n",
    "      else: \n",
    "        indices=[]\n",
    "\n",
    "      with warnings.catch_warnings():\n",
    "        #supress the warnings for unknown classes\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        y = mlb.transform([indices])\n",
    "\n",
    "      # with warnings.catch_warnings():\n",
    "      #   #supress the warnings for unknown classes\n",
    "      #   warnings.simplefilter(\"ignore\")\n",
    "      #   if entryId in iprIds:\n",
    "      #     x  = mlbInterPro.transform([iprIds[entryId]])\n",
    "      #   else:\n",
    "      #     x  = mlbInterPro.transform([[]])\n",
    "\n",
    "      #Adding all the physiochemical properties (N = 53)\n",
    "      pcProps = PCDict[entryId]\n",
    "      # extended = [ *pcProps, *x[0]]\n",
    "     \n",
    "      yield (np.array(pcProps),y[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = generator()\n",
    "test = next(g)\n",
    "print(\"The first sample: \\n{}\\n{}\".format(test[0].shape, test[0][0:60]))\n",
    "print(\"The first output: \\n{}\\n{}\".format(test[1].shape, test[1][0:60]))\n",
    "print(\"The first sample has {} input classes\".format(np.count_nonzero(test[0])))\n",
    "print(\"The first sample has {} output classes\".format(np.count_nonzero(test[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=[]\n",
    "Xval=[]\n",
    "ytrain=[]\n",
    "yval=[]\n",
    "\n",
    "genTrain = generator()\n",
    "genVal=generatorVal()\n",
    "\n",
    "for data in tqdm(genTrain):\n",
    "    Xtrain.append(data[0])\n",
    "    ytrain.append(data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ec893",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain= np.array(Xtrain)\n",
    "ytrain= np.array(ytrain)\n",
    "print(Xtrain.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10c4a51a",
   "metadata": {},
   "source": [
    "## Chain of Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "\n",
    "\n",
    "base_lr = LogisticRegression(solver='lbfgs', random_state=0)\n",
    "chain = ClassifierChain(base_lr, order=GOSortIndices, random_state=0)\n",
    "\n",
    "chain.fit(Xtrain, ytrain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
